{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e2f8012e-f322-4234-aa7f-212c6c1620de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "073131aa-9d82-4037-91e6-8a5d750a0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT_DIR = '/mnt/swordfish-pool2/ndeas/prob_models/results/' #'/burg/nlp/users/zfh2000/style_results'\n",
    "\n",
    "# DATA_SRC = '/mnt/swordfish-pool2/ndeas/prob_models/data/enron_processed.json' #'/burg/nlp/users/zfh2000/enron_processed.json'\n",
    "# SPLITS_PATH = '/mnt/swordfish-pool2/ndeas/prob_models/data/authors_splits.json' #'/burg/home/zfh2000/styletopicmodeling/scripts/authors_splits.json'\n",
    "# SYNTHETIC_SRC = '/mnt/swordfish-pool2/ndeas/prob_models/data/gpt_4_enron_processed.json' #'/burg/nlp/users/zfh2000/gpt_4_enron_processed.json'\n",
    "\n",
    "OUT_DIR = '/burg/nlp/users/zfh2000/style_results'\n",
    "\n",
    "DATA_SRC = '/burg/nlp/users/zfh2000/enron_processed.json'\n",
    "SPLITS_PATH = '/burg/home/zfh2000/styletopicmodeling/scripts/authors_splits.json'\n",
    "SYNTHETIC_SRC = '/burg/nlp/users/zfh2000/gpt_4_enron_processed.json'\n",
    "\n",
    "# NICK: files have been uploaded here: https://drive.google.com/drive/folders/1uF9GWEGe4aqSeo2MlachAWR9bTHsJq1q?usp=sharing\n",
    "# ['formality','irony', 'sentences', 'punc_tags', 'pos_bigrams', 'morph_tags', 'pos', 'casing', 'question',  'passive', 'emotion_task', 'sentiment_task', 'emoji_task']\n",
    "# NICK: Unsure which of of these features we should include!\n",
    "\n",
    "META_FEATURES = ['pos_bigrams'] # formality, casing, emoji, punctuation\n",
    "EXPERIMENT_NAME = '_'.join(META_FEATURES)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 2 #8\n",
    "\n",
    "LR = 1e-2\n",
    "BETAS = (0.99, 0.999)\n",
    "EPS   = 1e-8\n",
    "CLIP_NORM = 10.\n",
    "ADAM_ARGS = {'lr': LR, 'betas': BETAS, 'eps': EPS, 'clip_norm': CLIP_NORM}\n",
    "\n",
    "DEVICE = torch.device('cuda:0')\n",
    "\n",
    "NUM_TOPICS = 10 #10 #20 # NICK totally subject to changes\n",
    "NUM_STYLES = 5  #20\n",
    "HIDDEN_DIM = 64\n",
    "DROPOUT    = 0 #0.2\n",
    "\n",
    "THETA_PRIOR_DIST = 'gaussian'\n",
    "THETA_PRIOR_LOC = 0.\n",
    "THETA_PRIOR_SCALE = 1.\n",
    "\n",
    "KAPPA_PRIOR_DIST = 'gaussian' #'laplace'\n",
    "KAPPA_PRIOR_LOC = 0.\n",
    "KAPPA_PRIOR_SCALE = 5 #10 #1.\n",
    "\n",
    "STYLE_TOPIC_LINK = 'none' #'kappa_doc' #'none'\n",
    "\n",
    "NUM_EPOCHS = 5 #100\n",
    "\n",
    "MAX_DF=0.7\n",
    "MIN_DF=20\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "info = {\n",
    "    'experiment_name': EXPERIMENT_NAME,\n",
    "    'num_topics': NUM_TOPICS,\n",
    "    'num_styles': NUM_STYLES,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'dropout': DROPOUT,\n",
    "    'theta_prior_dist': THETA_PRIOR_DIST,\n",
    "    'theta_prior_loc': THETA_PRIOR_LOC,\n",
    "    'theta_prior_scale': THETA_PRIOR_SCALE,\n",
    "    'kappa_prior_dist': KAPPA_PRIOR_DIST,\n",
    "    'kappa_prior_loc': KAPPA_PRIOR_LOC,\n",
    "    'kappa_prior_scale': KAPPA_PRIOR_SCALE,\n",
    "    'style_topic_link': STYLE_TOPIC_LINK,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'date': date,\n",
    "    'lr': LR,\n",
    "    'betas': BETAS,\n",
    "    'eps': EPS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'clip_norm': CLIP_NORM,\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'device': DEVICE.type,\n",
    "    'meta_features': META_FEATURES,\n",
    "    'DATA_SRC': DATA_SRC,\n",
    "    'SPLITS_PATH': SPLITS_PATH,\n",
    "    'SYNTHETIC_SRC': SYNTHETIC_SRC,\n",
    "    'MAX_DF': MAX_DF,\n",
    "    'MIN_DF': MIN_DF,\n",
    "\n",
    "}\n",
    "PATH = os.path.join(OUT_DIR, EXPERIMENT_NAME, date)\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "with open(os.path.join(PATH, 'info.json'), 'w') as f:\n",
    "    json.dump(info, f, indent=4)\n",
    "\n",
    "DATA_DIR_PATH = os.path.join(OUT_DIR,EXPERIMENT_NAME, f'maxdf{MAX_DF}_mindf{MIN_DF}_DATA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b5753-b006-44c8-885f-32f70635d4ca",
   "metadata": {},
   "source": [
    "# Data Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec51068-a48f-46e4-9ae6-c1b4eb9d8244",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f79754a-9fa0-4fdd-a647-0084e702f05f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Here is our forecast', 'info': {'author': 'allen-p', 'from': 'From: phillip.allen@enron.com', 'to': 'X-To: Tim Belden', 'cc': 'X-cc:', 'bcc': 'X-bcc:', 'meta': 'Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)'}, 'sentences': {'sentences:it-cleft': 0.0, 'sentences:pseudo-cleft': 0.0, 'sentences:all-cleft': 0.0, 'sentences:there-cleft': 0.0, 'sentences:if-because-cleft': 0.0, 'sentences:passive': 0.0, 'sentences:subj-relcl': 0.0, 'sentences:obj-relcl': 0.0, 'sentences:tag-question': 0.0, 'sentences:coordinate-clause': 0.0, 'none': 1}, 'passive': [0], 'formality': ['Formal'], 'pos': {'ADJ': 0, 'ADP': 0, 'ADV': 1, 'CONJ': 0, 'DET': 0, 'NOUN': 1, 'NUM': 0, 'PRT': 0, 'PRON': 1, 'VERB': 1, '.': 0, 'X': 0}, 'question': ['statement'], 'gram2vec': {'pos_unigrams:ADJ': 0.0, 'pos_unigrams:ADP': 0.0, 'pos_unigrams:ADV': 0.25, 'pos_unigrams:AUX': 0.25, 'pos_unigrams:CCONJ': 0.0, 'pos_unigrams:DET': 0.0, 'pos_unigrams:INTJ': 0.0, 'pos_unigrams:NOUN': 0.25, 'pos_unigrams:NUM': 0.0, 'pos_unigrams:PART': 0.0, 'pos_unigrams:PRON': 0.25, 'pos_unigrams:PROPN': 0.0, 'pos_unigrams:PUNCT': 0.0, 'pos_unigrams:SCONJ': 0.0, 'pos_unigrams:SYM': 0.0, 'pos_unigrams:VERB': 0.0, 'pos_unigrams:X': 0.0, 'pos_unigrams:SPACE': 0.0, 'pos_bigrams:ADJ ADP': 0.0, 'pos_bigrams:ADP ADJ': 0.0, 'pos_bigrams:ADJ ADV': 0.0, 'pos_bigrams:ADV ADJ': 0.0, 'pos_bigrams:ADJ AUX': 0.0, 'pos_bigrams:AUX ADJ': 0.0, 'pos_bigrams:ADJ CCONJ': 0.0, 'pos_bigrams:CCONJ ADJ': 0.0, 'pos_bigrams:ADJ DET': 0.0, 'pos_bigrams:DET ADJ': 0.0, 'pos_bigrams:ADJ INTJ': 0.0, 'pos_bigrams:INTJ ADJ': 0.0, 'pos_bigrams:ADJ NOUN': 0.0, 'pos_bigrams:NOUN ADJ': 0.0, 'pos_bigrams:ADJ NUM': 0.0, 'pos_bigrams:NUM ADJ': 0.0, 'pos_bigrams:ADJ PART': 0.0, 'pos_bigrams:PART ADJ': 0.0, 'pos_bigrams:ADJ PRON': 0.0, 'pos_bigrams:PRON ADJ': 0.0, 'pos_bigrams:ADJ PROPN': 0.0, 'pos_bigrams:PROPN ADJ': 0.0, 'pos_bigrams:ADJ PUNCT': 0.0, 'pos_bigrams:PUNCT ADJ': 0.0, 'pos_bigrams:ADJ SCONJ': 0.0, 'pos_bigrams:SCONJ ADJ': 0.0, 'pos_bigrams:ADJ SYM': 0.0, 'pos_bigrams:SYM ADJ': 0.0, 'pos_bigrams:ADJ VERB': 0.0, 'pos_bigrams:VERB ADJ': 0.0, 'pos_bigrams:ADJ X': 0.0, 'pos_bigrams:X ADJ': 0.0, 'pos_bigrams:ADJ SPACE': 0.0, 'pos_bigrams:SPACE ADJ': 0.0, 'pos_bigrams:ADP ADV': 0.0, 'pos_bigrams:ADV ADP': 0.0, 'pos_bigrams:ADP AUX': 0.0, 'pos_bigrams:AUX ADP': 0.0, 'pos_bigrams:ADP CCONJ': 0.0, 'pos_bigrams:CCONJ ADP': 0.0, 'pos_bigrams:ADP DET': 0.0, 'pos_bigrams:DET ADP': 0.0, 'pos_bigrams:ADP INTJ': 0.0, 'pos_bigrams:INTJ ADP': 0.0, 'pos_bigrams:ADP NOUN': 0.0, 'pos_bigrams:NOUN ADP': 0.0, 'pos_bigrams:ADP NUM': 0.0, 'pos_bigrams:NUM ADP': 0.0, 'pos_bigrams:ADP PART': 0.0, 'pos_bigrams:PART ADP': 0.0, 'pos_bigrams:ADP PRON': 0.0, 'pos_bigrams:PRON ADP': 0.0, 'pos_bigrams:ADP PROPN': 0.0, 'pos_bigrams:PROPN ADP': 0.0, 'pos_bigrams:ADP PUNCT': 0.0, 'pos_bigrams:PUNCT ADP': 0.0, 'pos_bigrams:ADP SCONJ': 0.0, 'pos_bigrams:SCONJ ADP': 0.0, 'pos_bigrams:ADP SYM': 0.0, 'pos_bigrams:SYM ADP': 0.0, 'pos_bigrams:ADP VERB': 0.0, 'pos_bigrams:VERB ADP': 0.0, 'pos_bigrams:ADP X': 0.0, 'pos_bigrams:X ADP': 0.0, 'pos_bigrams:ADP SPACE': 0.0, 'pos_bigrams:SPACE ADP': 0.0, 'pos_bigrams:ADV AUX': 0.3333333333333333, 'pos_bigrams:AUX ADV': 0.0, 'pos_bigrams:ADV CCONJ': 0.0, 'pos_bigrams:CCONJ ADV': 0.0, 'pos_bigrams:ADV DET': 0.0, 'pos_bigrams:DET ADV': 0.0, 'pos_bigrams:ADV INTJ': 0.0, 'pos_bigrams:INTJ ADV': 0.0, 'pos_bigrams:ADV NOUN': 0.0, 'pos_bigrams:NOUN ADV': 0.0, 'pos_bigrams:ADV NUM': 0.0, 'pos_bigrams:NUM ADV': 0.0, 'pos_bigrams:ADV PART': 0.0, 'pos_bigrams:PART ADV': 0.0, 'pos_bigrams:ADV PRON': 0.0, 'pos_bigrams:PRON ADV': 0.0, 'pos_bigrams:ADV PROPN': 0.0, 'pos_bigrams:PROPN ADV': 0.0, 'pos_bigrams:ADV PUNCT': 0.0, 'pos_bigrams:PUNCT ADV': 0.0, 'pos_bigrams:ADV SCONJ': 0.0, 'pos_bigrams:SCONJ ADV': 0.0, 'pos_bigrams:ADV SYM': 0.0, 'pos_bigrams:SYM ADV': 0.0, 'pos_bigrams:ADV VERB': 0.0, 'pos_bigrams:VERB ADV': 0.0, 'pos_bigrams:ADV X': 0.0, 'pos_bigrams:X ADV': 0.0, 'pos_bigrams:ADV SPACE': 0.0, 'pos_bigrams:SPACE ADV': 0.0, 'pos_bigrams:AUX CCONJ': 0.0, 'pos_bigrams:CCONJ AUX': 0.0, 'pos_bigrams:AUX DET': 0.0, 'pos_bigrams:DET AUX': 0.0, 'pos_bigrams:AUX INTJ': 0.0, 'pos_bigrams:INTJ AUX': 0.0, 'pos_bigrams:AUX NOUN': 0.0, 'pos_bigrams:NOUN AUX': 0.0, 'pos_bigrams:AUX NUM': 0.0, 'pos_bigrams:NUM AUX': 0.0, 'pos_bigrams:AUX PART': 0.0, 'pos_bigrams:PART AUX': 0.0, 'pos_bigrams:AUX PRON': 0.3333333333333333, 'pos_bigrams:PRON AUX': 0.0, 'pos_bigrams:AUX PROPN': 0.0, 'pos_bigrams:PROPN AUX': 0.0, 'pos_bigrams:AUX PUNCT': 0.0, 'pos_bigrams:PUNCT AUX': 0.0, 'pos_bigrams:AUX SCONJ': 0.0, 'pos_bigrams:SCONJ AUX': 0.0, 'pos_bigrams:AUX SYM': 0.0, 'pos_bigrams:SYM AUX': 0.0, 'pos_bigrams:AUX VERB': 0.0, 'pos_bigrams:VERB AUX': 0.0, 'pos_bigrams:AUX X': 0.0, 'pos_bigrams:X AUX': 0.0, 'pos_bigrams:AUX SPACE': 0.0, 'pos_bigrams:SPACE AUX': 0.0, 'pos_bigrams:CCONJ DET': 0.0, 'pos_bigrams:DET CCONJ': 0.0, 'pos_bigrams:CCONJ INTJ': 0.0, 'pos_bigrams:INTJ CCONJ': 0.0, 'pos_bigrams:CCONJ NOUN': 0.0, 'pos_bigrams:NOUN CCONJ': 0.0, 'pos_bigrams:CCONJ NUM': 0.0, 'pos_bigrams:NUM CCONJ': 0.0, 'pos_bigrams:CCONJ PART': 0.0, 'pos_bigrams:PART CCONJ': 0.0, 'pos_bigrams:CCONJ PRON': 0.0, 'pos_bigrams:PRON CCONJ': 0.0, 'pos_bigrams:CCONJ PROPN': 0.0, 'pos_bigrams:PROPN CCONJ': 0.0, 'pos_bigrams:CCONJ PUNCT': 0.0, 'pos_bigrams:PUNCT CCONJ': 0.0, 'pos_bigrams:CCONJ SCONJ': 0.0, 'pos_bigrams:SCONJ CCONJ': 0.0, 'pos_bigrams:CCONJ SYM': 0.0, 'pos_bigrams:SYM CCONJ': 0.0, 'pos_bigrams:CCONJ VERB': 0.0, 'pos_bigrams:VERB CCONJ': 0.0, 'pos_bigrams:CCONJ X': 0.0, 'pos_bigrams:X CCONJ': 0.0, 'pos_bigrams:CCONJ SPACE': 0.0, 'pos_bigrams:SPACE CCONJ': 0.0, 'pos_bigrams:DET INTJ': 0.0, 'pos_bigrams:INTJ DET': 0.0, 'pos_bigrams:DET NOUN': 0.0, 'pos_bigrams:NOUN DET': 0.0, 'pos_bigrams:DET NUM': 0.0, 'pos_bigrams:NUM DET': 0.0, 'pos_bigrams:DET PART': 0.0, 'pos_bigrams:PART DET': 0.0, 'pos_bigrams:DET PRON': 0.0, 'pos_bigrams:PRON DET': 0.0, 'pos_bigrams:DET PROPN': 0.0, 'pos_bigrams:PROPN DET': 0.0, 'pos_bigrams:DET PUNCT': 0.0, 'pos_bigrams:PUNCT DET': 0.0, 'pos_bigrams:DET SCONJ': 0.0, 'pos_bigrams:SCONJ DET': 0.0, 'pos_bigrams:DET SYM': 0.0, 'pos_bigrams:SYM DET': 0.0, 'pos_bigrams:DET VERB': 0.0, 'pos_bigrams:VERB DET': 0.0, 'pos_bigrams:DET X': 0.0, 'pos_bigrams:X DET': 0.0, 'pos_bigrams:DET SPACE': 0.0, 'pos_bigrams:SPACE DET': 0.0, 'pos_bigrams:INTJ NOUN': 0.0, 'pos_bigrams:NOUN INTJ': 0.0, 'pos_bigrams:INTJ NUM': 0.0, 'pos_bigrams:NUM INTJ': 0.0, 'pos_bigrams:INTJ PART': 0.0, 'pos_bigrams:PART INTJ': 0.0, 'pos_bigrams:INTJ PRON': 0.0, 'pos_bigrams:PRON INTJ': 0.0, 'pos_bigrams:INTJ PROPN': 0.0, 'pos_bigrams:PROPN INTJ': 0.0, 'pos_bigrams:INTJ PUNCT': 0.0, 'pos_bigrams:PUNCT INTJ': 0.0, 'pos_bigrams:INTJ SCONJ': 0.0, 'pos_bigrams:SCONJ INTJ': 0.0, 'pos_bigrams:INTJ SYM': 0.0, 'pos_bigrams:SYM INTJ': 0.0, 'pos_bigrams:INTJ VERB': 0.0, 'pos_bigrams:VERB INTJ': 0.0, 'pos_bigrams:INTJ X': 0.0, 'pos_bigrams:X INTJ': 0.0, 'pos_bigrams:INTJ SPACE': 0.0, 'pos_bigrams:SPACE INTJ': 0.0, 'pos_bigrams:NOUN NUM': 0.0, 'pos_bigrams:NUM NOUN': 0.0, 'pos_bigrams:NOUN PART': 0.0, 'pos_bigrams:PART NOUN': 0.0, 'pos_bigrams:NOUN PRON': 0.0, 'pos_bigrams:PRON NOUN': 0.3333333333333333, 'pos_bigrams:NOUN PROPN': 0.0, 'pos_bigrams:PROPN NOUN': 0.0, 'pos_bigrams:NOUN PUNCT': 0.0, 'pos_bigrams:PUNCT NOUN': 0.0, 'pos_bigrams:NOUN SCONJ': 0.0, 'pos_bigrams:SCONJ NOUN': 0.0, 'pos_bigrams:NOUN SYM': 0.0, 'pos_bigrams:SYM NOUN': 0.0, 'pos_bigrams:NOUN VERB': 0.0, 'pos_bigrams:VERB NOUN': 0.0, 'pos_bigrams:NOUN X': 0.0, 'pos_bigrams:X NOUN': 0.0, 'pos_bigrams:NOUN SPACE': 0.0, 'pos_bigrams:SPACE NOUN': 0.0, 'pos_bigrams:NUM PART': 0.0, 'pos_bigrams:PART NUM': 0.0, 'pos_bigrams:NUM PRON': 0.0, 'pos_bigrams:PRON NUM': 0.0, 'pos_bigrams:NUM PROPN': 0.0, 'pos_bigrams:PROPN NUM': 0.0, 'pos_bigrams:NUM PUNCT': 0.0, 'pos_bigrams:PUNCT NUM': 0.0, 'pos_bigrams:NUM SCONJ': 0.0, 'pos_bigrams:SCONJ NUM': 0.0, 'pos_bigrams:NUM SYM': 0.0, 'pos_bigrams:SYM NUM': 0.0, 'pos_bigrams:NUM VERB': 0.0, 'pos_bigrams:VERB NUM': 0.0, 'pos_bigrams:NUM X': 0.0, 'pos_bigrams:X NUM': 0.0, 'pos_bigrams:NUM SPACE': 0.0, 'pos_bigrams:SPACE NUM': 0.0, 'pos_bigrams:PART PRON': 0.0, 'pos_bigrams:PRON PART': 0.0, 'pos_bigrams:PART PROPN': 0.0, 'pos_bigrams:PROPN PART': 0.0, 'pos_bigrams:PART PUNCT': 0.0, 'pos_bigrams:PUNCT PART': 0.0, 'pos_bigrams:PART SCONJ': 0.0, 'pos_bigrams:SCONJ PART': 0.0, 'pos_bigrams:PART SYM': 0.0, 'pos_bigrams:SYM PART': 0.0, 'pos_bigrams:PART VERB': 0.0, 'pos_bigrams:VERB PART': 0.0, 'pos_bigrams:PART X': 0.0, 'pos_bigrams:X PART': 0.0, 'pos_bigrams:PART SPACE': 0.0, 'pos_bigrams:SPACE PART': 0.0, 'pos_bigrams:PRON PROPN': 0.0, 'pos_bigrams:PROPN PRON': 0.0, 'pos_bigrams:PRON PUNCT': 0.0, 'pos_bigrams:PUNCT PRON': 0.0, 'pos_bigrams:PRON SCONJ': 0.0, 'pos_bigrams:SCONJ PRON': 0.0, 'pos_bigrams:PRON SYM': 0.0, 'pos_bigrams:SYM PRON': 0.0, 'pos_bigrams:PRON VERB': 0.0, 'pos_bigrams:VERB PRON': 0.0, 'pos_bigrams:PRON X': 0.0, 'pos_bigrams:X PRON': 0.0, 'pos_bigrams:PRON SPACE': 0.0, 'pos_bigrams:SPACE PRON': 0.0, 'pos_bigrams:PROPN PUNCT': 0.0, 'pos_bigrams:PUNCT PROPN': 0.0, 'pos_bigrams:PROPN SCONJ': 0.0, 'pos_bigrams:SCONJ PROPN': 0.0, 'pos_bigrams:PROPN SYM': 0.0, 'pos_bigrams:SYM PROPN': 0.0, 'pos_bigrams:PROPN VERB': 0.0, 'pos_bigrams:VERB PROPN': 0.0, 'pos_bigrams:PROPN X': 0.0, 'pos_bigrams:X PROPN': 0.0, 'pos_bigrams:PROPN SPACE': 0.0, 'pos_bigrams:SPACE PROPN': 0.0, 'pos_bigrams:PUNCT SCONJ': 0.0, 'pos_bigrams:SCONJ PUNCT': 0.0, 'pos_bigrams:PUNCT SYM': 0.0, 'pos_bigrams:SYM PUNCT': 0.0, 'pos_bigrams:PUNCT VERB': 0.0, 'pos_bigrams:VERB PUNCT': 0.0, 'pos_bigrams:PUNCT X': 0.0, 'pos_bigrams:X PUNCT': 0.0, 'pos_bigrams:PUNCT SPACE': 0.0, 'pos_bigrams:SPACE PUNCT': 0.0, 'pos_bigrams:SCONJ SYM': 0.0, 'pos_bigrams:SYM SCONJ': 0.0, 'pos_bigrams:SCONJ VERB': 0.0, 'pos_bigrams:VERB SCONJ': 0.0, 'pos_bigrams:SCONJ X': 0.0, 'pos_bigrams:X SCONJ': 0.0, 'pos_bigrams:SCONJ SPACE': 0.0, 'pos_bigrams:SPACE SCONJ': 0.0, 'pos_bigrams:SYM VERB': 0.0, 'pos_bigrams:VERB SYM': 0.0, 'pos_bigrams:SYM X': 0.0, 'pos_bigrams:X SYM': 0.0, 'pos_bigrams:SYM SPACE': 0.0, 'pos_bigrams:SPACE SYM': 0.0, 'pos_bigrams:VERB X': 0.0, 'pos_bigrams:X VERB': 0.0, 'pos_bigrams:VERB SPACE': 0.0, 'pos_bigrams:SPACE VERB': 0.0, 'pos_bigrams:X SPACE': 0.0, 'pos_bigrams:SPACE X': 0.0, 'pos_bigrams:ADJ ADJ': 0.0, 'pos_bigrams:ADP ADP': 0.0, 'pos_bigrams:ADV ADV': 0.0, 'pos_bigrams:AUX AUX': 0.0, 'pos_bigrams:CCONJ CCONJ': 0.0, 'pos_bigrams:DET DET': 0.0, 'pos_bigrams:INTJ INTJ': 0.0, 'pos_bigrams:NOUN NOUN': 0.0, 'pos_bigrams:NUM NUM': 0.0, 'pos_bigrams:PART PART': 0.0, 'pos_bigrams:PRON PRON': 0.0, 'pos_bigrams:PROPN PROPN': 0.0, 'pos_bigrams:PUNCT PUNCT': 0.0, 'pos_bigrams:SCONJ SCONJ': 0.0, 'pos_bigrams:SYM SYM': 0.0, 'pos_bigrams:VERB VERB': 0.0, 'pos_bigrams:X X': 0.0, 'pos_bigrams:SPACE SPACE': 0.0, 'func_words:i': 0.0, 'func_words:me': 0.0, 'func_words:my': 0.0, 'func_words:myself': 0.0, 'func_words:we': 0.0, 'func_words:our': 0.5, 'func_words:ours': 0.0, 'func_words:ourselves': 0.0, 'func_words:you': 0.0, \"func_words:'re\": 0.0, \"func_words:'ve\": 0.0, \"func_words:'ll\": 0.0, \"func_words:'d\": 0.0, \"func_words:'s\": 0.0, \"func_words:'t\": 0.0, 'func_words:your': 0.0, 'func_words:yours': 0.0, 'func_words:yourself': 0.0, 'func_words:yourselves': 0.0, 'func_words:he': 0.0, 'func_words:him': 0.0, 'func_words:his': 0.0, 'func_words:himself': 0.0, 'func_words:she': 0.0, 'func_words:her': 0.0, 'func_words:ers': 0.0, 'func_words:herself': 0.0, 'func_words:it': 0.0, 'func_words:its': 0.0, 'func_words:itself': 0.0, 'func_words:they': 0.0, 'func_words:them': 0.0, 'func_words:their': 0.0, 'func_words:theirs': 0.0, 'func_words:themselves': 0.0, 'func_words:what': 0.0, 'func_words:which': 0.0, 'func_words:who': 0.0, 'func_words:this': 0.0, 'func_words:that': 0.0, 'func_words:these': 0.0, 'func_words:those': 0.0, 'func_words:am': 0.0, 'func_words:is': 0.5, 'func_words:are': 0.0, 'func_words:was': 0.0, 'func_words:were': 0.0, 'func_words:be': 0.0, 'func_words:been': 0.0, 'func_words:being': 0.0, 'func_words:have': 0.0, 'func_words:has': 0.0, 'func_words:had': 0.0, 'func_words:having': 0.0, 'func_words:do': 0.0, 'func_words:does': 0.0, 'func_words:did': 0.0, 'func_words:doing': 0.0, 'func_words:a': 0.0, 'func_words:an': 0.0, 'func_words:the': 0.0, 'func_words:and': 0.0, 'func_words:but': 0.0, 'func_words:if': 0.0, 'func_words:or': 0.0, 'func_words:because': 0.0, 'func_words:as': 0.0, 'func_words:until': 0.0, 'func_words:while': 0.0, 'func_words:of': 0.0, 'func_words:at': 0.0, 'func_words:by': 0.0, 'func_words:for': 0.0, 'func_words:with': 0.0, 'func_words:about': 0.0, 'func_words:against': 0.0, 'func_words:between': 0.0, 'func_words:into': 0.0, 'func_words:through': 0.0, 'func_words:during': 0.0, 'func_words:before': 0.0, 'func_words:after': 0.0, 'func_words:above': 0.0, 'func_words:below': 0.0, 'func_words:to': 0.0, 'func_words:from': 0.0, 'func_words:up': 0.0, 'func_words:down': 0.0, 'func_words:in': 0.0, 'func_words:out': 0.0, 'func_words:on': 0.0, 'func_words:off': 0.0, 'func_words:over': 0.0, 'func_words:under': 0.0, 'func_words:again': 0.0, 'func_words:further': 0.0, 'func_words:then': 0.0, 'func_words:once': 0.0, 'func_words:here': 0.0, 'func_words:there': 0.0, 'func_words:when': 0.0, 'func_words:where': 0.0, 'func_words:why': 0.0, 'func_words:how': 0.0, 'func_words:all': 0.0, 'func_words:any': 0.0, 'func_words:both': 0.0, 'func_words:each': 0.0, 'func_words:few': 0.0, 'func_words:more': 0.0, 'func_words:most': 0.0, 'func_words:other': 0.0, 'func_words:some': 0.0, 'func_words:such': 0.0, 'func_words:no': 0.0, 'func_words:nor': 0.0, 'func_words:not': 0.0, 'func_words:only': 0.0, 'func_words:own': 0.0, 'func_words:same': 0.0, 'func_words:so': 0.0, 'func_words:than': 0.0, 'func_words:too': 0.0, 'func_words:very': 0.0, 'func_words:can': 0.0, 'func_words:will': 0.0, 'func_words:just': 0.0, 'func_words:don': 0.0, 'func_words:should': 0.0, 'func_words:now': 0.0, 'func_words:ain': 0.0, 'func_words:aren': 0.0, 'func_words:couldn': 0.0, 'func_words:didn': 0.0, 'func_words:doesn': 0.0, 'func_words:hadn': 0.0, 'func_words:hasn': 0.0, 'func_words:haven': 0.0, 'func_words:isn': 0.0, 'func_words:ma': 0.0, 'func_words:shouldn': 0.0, 'func_words:wasn': 0.0, 'func_words:weren': 0.0, 'func_words:won': 0.0, 'func_words:wouldn': 0.0, 'punctuation:.': 0.0, 'punctuation:,': 0.0, 'punctuation::': 0.0, 'punctuation:;': 0.0, \"punctuation:'\": 0.0, 'punctuation:\"': 0.0, 'punctuation:?': 0.0, 'punctuation:!': 0.0, 'punctuation:`': 0.0, 'punctuation:*': 0.0, 'punctuation:&': 0.0, 'punctuation:_': 0.0, 'punctuation:-': 0.0, 'punctuation:%': 0.0, 'punctuation:(': 0.0, 'punctuation:)': 0.0, 'punctuation:–': 0.0, 'punctuation:‘': 0.0, 'punctuation:’': 0.0, 'emojis:😅': 0.0, 'emojis:😂': 0.0, 'emojis:😊': 0.0, 'emojis:❤️': 0.0, 'emojis:😭': 0.0, 'emojis:👍': 0.0, 'emojis:👌': 0.0, 'emojis:😍': 0.0, 'emojis:💕': 0.0, 'emojis:🥰': 0.0, 'dep_labels:ROOT': 0.0, 'dep_labels:acl': 0.0, 'dep_labels:acomp': 0.0, 'dep_labels:advcl': 0.0, 'dep_labels:advmod': 0.0, 'dep_labels:agent': 0.0, 'dep_labels:amod': 0.0, 'dep_labels:appos': 0.0, 'dep_labels:attr': 0.0, 'dep_labels:aux': 0.0, 'dep_labels:auxpass': 0.0, 'dep_labels:case': 0.0, 'dep_labels:cc': 0.0, 'dep_labels:ccomp': 0.0, 'dep_labels:compound': 0.0, 'dep_labels:conj': 0.0, 'dep_labels:csubj': 0.0, 'dep_labels:csubjpass': 0.0, 'dep_labels:dative': 0.0, 'dep_labels:dep': 0.0, 'dep_labels:det': 0.0, 'dep_labels:dobj': 0.0, 'dep_labels:expl': 0.0, 'dep_labels:intj': 0.0, 'dep_labels:mark': 0.0, 'dep_labels:meta': 0.0, 'dep_labels:neg': 0.0, 'dep_labels:nmod': 0.0, 'dep_labels:npadvmod': 0.0, 'dep_labels:nsubj': 0.0, 'dep_labels:nsubjpass': 0.0, 'dep_labels:nummod': 0.0, 'dep_labels:oprd': 0.0, 'dep_labels:parataxis': 0.0, 'dep_labels:pcomp': 0.0, 'dep_labels:pobj': 0.0, 'dep_labels:poss': 0.0, 'dep_labels:preconj': 0.0, 'dep_labels:predet': 0.0, 'dep_labels:prep': 0.0, 'dep_labels:prt': 0.0, 'dep_labels:punct': 0.0, 'dep_labels:quantmod': 0.0, 'dep_labels:relcl': 0.0, 'dep_labels:xcomp': 0.0, 'morph_tags:Aspect=Perf': 0.0, 'morph_tags:Aspect=Prog': 0.0, 'morph_tags:Case=Acc': 0.0, 'morph_tags:Case=Nom': 0.0, 'morph_tags:ConjType=Cmp': 0.0, 'morph_tags:Definite=Def': 0.0, 'morph_tags:Definite=Ind': 0.0, 'morph_tags:Degree=Cmp': 0.0, 'morph_tags:Degree=Pos': 0.0, 'morph_tags:Degree=Sup': 0.0, 'morph_tags:Foreign=Yes': 0.0, 'morph_tags:Gender=Fem': 0.0, 'morph_tags:Gender=Masc': 0.0, 'morph_tags:Gender=Neut': 0.0, 'morph_tags:Hyph=Yes': 0.0, 'morph_tags:Mood=Ind': 0.09090909090909091, 'morph_tags:NumType=Card': 0.0, 'morph_tags:NumType=Mult': 0.0, 'morph_tags:NumType=Ord': 0.0, 'morph_tags:Number=Plur': 0.09090909090909091, 'morph_tags:Number=Sing': 0.18181818181818182, 'morph_tags:Person=1': 0.09090909090909091, 'morph_tags:Person=2': 0.0, 'morph_tags:Person=3': 0.09090909090909091, 'morph_tags:Polarity=Neg': 0.0, 'morph_tags:Poss=Yes': 0.09090909090909091, 'morph_tags:PronType=Art': 0.0, 'morph_tags:PronType=Dem': 0.09090909090909091, 'morph_tags:PronType=Ind': 0.0, 'morph_tags:PronType=Prs': 0.09090909090909091, 'morph_tags:PronType=Rel': 0.0, 'morph_tags:PunctSide=Fin': 0.0, 'morph_tags:PunctSide=Ini': 0.0, 'morph_tags:PunctType=Brck': 0.0, 'morph_tags:PunctType=Comm': 0.0, 'morph_tags:PunctType=Dash': 0.0, 'morph_tags:PunctType=Peri': 0.0, 'morph_tags:PunctType=Quot': 0.0, 'morph_tags:Reflex=Yes': 0.0, 'morph_tags:Tense=Past': 0.0, 'morph_tags:Tense=Pres': 0.09090909090909091, 'morph_tags:VerbForm=Fin': 0.09090909090909091, 'morph_tags:VerbForm=Ger': 0.0, 'morph_tags:VerbForm=Inf': 0.0, 'morph_tags:VerbForm=Part': 0.0, 'morph_tags:VerbType=Mod': 0.0, 'sentences:it-cleft': 0.0, 'sentences:pseudo-cleft': 0.0, 'sentences:all-cleft': 0.0, 'sentences:there-cleft': 0.0, 'sentences:if-because-cleft': 0.0, 'sentences:passive': 0.0, 'sentences:subj-relcl': 0.0, 'sentences:obj-relcl': 0.0, 'sentences:tag-question': 0.0, 'sentences:coordinate-clause': 0.0}, 'emotion': {'neutral': 1, 'approval': 0, 'realization': 0, 'annoyance': 0, 'excitement': 0, 'optimism': 0, 'joy': 0, 'fear': 0, 'desire': 0, 'sadness': 0, 'disappointment': 0, 'confusion': 0, 'anger': 0, 'curiosity': 0, 'surprise': 0, 'amusement': 0, 'admiration': 0, 'disapproval': 0, 'disgust': 0, 'caring': 0, 'gratitude': 0, 'love': 0, 'nervousness': 0, 'embarrassment': 0, 'grief': 0, 'relief': 0, 'remorse': 0, 'pride': 0, 'unknown': 0}, 'emoji_task': ['😎'], 'emotion_task': ['optimism'], 'hate_task': ['not-hate'], 'irony_task': ['non_irony'], 'offensive_task': ['not-offensive'], 'sentiment_task': ['neutral'], 'casing': ['mixed'], 'morph_tags': {'morph_tags:Aspect=Perf': 0.0, 'morph_tags:Aspect=Prog': 0.0, 'morph_tags:Case=Acc': 0.0, 'morph_tags:Case=Nom': 0.0, 'morph_tags:ConjType=Cmp': 0.0, 'morph_tags:Definite=Def': 0.0, 'morph_tags:Definite=Ind': 0.0, 'morph_tags:Degree=Cmp': 0.0, 'morph_tags:Degree=Pos': 0.0, 'morph_tags:Degree=Sup': 0.0, 'morph_tags:Foreign=Yes': 0.0, 'morph_tags:Gender=Fem': 0.0, 'morph_tags:Gender=Masc': 0.0, 'morph_tags:Gender=Neut': 0.0, 'morph_tags:Hyph=Yes': 0.0, 'morph_tags:Mood=Ind': 0.09090909090909091, 'morph_tags:NumType=Card': 0.0, 'morph_tags:NumType=Mult': 0.0, 'morph_tags:NumType=Ord': 0.0, 'morph_tags:Number=Plur': 0.09090909090909091, 'morph_tags:Number=Sing': 0.18181818181818182, 'morph_tags:Person=1': 0.09090909090909091, 'morph_tags:Person=2': 0.0, 'morph_tags:Person=3': 0.09090909090909091, 'morph_tags:Polarity=Neg': 0.0, 'morph_tags:Poss=Yes': 0.09090909090909091, 'morph_tags:PronType=Art': 0.0, 'morph_tags:PronType=Dem': 0.09090909090909091, 'morph_tags:PronType=Ind': 0.0, 'morph_tags:PronType=Prs': 0.09090909090909091, 'morph_tags:PronType=Rel': 0.0, 'morph_tags:PunctSide=Fin': 0.0, 'morph_tags:PunctSide=Ini': 0.0, 'morph_tags:PunctType=Brck': 0.0, 'morph_tags:PunctType=Comm': 0.0, 'morph_tags:PunctType=Dash': 0.0, 'morph_tags:PunctType=Peri': 0.0, 'morph_tags:PunctType=Quot': 0.0, 'morph_tags:Reflex=Yes': 0.0, 'morph_tags:Tense=Past': 0.0, 'morph_tags:Tense=Pres': 0.09090909090909091, 'morph_tags:VerbForm=Fin': 0.09090909090909091, 'morph_tags:VerbForm=Ger': 0.0, 'morph_tags:VerbForm=Inf': 0.0, 'morph_tags:VerbForm=Part': 0.0, 'morph_tags:VerbType=Mod': 0.0}, 'punc_tags': {'punctuation:.': 0.0, 'punctuation:,': 0.0, 'punctuation::': 0.0, 'punctuation:;': 0.0, \"punctuation:'\": 0.0, 'punctuation:\"': 0.0, 'punctuation:?': 0.0, 'punctuation:!': 0.0, 'punctuation:`': 0.0, 'punctuation:*': 0.0, 'punctuation:&': 0.0, 'punctuation:_': 0.0, 'punctuation:-': 0.0, 'punctuation:%': 0.0, 'punctuation:(': 0.0, 'punctuation:)': 0.0, 'punctuation:–': 0.0, 'punctuation:‘': 0.0, 'punctuation:’': 0.0, 'none': 1}, 'pos_bigrams': {'pos_bigrams:ADJ ADP': 0.0, 'pos_bigrams:ADP ADJ': 0.0, 'pos_bigrams:ADJ ADV': 0.0, 'pos_bigrams:ADV ADJ': 0.0, 'pos_bigrams:ADJ AUX': 0.0, 'pos_bigrams:AUX ADJ': 0.0, 'pos_bigrams:ADJ CCONJ': 0.0, 'pos_bigrams:CCONJ ADJ': 0.0, 'pos_bigrams:ADJ DET': 0.0, 'pos_bigrams:DET ADJ': 0.0, 'pos_bigrams:ADJ INTJ': 0.0, 'pos_bigrams:INTJ ADJ': 0.0, 'pos_bigrams:ADJ NOUN': 0.0, 'pos_bigrams:NOUN ADJ': 0.0, 'pos_bigrams:ADJ NUM': 0.0, 'pos_bigrams:NUM ADJ': 0.0, 'pos_bigrams:ADJ PART': 0.0, 'pos_bigrams:PART ADJ': 0.0, 'pos_bigrams:ADJ PRON': 0.0, 'pos_bigrams:PRON ADJ': 0.0, 'pos_bigrams:ADJ PROPN': 0.0, 'pos_bigrams:PROPN ADJ': 0.0, 'pos_bigrams:ADJ PUNCT': 0.0, 'pos_bigrams:PUNCT ADJ': 0.0, 'pos_bigrams:ADJ SCONJ': 0.0, 'pos_bigrams:SCONJ ADJ': 0.0, 'pos_bigrams:ADJ SYM': 0.0, 'pos_bigrams:SYM ADJ': 0.0, 'pos_bigrams:ADJ VERB': 0.0, 'pos_bigrams:VERB ADJ': 0.0, 'pos_bigrams:ADJ X': 0.0, 'pos_bigrams:X ADJ': 0.0, 'pos_bigrams:ADJ SPACE': 0.0, 'pos_bigrams:SPACE ADJ': 0.0, 'pos_bigrams:ADP ADV': 0.0, 'pos_bigrams:ADV ADP': 0.0, 'pos_bigrams:ADP AUX': 0.0, 'pos_bigrams:AUX ADP': 0.0, 'pos_bigrams:ADP CCONJ': 0.0, 'pos_bigrams:CCONJ ADP': 0.0, 'pos_bigrams:ADP DET': 0.0, 'pos_bigrams:DET ADP': 0.0, 'pos_bigrams:ADP INTJ': 0.0, 'pos_bigrams:INTJ ADP': 0.0, 'pos_bigrams:ADP NOUN': 0.0, 'pos_bigrams:NOUN ADP': 0.0, 'pos_bigrams:ADP NUM': 0.0, 'pos_bigrams:NUM ADP': 0.0, 'pos_bigrams:ADP PART': 0.0, 'pos_bigrams:PART ADP': 0.0, 'pos_bigrams:ADP PRON': 0.0, 'pos_bigrams:PRON ADP': 0.0, 'pos_bigrams:ADP PROPN': 0.0, 'pos_bigrams:PROPN ADP': 0.0, 'pos_bigrams:ADP PUNCT': 0.0, 'pos_bigrams:PUNCT ADP': 0.0, 'pos_bigrams:ADP SCONJ': 0.0, 'pos_bigrams:SCONJ ADP': 0.0, 'pos_bigrams:ADP SYM': 0.0, 'pos_bigrams:SYM ADP': 0.0, 'pos_bigrams:ADP VERB': 0.0, 'pos_bigrams:VERB ADP': 0.0, 'pos_bigrams:ADP X': 0.0, 'pos_bigrams:X ADP': 0.0, 'pos_bigrams:ADP SPACE': 0.0, 'pos_bigrams:SPACE ADP': 0.0, 'pos_bigrams:ADV AUX': 0.3333333333333333, 'pos_bigrams:AUX ADV': 0.0, 'pos_bigrams:ADV CCONJ': 0.0, 'pos_bigrams:CCONJ ADV': 0.0, 'pos_bigrams:ADV DET': 0.0, 'pos_bigrams:DET ADV': 0.0, 'pos_bigrams:ADV INTJ': 0.0, 'pos_bigrams:INTJ ADV': 0.0, 'pos_bigrams:ADV NOUN': 0.0, 'pos_bigrams:NOUN ADV': 0.0, 'pos_bigrams:ADV NUM': 0.0, 'pos_bigrams:NUM ADV': 0.0, 'pos_bigrams:ADV PART': 0.0, 'pos_bigrams:PART ADV': 0.0, 'pos_bigrams:ADV PRON': 0.0, 'pos_bigrams:PRON ADV': 0.0, 'pos_bigrams:ADV PROPN': 0.0, 'pos_bigrams:PROPN ADV': 0.0, 'pos_bigrams:ADV PUNCT': 0.0, 'pos_bigrams:PUNCT ADV': 0.0, 'pos_bigrams:ADV SCONJ': 0.0, 'pos_bigrams:SCONJ ADV': 0.0, 'pos_bigrams:ADV SYM': 0.0, 'pos_bigrams:SYM ADV': 0.0, 'pos_bigrams:ADV VERB': 0.0, 'pos_bigrams:VERB ADV': 0.0, 'pos_bigrams:ADV X': 0.0, 'pos_bigrams:X ADV': 0.0, 'pos_bigrams:ADV SPACE': 0.0, 'pos_bigrams:SPACE ADV': 0.0, 'pos_bigrams:AUX CCONJ': 0.0, 'pos_bigrams:CCONJ AUX': 0.0, 'pos_bigrams:AUX DET': 0.0, 'pos_bigrams:DET AUX': 0.0, 'pos_bigrams:AUX INTJ': 0.0, 'pos_bigrams:INTJ AUX': 0.0, 'pos_bigrams:AUX NOUN': 0.0, 'pos_bigrams:NOUN AUX': 0.0, 'pos_bigrams:AUX NUM': 0.0, 'pos_bigrams:NUM AUX': 0.0, 'pos_bigrams:AUX PART': 0.0, 'pos_bigrams:PART AUX': 0.0, 'pos_bigrams:AUX PRON': 0.3333333333333333, 'pos_bigrams:PRON AUX': 0.0, 'pos_bigrams:AUX PROPN': 0.0, 'pos_bigrams:PROPN AUX': 0.0, 'pos_bigrams:AUX PUNCT': 0.0, 'pos_bigrams:PUNCT AUX': 0.0, 'pos_bigrams:AUX SCONJ': 0.0, 'pos_bigrams:SCONJ AUX': 0.0, 'pos_bigrams:AUX SYM': 0.0, 'pos_bigrams:SYM AUX': 0.0, 'pos_bigrams:AUX VERB': 0.0, 'pos_bigrams:VERB AUX': 0.0, 'pos_bigrams:AUX X': 0.0, 'pos_bigrams:X AUX': 0.0, 'pos_bigrams:AUX SPACE': 0.0, 'pos_bigrams:SPACE AUX': 0.0, 'pos_bigrams:CCONJ DET': 0.0, 'pos_bigrams:DET CCONJ': 0.0, 'pos_bigrams:CCONJ INTJ': 0.0, 'pos_bigrams:INTJ CCONJ': 0.0, 'pos_bigrams:CCONJ NOUN': 0.0, 'pos_bigrams:NOUN CCONJ': 0.0, 'pos_bigrams:CCONJ NUM': 0.0, 'pos_bigrams:NUM CCONJ': 0.0, 'pos_bigrams:CCONJ PART': 0.0, 'pos_bigrams:PART CCONJ': 0.0, 'pos_bigrams:CCONJ PRON': 0.0, 'pos_bigrams:PRON CCONJ': 0.0, 'pos_bigrams:CCONJ PROPN': 0.0, 'pos_bigrams:PROPN CCONJ': 0.0, 'pos_bigrams:CCONJ PUNCT': 0.0, 'pos_bigrams:PUNCT CCONJ': 0.0, 'pos_bigrams:CCONJ SCONJ': 0.0, 'pos_bigrams:SCONJ CCONJ': 0.0, 'pos_bigrams:CCONJ SYM': 0.0, 'pos_bigrams:SYM CCONJ': 0.0, 'pos_bigrams:CCONJ VERB': 0.0, 'pos_bigrams:VERB CCONJ': 0.0, 'pos_bigrams:CCONJ X': 0.0, 'pos_bigrams:X CCONJ': 0.0, 'pos_bigrams:CCONJ SPACE': 0.0, 'pos_bigrams:SPACE CCONJ': 0.0, 'pos_bigrams:DET INTJ': 0.0, 'pos_bigrams:INTJ DET': 0.0, 'pos_bigrams:DET NOUN': 0.0, 'pos_bigrams:NOUN DET': 0.0, 'pos_bigrams:DET NUM': 0.0, 'pos_bigrams:NUM DET': 0.0, 'pos_bigrams:DET PART': 0.0, 'pos_bigrams:PART DET': 0.0, 'pos_bigrams:DET PRON': 0.0, 'pos_bigrams:PRON DET': 0.0, 'pos_bigrams:DET PROPN': 0.0, 'pos_bigrams:PROPN DET': 0.0, 'pos_bigrams:DET PUNCT': 0.0, 'pos_bigrams:PUNCT DET': 0.0, 'pos_bigrams:DET SCONJ': 0.0, 'pos_bigrams:SCONJ DET': 0.0, 'pos_bigrams:DET SYM': 0.0, 'pos_bigrams:SYM DET': 0.0, 'pos_bigrams:DET VERB': 0.0, 'pos_bigrams:VERB DET': 0.0, 'pos_bigrams:DET X': 0.0, 'pos_bigrams:X DET': 0.0, 'pos_bigrams:DET SPACE': 0.0, 'pos_bigrams:SPACE DET': 0.0, 'pos_bigrams:INTJ NOUN': 0.0, 'pos_bigrams:NOUN INTJ': 0.0, 'pos_bigrams:INTJ NUM': 0.0, 'pos_bigrams:NUM INTJ': 0.0, 'pos_bigrams:INTJ PART': 0.0, 'pos_bigrams:PART INTJ': 0.0, 'pos_bigrams:INTJ PRON': 0.0, 'pos_bigrams:PRON INTJ': 0.0, 'pos_bigrams:INTJ PROPN': 0.0, 'pos_bigrams:PROPN INTJ': 0.0, 'pos_bigrams:INTJ PUNCT': 0.0, 'pos_bigrams:PUNCT INTJ': 0.0, 'pos_bigrams:INTJ SCONJ': 0.0, 'pos_bigrams:SCONJ INTJ': 0.0, 'pos_bigrams:INTJ SYM': 0.0, 'pos_bigrams:SYM INTJ': 0.0, 'pos_bigrams:INTJ VERB': 0.0, 'pos_bigrams:VERB INTJ': 0.0, 'pos_bigrams:INTJ X': 0.0, 'pos_bigrams:X INTJ': 0.0, 'pos_bigrams:INTJ SPACE': 0.0, 'pos_bigrams:SPACE INTJ': 0.0, 'pos_bigrams:NOUN NUM': 0.0, 'pos_bigrams:NUM NOUN': 0.0, 'pos_bigrams:NOUN PART': 0.0, 'pos_bigrams:PART NOUN': 0.0, 'pos_bigrams:NOUN PRON': 0.0, 'pos_bigrams:PRON NOUN': 0.3333333333333333, 'pos_bigrams:NOUN PROPN': 0.0, 'pos_bigrams:PROPN NOUN': 0.0, 'pos_bigrams:NOUN PUNCT': 0.0, 'pos_bigrams:PUNCT NOUN': 0.0, 'pos_bigrams:NOUN SCONJ': 0.0, 'pos_bigrams:SCONJ NOUN': 0.0, 'pos_bigrams:NOUN SYM': 0.0, 'pos_bigrams:SYM NOUN': 0.0, 'pos_bigrams:NOUN VERB': 0.0, 'pos_bigrams:VERB NOUN': 0.0, 'pos_bigrams:NOUN X': 0.0, 'pos_bigrams:X NOUN': 0.0, 'pos_bigrams:NOUN SPACE': 0.0, 'pos_bigrams:SPACE NOUN': 0.0, 'pos_bigrams:NUM PART': 0.0, 'pos_bigrams:PART NUM': 0.0, 'pos_bigrams:NUM PRON': 0.0, 'pos_bigrams:PRON NUM': 0.0, 'pos_bigrams:NUM PROPN': 0.0, 'pos_bigrams:PROPN NUM': 0.0, 'pos_bigrams:NUM PUNCT': 0.0, 'pos_bigrams:PUNCT NUM': 0.0, 'pos_bigrams:NUM SCONJ': 0.0, 'pos_bigrams:SCONJ NUM': 0.0, 'pos_bigrams:NUM SYM': 0.0, 'pos_bigrams:SYM NUM': 0.0, 'pos_bigrams:NUM VERB': 0.0, 'pos_bigrams:VERB NUM': 0.0, 'pos_bigrams:NUM X': 0.0, 'pos_bigrams:X NUM': 0.0, 'pos_bigrams:NUM SPACE': 0.0, 'pos_bigrams:SPACE NUM': 0.0, 'pos_bigrams:PART PRON': 0.0, 'pos_bigrams:PRON PART': 0.0, 'pos_bigrams:PART PROPN': 0.0, 'pos_bigrams:PROPN PART': 0.0, 'pos_bigrams:PART PUNCT': 0.0, 'pos_bigrams:PUNCT PART': 0.0, 'pos_bigrams:PART SCONJ': 0.0, 'pos_bigrams:SCONJ PART': 0.0, 'pos_bigrams:PART SYM': 0.0, 'pos_bigrams:SYM PART': 0.0, 'pos_bigrams:PART VERB': 0.0, 'pos_bigrams:VERB PART': 0.0, 'pos_bigrams:PART X': 0.0, 'pos_bigrams:X PART': 0.0, 'pos_bigrams:PART SPACE': 0.0, 'pos_bigrams:SPACE PART': 0.0, 'pos_bigrams:PRON PROPN': 0.0, 'pos_bigrams:PROPN PRON': 0.0, 'pos_bigrams:PRON PUNCT': 0.0, 'pos_bigrams:PUNCT PRON': 0.0, 'pos_bigrams:PRON SCONJ': 0.0, 'pos_bigrams:SCONJ PRON': 0.0, 'pos_bigrams:PRON SYM': 0.0, 'pos_bigrams:SYM PRON': 0.0, 'pos_bigrams:PRON VERB': 0.0, 'pos_bigrams:VERB PRON': 0.0, 'pos_bigrams:PRON X': 0.0, 'pos_bigrams:X PRON': 0.0, 'pos_bigrams:PRON SPACE': 0.0, 'pos_bigrams:SPACE PRON': 0.0, 'pos_bigrams:PROPN PUNCT': 0.0, 'pos_bigrams:PUNCT PROPN': 0.0, 'pos_bigrams:PROPN SCONJ': 0.0, 'pos_bigrams:SCONJ PROPN': 0.0, 'pos_bigrams:PROPN SYM': 0.0, 'pos_bigrams:SYM PROPN': 0.0, 'pos_bigrams:PROPN VERB': 0.0, 'pos_bigrams:VERB PROPN': 0.0, 'pos_bigrams:PROPN X': 0.0, 'pos_bigrams:X PROPN': 0.0, 'pos_bigrams:PROPN SPACE': 0.0, 'pos_bigrams:SPACE PROPN': 0.0, 'pos_bigrams:PUNCT SCONJ': 0.0, 'pos_bigrams:SCONJ PUNCT': 0.0, 'pos_bigrams:PUNCT SYM': 0.0, 'pos_bigrams:SYM PUNCT': 0.0, 'pos_bigrams:PUNCT VERB': 0.0, 'pos_bigrams:VERB PUNCT': 0.0, 'pos_bigrams:PUNCT X': 0.0, 'pos_bigrams:X PUNCT': 0.0, 'pos_bigrams:PUNCT SPACE': 0.0, 'pos_bigrams:SPACE PUNCT': 0.0, 'pos_bigrams:SCONJ SYM': 0.0, 'pos_bigrams:SYM SCONJ': 0.0, 'pos_bigrams:SCONJ VERB': 0.0, 'pos_bigrams:VERB SCONJ': 0.0, 'pos_bigrams:SCONJ X': 0.0, 'pos_bigrams:X SCONJ': 0.0, 'pos_bigrams:SCONJ SPACE': 0.0, 'pos_bigrams:SPACE SCONJ': 0.0, 'pos_bigrams:SYM VERB': 0.0, 'pos_bigrams:VERB SYM': 0.0, 'pos_bigrams:SYM X': 0.0, 'pos_bigrams:X SYM': 0.0, 'pos_bigrams:SYM SPACE': 0.0, 'pos_bigrams:SPACE SYM': 0.0, 'pos_bigrams:VERB X': 0.0, 'pos_bigrams:X VERB': 0.0, 'pos_bigrams:VERB SPACE': 0.0, 'pos_bigrams:SPACE VERB': 0.0, 'pos_bigrams:X SPACE': 0.0, 'pos_bigrams:SPACE X': 0.0, 'pos_bigrams:ADJ ADJ': 0.0, 'pos_bigrams:ADP ADP': 0.0, 'pos_bigrams:ADV ADV': 0.0, 'pos_bigrams:AUX AUX': 0.0, 'pos_bigrams:CCONJ CCONJ': 0.0, 'pos_bigrams:DET DET': 0.0, 'pos_bigrams:INTJ INTJ': 0.0, 'pos_bigrams:NOUN NOUN': 0.0, 'pos_bigrams:NUM NUM': 0.0, 'pos_bigrams:PART PART': 0.0, 'pos_bigrams:PRON PRON': 0.0, 'pos_bigrams:PROPN PROPN': 0.0, 'pos_bigrams:PUNCT PUNCT': 0.0, 'pos_bigrams:SCONJ SCONJ': 0.0, 'pos_bigrams:SYM SYM': 0.0, 'pos_bigrams:VERB VERB': 0.0, 'pos_bigrams:X X': 0.0, 'pos_bigrams:SPACE SPACE': 0.0}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  8.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from preprocess import clean_up_features, get_possible_values, extract_features\n",
    "\n",
    "if not os.path.exists(DATA_DIR_PATH):\n",
    "    with open(DATA_SRC, 'r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    \n",
    "    with open(SYNTHETIC_SRC, 'r') as in_file:\n",
    "       synthetic_data = json.load(in_file)\n",
    "    \n",
    "    # print(synthetic_data[:1])\n",
    "    \n",
    "    data = data + synthetic_data\n",
    "    \n",
    "    with open(SPLITS_PATH, 'r') as in_file:\n",
    "        SPLITS = json.load(in_file)\n",
    "    \n",
    "    # data.head(2)\n",
    "    for x in data:\n",
    "        clean_up_features(x)\n",
    "        \n",
    "    print(data[:1])\n",
    "    \n",
    "    meta_feature_to_names = {}\n",
    "    \n",
    "    for key in META_FEATURES:\n",
    "        meta_feature_to_names[key] = get_possible_values(data, key)\n",
    "    \n",
    "    extracted_features = [extract_features(d, meta_feature_to_names) for d in data]\n",
    "    \n",
    "    # extracted_features[2]\n",
    "    \n",
    "    training = []\n",
    "    holdout = []\n",
    "\n",
    "    author_labels = {'training':[], 'holdout':[]}\n",
    "\n",
    "    raw_text = {'training':[], 'holdout':[]}\n",
    "    \n",
    "    authors = [d['info']['from'] for d in data]\n",
    "    texts = [d['text'] for d in data]\n",
    "    for author, t, d  in zip(authors, texts, extracted_features):\n",
    "        if author.startswith('gpt') or author in SPLITS['train']:\n",
    "            training.append(d)\n",
    "            author_labels['training'].append(author)\n",
    "            raw_text['training'].append(t)\n",
    "            \n",
    "    \n",
    "        else:\n",
    "            holdout.append(d)\n",
    "            author_labels['holdout'].append(author)\n",
    "            assert author in SPLITS['test']\n",
    "            raw_text['holdout'].append(t)\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(max_df=MAX_DF, min_df=MIN_DF, stop_words='english')\n",
    "    vectorizer.fit([d['text'] for d in training])\n",
    "    \n",
    "    bows = {}\n",
    "\n",
    "    meta_vectorized = {}\n",
    "    \n",
    "    for split_name, data_split in zip(['training','holdout'], [training, holdout]):\n",
    "         bows[split_name] = vectorizer.transform([d['text'] for d in data_split])\n",
    "         meta_vectorized[split_name] = {}\n",
    "         for key in tqdm(sorted(data_split[0].keys())):\n",
    "            if key in ['text']: continue\n",
    "            meta_vectorized[split_name][key] = sparse.csr_matrix(np.stack([d[key] for d in data_split]))\n",
    "            assert(bows[split_name].shape[0] == meta_vectorized[split_name][key].shape[0])\n",
    "\n",
    "    os.makedirs(DATA_DIR_PATH, exist_ok=False)\n",
    "    pickle.dump(vectorizer, open(os.path.join(DATA_DIR_PATH,\"vectorizer.pickle\"), \"wb\"))\n",
    "    pickle.dump(bows, open(os.path.join(DATA_DIR_PATH,\"bows.pickle\"), \"wb\"))\n",
    "    pickle.dump(meta_vectorized, open(os.path.join(DATA_DIR_PATH,\"meta_vectorized.pickle\"), \"wb\"))\n",
    "    \n",
    "    json.dump(raw_text, open(os.path.join(DATA_DIR_PATH,\"raw_text.json\"), \"w\"))\n",
    "    json.dump(author_labels, open(os.path.join(DATA_DIR_PATH,\"authors_json.json\"), \"w\"))\n",
    "    json.dump(meta_feature_to_names, open(os.path.join(DATA_DIR_PATH,\"meta_feature_to_names.json\"), \"w\"))\n",
    "    \n",
    "else:\n",
    "    print(f\"LOADING FROM {DATA_DIR_PATH}\")\n",
    "    with open(os.path.join(DATA_DIR_PATH, 'bows.pickle'), 'rb') as in_file:\n",
    "        bows = pickle.load(in_file)\n",
    "        \n",
    "    with open(os.path.join(DATA_DIR_PATH, 'meta_vectorized.pickle'), 'rb') as in_file:\n",
    "        meta_vectorized = pickle.load(in_file)    \n",
    "    \n",
    "    with open(os.path.join(DATA_DIR_PATH,\"vectorizer.pickle\"), 'rb') as in_file:\n",
    "        vectorizer = pickle.load(in_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97291dbb-5aa9-4612-8961-a629cfa16c13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fa09ece7-3827-47a7-b7fc-a441a589298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DOCS   = bows['training'].shape[0]\n",
    "VOCAB_SIZE = bows['training'].shape[1]\n",
    "META_SIZE  = {k:v.shape[1] for k,v in meta_vectorized['training'].items()}\n",
    "IDX_TO_TERM = {v:k for k,v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "14566e36-f4aa-4812-8633-d6e6c638f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Dims: (66668, 3098)\n",
      "META Dims: {'pos_bigrams': 324}\n"
     ]
    }
   ],
   "source": [
    "print(f'BOW Dims: {bows[\"training\"].shape}')\n",
    "print(f'META Dims: {META_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e3a9e-420d-4b50-887a-6e00b5ec98f7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "eae84587-4202-4136-a98a-8cc5f52b4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodslda_cls import DocMetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e16dec22-8379-430b-8f25-5675e8ef8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DocMetaData(bows['training'], meta_vectorized['training'])\n",
    "eval_dataset = DocMetaData(bows['holdout'], meta_vectorized['holdout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e231698c-1fd3-48de-98ee-43843613b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle=True)\n",
    "eval_dl = DataLoader(eval_dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "eb31ab8a-2164-4454-93be-ea3fb6899331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow torch.Size([128, 3098])\n",
      "meta\n",
      "\tpos_bigrams torch.Size([128, 324])\n"
     ]
    }
   ],
   "source": [
    "test_ = next(iter(dl))\n",
    "for k, v in test_.items():\n",
    "    if isinstance(v, dict):\n",
    "        print(k)\n",
    "        for k1, v1 in v.items():\n",
    "             print('\\t'+k1, v1.shape)\n",
    "    else:\n",
    "        print(k, v.shape)\n",
    "            \n",
    "    # print(f'{k:7s}: {str(v.shape):25s} {str(v.dtype):25s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b0ce8-9814-431f-8ecb-f557327c933d",
   "metadata": {},
   "source": [
    "# Model Definition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3b961-5983-4575-af2a-b5fe5ca56f84",
   "metadata": {},
   "source": [
    "## ProdSLDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d7d033aa-6189-415b-8d37-80ca01b8dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prodslda_cls import ProdSLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3896ff-e21e-491b-81a9-506b2ad6b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocMetaData(Dataset):\n",
    "    \n",
    "    def __init__(self, bows, metas, dtype = np.float32):\n",
    "        self.bows = bows\n",
    "        self.metas = metas\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.bows.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        bow = self.bows[idx].toarray().astype(self.dtype)[0]\n",
    "        \n",
    "        meta = {key:self.metas[key][idx].toarray().astype(self.dtype)[0] for key in self.metas}\n",
    "        \n",
    "        batch = {\n",
    "            'bow': bow,\n",
    "            'meta': meta,\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "class GeneralEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, size_dict, num_styles, hidden, dropout, eps = 1e-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eps = eps\n",
    "\n",
    "        self.sizes = size_dict\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        \n",
    "        # self.fc1_doc = nn.Linear(vocab_size, hidden)\n",
    "        # self.fc1_meta = nn.Linear(meta_size, hidden)\n",
    "\n",
    "        self.features = sorted(size_dict.keys())\n",
    "        assert len(self.features) == 1\n",
    "        # self.fc1s = nn.ModuleDict({feature:nn.Linear(self.sizes[feature], hidden) for feature in self.features})\n",
    "        self.fc1s = nn.Linear(self.sizes[self.features[0]], hidden)\n",
    "        self.fc2 = nn.Linear(len(self.features) * hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_styles)\n",
    "        self.fclv = nn.Linear(hidden, num_styles)\n",
    "\n",
    "        self.bnmu = nn.BatchNorm1d(num_styles, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_styles, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs): #inputs_doc, inputs_meta):\n",
    "        assert isinstance(inputs, dict)\n",
    "        first_hiddens = []\n",
    "        # for _, feature in enumerate(self.features):\n",
    "        first_hiddens.append(F.relu(self.fc1s[self.features[0]](inputs[self.features[0]])))\n",
    "        \n",
    "        h = torch.hstack(first_hiddens)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # μ and Σ are the outputs\n",
    "        logkappa_loc = self.bnmu(self.fcmu(h))\n",
    "        logkappa_logvar = self.bnlv(self.fclv(h))\n",
    "        logkappa_scale = self.eps + (0.5 * logkappa_logvar).exp()  # Enforces positivity\n",
    "        return logkappa_loc, logkappa_scale\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.drop(inputs)\n",
    "        # the output is σ(βθ)\n",
    "        return self.bn(self.beta(inputs))\n",
    "    \n",
    "class MetaDocDecoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, num_styles, dropout):\n",
    "        super().__init__()\n",
    "        self.t_beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.s_beta = nn.Linear(num_styles, vocab_size, bias=False)\n",
    "        self.t_bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.s_bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs_doc, inputs_meta):\n",
    "        inputs_doc  = self.drop(inputs_doc)\n",
    "        inputs_meta = self.drop(inputs_meta)\n",
    "        # the output is σ(βθ)\n",
    "        \n",
    "        dist_t = self.t_bn(self.t_beta(inputs_doc))\n",
    "        dist_s = self.s_bn(self.s_beta(inputs_meta))\n",
    "        \n",
    "        return 0.5 * (dist_t + dist_s)\n",
    "\n",
    "class ProdSLDA(nn.Module):\n",
    "    \n",
    "    PRIOR_DISTS  = {'gaussian': dist.Normal,\n",
    "                    'laplace': dist.Laplace,\n",
    "                   }\n",
    "    TK_LINKS     = ('none', # Model style and documents independently\n",
    "                    'kappa_doc', # Allow kappa to effect word distributions\n",
    "                    'kappa_doc_style', # Allow kappa to effect word distributions and sampled words to effect style\n",
    "                   )\n",
    "    \n",
    "    def __init__(self, vocab_size, meta_sizes, num_topics, num_styles, hidden, dropout, \n",
    "                 theta_prior_dist = 'gaussian', theta_prior_loc = 0., theta_prior_scale = 1.,\n",
    "                 kappa_prior_dist = 'laplace', kappa_prior_loc = 0., kappa_prior_scale = 1.,\n",
    "                 style_topic_link = 'none',\n",
    "                 eps = 1e-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Global model variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.meta_sizes = meta_sizes\n",
    "        self.num_topics = num_topics\n",
    "        self.num_styles = num_styles\n",
    "        self.hidden     = hidden\n",
    "        self.dropout    = dropout\n",
    "\n",
    "        self.meta_features = sorted(self.meta_sizes.keys())\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "        # Theta Prior\n",
    "        if theta_prior_dist not in ProdSLDA.PRIOR_DISTS.keys():\n",
    "            raise ValueError(f'Theta prior {theta_prior_dist} not yet implemented. Must be one of {\", \".join(ProdSLDA.PRIOR_DISTS.keys())}')\n",
    "        self.theta_prior_dist = theta_prior_dist\n",
    "        \n",
    "        self.theta_prior_scale = theta_prior_scale\n",
    "        self.theta_prior_loc = theta_prior_loc\n",
    "        \n",
    "        # Kappa Prior\n",
    "        if kappa_prior_dist not in ProdSLDA.PRIOR_DISTS.keys():\n",
    "            raise ValueError(f'Kappa prior {kappa_prior_dist} not yet implemented. Must be one of {\", \".join(ProdSLDA.PRIOR_DISTS.keys())}')\n",
    "        self.kappa_prior_dist = kappa_prior_dist\n",
    "        \n",
    "        self.kappa_prior_scale = kappa_prior_scale\n",
    "        self.kappa_prior_loc = kappa_prior_loc\n",
    "        \n",
    "        \n",
    "        # Document style linking\n",
    "        self.style_topic_link = style_topic_link\n",
    "        \n",
    "        if self.style_topic_link not in ProdSLDA.TK_LINKS:\n",
    "            raise ValueError(f'Link {self.style_topic_link} not yet implemented. Must be one of {\", \".join(ProdSLDA.TK_LINKS)}')\n",
    "        elif self.style_topic_link == 'none':\n",
    "            # Independent modeling of style and topic, all normal encoder/decoders\n",
    "            \n",
    "            self.encoder = GeneralEncoder({'doc':vocab_size}, num_topics, hidden, dropout, self.eps)\n",
    "            self.decoder = Decoder(vocab_size, num_topics, dropout)\n",
    "            self.style_encoder = GeneralEncoder(meta_sizes, num_styles, hidden, dropout, self.eps)\n",
    "            self.style_decoder = nn.ModuleDict({feature: Decoder(meta_s, num_styles, dropout) for feature, meta_s in meta_sizes.items()})\n",
    "            \n",
    "        elif self.style_topic_link == 'kappa_doc':\n",
    "            # raise NotImplementedError()\n",
    "            # Doc influences kappa encoding, style encoder takes in doc\n",
    "            self.encoder = GeneralEncoder({'doc':vocab_size}, num_styles, hidden, dropout, self.eps)\n",
    "            self.style_encoder = GeneralEncoder({'doc':vocab_size, **meta_sizes}, num_styles, hidden, dropout, self.eps)\n",
    "\n",
    "            self.decoder = MetaDocDecoder(vocab_size=vocab_size, num_topics=num_topics, num_styles=num_styles, dropout=dropout)\n",
    "            self.style_decoder = nn.ModuleDict({feature: Decoder(meta_s, num_styles, dropout) for feature, meta_s in meta_sizes.items()})\n",
    "\n",
    "\n",
    "    def model(self, docs, metas):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        pyro.module(\"style_decoder\", self.style_decoder)\n",
    "        \n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics)) * self.theta_prior_loc\n",
    "            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics)) * self.theta_prior_scale\n",
    "            logkappa_loc = docs.new_zeros((docs.shape[0], self.num_styles)) * self.kappa_prior_loc\n",
    "            logkappa_scale = docs.new_ones((docs.shape[0], self.num_styles)) * self.kappa_prior_scale\n",
    "            \n",
    "            # if self.style_topic_link == 'kappa_doc':\n",
    "                # logtheta_s_loc = docs.new_zeros((docs.shape[0], self.num_topics)) * self.theta_prior_loc\n",
    "                # logtheta_s_scale = docs.new_ones((docs.shape[0], self.num_topics)) * self.theta_prior_scale\n",
    "                \n",
    "                # logtheta_s = pyro.sample(\n",
    "                    # \"logtheta_s\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_s_loc, logtheta_s_scale).to_event(1))\n",
    "                \n",
    "                # theta_s = F.softmax(logtheta_s, -1)\n",
    "            \n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_loc, logtheta_scale).to_event(1))\n",
    "            logkappa = pyro.sample(\n",
    "                \"logkappa\", ProdSLDA.PRIOR_DISTS[self.kappa_prior_dist](logkappa_loc, logkappa_scale).to_event(1))\n",
    "            \n",
    "            theta = F.softmax(logtheta, -1)\n",
    "            kappa = F.softmax(logkappa, -1)\n",
    "\n",
    "            if self.style_topic_link == 'none':\n",
    "                word_logits = self.decoder(theta)\n",
    "            elif self.style_topic_link == 'kappa_doc':\n",
    "                word_logits = self.decoder(theta, kappa)\n",
    "                \n",
    "            style_logits = {feature:self.style_decoder[feature](kappa) for feature in self.meta_features}\n",
    "\n",
    "            total_count = int(docs.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs_doc',\n",
    "                dist.Multinomial(total_count, logits = word_logits),\n",
    "                obs=docs\n",
    "            )\n",
    "\n",
    "            # for feature in self.meta_features:\n",
    "            #     total_s_count = int(metas[feature].sum(-1).max())\n",
    "            #     pyro.sample(\n",
    "            #         'obs_meta_'+feature,\n",
    "            #         dist.Multinomial(total_s_count, logits = style_logits[feature]),\n",
    "            #         obs=metas[feature]\n",
    "            #     )\n",
    "\n",
    "    def guide(self, docs, metas):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        pyro.module(\"style_encoder\", self.style_encoder)\n",
    "            \n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution,\n",
    "            # where μ and Σ are the encoder network outputs\n",
    "            \n",
    "            if self.style_topic_link == 'none':\n",
    "                logtheta_loc, logtheta_scale = self.encoder({'doc':docs})\n",
    "                logkappa_loc, logkappa_scale = self.style_encoder(metas)\n",
    "\n",
    "            elif self.style_topic_link == 'kappa_doc':\n",
    "                # raise NotImplementedError()\n",
    "                # logtheta_loc, logtheta_scale, logkappa_d_loc, logkappa_d_scale = self.encoder({'doc':docs, **metas})\n",
    "                logtheta_loc, logtheta_scale  = self.encoder({'doc':docs})\n",
    "                logkappa_loc, logkappa_scale = self.style_encoder({'doc':docs, **metas})\n",
    "\n",
    "                # NICK what was the point of d_loc and d_scale? Shoudln't we be generating one set of kappas from both features?\n",
    "                \n",
    "                # Average theta loc from document and style\n",
    "                # logkappa_loc = 0.5 * (logkappa_loc + logkappa_d_loc) \n",
    "                # logkappa_scale = 0.5 * (logkappa_scale + logkappa_d_scale)\n",
    "            \n",
    "            # Sample logtheta/logkappa from guide\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_loc, logtheta_scale).to_event(1))\n",
    "            logkappa = pyro.sample(\n",
    "                \"logkappa\", ProdSLDA.PRIOR_DISTS[self.kappa_prior_dist](logkappa_loc, logkappa_scale).to_event(1))\n",
    "\n",
    "        # return logtheta, logkappa\n",
    "            \n",
    "\n",
    "    def beta_document(self):\n",
    "        if self.style_topic_link == 'none':\n",
    "            return {'beta_topic':self.decoder.beta.weight.cpu().detach().T}\n",
    "        elif self.style_topic_link == 'kappa_doc':\n",
    "            return {\n",
    "                'beta_topic':self.decoder.t_beta.weight.cpu().detach().T,\n",
    "                'beta_style':self.decoder.s_beta.weight.cpu().detach().T,\n",
    "            }\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    def beta_meta(self):\n",
    "        # beta matrix elements are the weights of the FC layer on the decoder\n",
    "        # return self.decoder.beta.weight.cpu().detach().T\n",
    "        retval = {}\n",
    "        for key, layer in self.style_decoder.items():\n",
    "            retval[key] = layer.beta.weight.cpu().detach().T\n",
    "        return retval\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93050dff-fe64-4737-862e-2e6baa0af03e",
   "metadata": {},
   "source": [
    "# Model Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714576bd-b9ff-48ad-b297-62471689e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 20\n",
    "# BATCH_SIZE = \n",
    "# NUM_STYLES = 2\n",
    "# DROPOUT    = 0.\n",
    "HIDDEN_DIM = 32\n",
    "LR = 1e10\n",
    "\n",
    "THETA_PRIOR_SCALE = 1\n",
    "# KAPPA_PRIOR_SCALE = 3\n",
    "\n",
    "CLIP_NORM = 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed443b9-6ebe-4351-aeb7-5c85caff576b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5cb86678-f33a-49a1-971c-1641b2da6e66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta_topic': tensor([[ 0.1728,  0.2147,  0.0582,  ..., -0.1210,  0.0876,  0.1361],\n",
      "        [ 0.1905,  0.0670,  0.1940,  ...,  0.1566, -0.0453, -0.0014],\n",
      "        [ 0.1763,  0.1895, -0.1289,  ..., -0.0864,  0.2122, -0.1905],\n",
      "        ...,\n",
      "        [-0.0557, -0.1346,  0.0220,  ..., -0.0825, -0.0639, -0.0241],\n",
      "        [ 0.0623,  0.0225, -0.1864,  ..., -0.1321,  0.1941,  0.0331],\n",
      "        [-0.1032,  0.1634, -0.1719,  ..., -0.0803, -0.1497, -0.2229]])}\n",
      "{'pos_bigrams': tensor([[-0.0282, -0.3210, -0.0408,  ..., -0.4412,  0.2576,  0.3431],\n",
      "        [ 0.0145,  0.2799,  0.2085,  ...,  0.0272,  0.4044, -0.2026],\n",
      "        [-0.3757,  0.3128, -0.2860,  ..., -0.3508,  0.3782, -0.3976],\n",
      "        [ 0.2708, -0.3604,  0.3101,  ..., -0.3844,  0.1634,  0.1410],\n",
      "        [-0.4264,  0.0748, -0.2532,  ...,  0.2403, -0.1511,  0.3345]])}\n"
     ]
    }
   ],
   "source": [
    "prod_slda = ProdSLDA(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    meta_sizes  = META_SIZE,\n",
    "    num_topics = NUM_TOPICS, num_styles = NUM_STYLES, \n",
    "    hidden = HIDDEN_DIM, dropout = DROPOUT, \n",
    "    theta_prior_dist = THETA_PRIOR_DIST, \n",
    "    theta_prior_loc = THETA_PRIOR_LOC, theta_prior_scale = THETA_PRIOR_SCALE,\n",
    "    kappa_prior_dist = KAPPA_PRIOR_DIST, \n",
    "    kappa_prior_loc = KAPPA_PRIOR_LOC, kappa_prior_scale = KAPPA_PRIOR_SCALE,\n",
    "    style_topic_link = STYLE_TOPIC_LINK,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(prod_slda.beta_document())\n",
    "print(prod_slda.beta_meta())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf790d2c-25ba-4769-8703-5b9e672a735f",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4487f140-7c91-4b84-9404-af5d21b233d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pos_bigrams': tensor([[-1.8304e+00, -1.8904e+00, -2.0618e+00,  ..., -1.5975e+00,\n",
       "          -2.6415e+00, -1.9489e+00],\n",
       "         [ 7.8684e-01,  1.0342e+00,  1.1435e+00,  ...,  1.0513e+00,\n",
       "           1.1998e+00,  1.0992e+00],\n",
       "         [ 8.8633e-01,  1.1214e+00,  1.3023e+00,  ...,  1.2147e+00,\n",
       "           1.2440e+00,  1.1934e+00],\n",
       "         [-4.2701e-01, -2.5076e-01, -3.2173e-01,  ..., -1.0346e-01,\n",
       "          -5.0079e-01, -2.3791e-01],\n",
       "         [-2.4423e-03,  1.2616e-01,  1.4202e-01,  ...,  2.4994e-01,\n",
       "           1.5193e-02,  2.2705e-01]])}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "prod_slda.train()\n",
    "print(DEVICE)\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2cc93883-15f9-411c-98b0-851ee1f1390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = pyro.optim.ClippedAdam(ADAM_ARGS)\n",
    "\n",
    "svi = SVI(\n",
    "    pyro.poutine.scale(prod_slda.model, BATCH_SIZE/len(dl.dataset)),\n",
    "    pyro.poutine.scale(prod_slda.model, BATCH_SIZE/len(dl.dataset)),\n",
    "    optim,\n",
    "    loss = TraceMeanField_ELBO()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4f1bea62-f21c-46d2-9963-9a2b4ee9e90c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Trace Shapes:            \n",
      "                           Param Sites:            \n",
      "              encoder$$$fc1s.doc.weight  32 3098   \n",
      "                encoder$$$fc1s.doc.bias       32   \n",
      "                   encoder$$$fc2.weight  32   32   \n",
      "                     encoder$$$fc2.bias       32   \n",
      "                  encoder$$$fcmu.weight  20   32   \n",
      "                    encoder$$$fcmu.bias       20   \n",
      "                  encoder$$$fclv.weight  20   32   \n",
      "                    encoder$$$fclv.bias       20   \n",
      "style_encoder$$$fc1s.pos_bigrams.weight  32  324   \n",
      "  style_encoder$$$fc1s.pos_bigrams.bias       32   \n",
      "             style_encoder$$$fc2.weight  32   32   \n",
      "               style_encoder$$$fc2.bias       32   \n",
      "            style_encoder$$$fcmu.weight   5   32   \n",
      "              style_encoder$$$fcmu.bias        5   \n",
      "            style_encoder$$$fclv.weight   5   32   \n",
      "              style_encoder$$$fclv.bias        5   \n",
      "                          Sample Sites:            \n",
      "                         documents dist        |   \n",
      "                                  value 128    |   \n",
      "                          logtheta dist 128    | 20\n",
      "                                  value 128    | 20\n",
      "                          logkappa dist 128    |  5\n",
      "                                  value 128    |  5\n"
     ]
    }
   ],
   "source": [
    "print(pyro.poutine.trace(prod_slda.guide).get_trace(test_['bow'].to(DEVICE), {k:v.to(DEVICE) for k,v in test_['meta'].items()}).format_shapes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7323f1de-8558-482e-91f2-4e05b4c822d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a08d6d3d-f8a6-412d-9abd-3510ccda0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c37b2-22a8-463a-9c4c-2aaec9f4e538",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TRAINING---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:47<00:00, 10.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:22<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6335.602\n",
      "Epoch 0: Eval: 2070.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:40<00:00, 12.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:16<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 6189.974\n",
      "Epoch 1: Eval: 2026.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [01:10<00:00,  7.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:10<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 6081.159\n",
      "Epoch 2: Eval: 2007.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:37<00:00, 13.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:15<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 6048.905\n",
      "Epoch 3: Eval: 1997.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:51<00:00, 10.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:11<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 6039.785\n",
      "Epoch 4: Eval: 1998.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:42<00:00, 12.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:09<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 6039.180\n",
      "Epoch 5: Eval: 1997.353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:41<00:00, 12.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:13<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 6035.003\n",
      "Epoch 6: Eval: 1998.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:47<00:00, 10.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:09<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 6033.626\n",
      "Epoch 7: Eval: 1995.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:33<00:00, 15.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:08<00:00, 21.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 6033.291\n",
      "Epoch 8: Eval: 1996.020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:27<00:00, 18.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:08<00:00, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 6030.642\n",
      "Epoch 9: Eval: 1995.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:32<00:00, 16.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:07<00:00, 24.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 6031.703\n",
      "Epoch 10: Eval: 1995.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:36<00:00, 14.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:08<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 6030.449\n",
      "Epoch 11: Eval: 1994.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 521/521 [00:32<00:00, 16.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:11<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 6029.111\n",
      "Epoch 12: Eval: 1996.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████▋                                                                      | 147/521 [00:08<00:08, 42.84it/s]"
     ]
    }
   ],
   "source": [
    "train_elbo = []\n",
    "val_elbo = []\n",
    "\n",
    "print(\"---TRAINING---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    epoch_elbo = 0.\n",
    "    prod_slda.train()\n",
    "    for batch in tqdm(dl):\n",
    "        \n",
    "        bow = batch['bow'].to(DEVICE)\n",
    "        bop = {k:v.to(DEVICE) for k,v in batch['meta'].items()}\n",
    "        \n",
    "        epoch_elbo += svi.step(bow, bop)\n",
    "\n",
    "    eval_elbo = 0\n",
    "    prod_slda.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dl):\n",
    "            bow = batch['bow'].to(DEVICE)\n",
    "            bop = {k:v.to(DEVICE) for k,v in batch['meta'].items()}\n",
    "    \n",
    "            # NICK: I think this may still be optimizing on holdout?\n",
    "            eval_elbo += svi.evaluate_loss(bow, bop)\n",
    "    \n",
    "        \n",
    "    print(f'Epoch {epoch}: {epoch_elbo:.3f}')\n",
    "    print(f'Epoch {epoch}: Eval: {eval_elbo:.3f}')\n",
    "    \n",
    "    train_elbo.append(epoch_elbo)\n",
    "    val_elbo.append(eval_elbo)\n",
    "    total_epochs +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "310e69d2-833d-41c4-a81c-e1b92c3f8e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_elbo)), [e/len(dl.dataset) for e in train_elbo])\n",
    "plt.plot(range(len(val_elbo)), [e/len(eval_dl.dataset) for e in val_elbo])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb88f5-5208-44eb-a101-bc489ddcc988",
   "metadata": {},
   "source": [
    "# Examine Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "58cf7aa1-d1d4-4436-92c2-cf204af3288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbeta = prod_slda.beta_document()['beta_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "9ca5e99a-a589-40bf-abd0-5016f747de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = (-tbeta).sort().indices[:,:10].numpy()\n",
    "top_weights = -(-tbeta).sort().values[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "e6eb32bf-3c05-435b-a150-be741f02c1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1217, -2.1740, -2.2175, -2.2197, -2.2307, -2.2520, -2.2545, -2.2796,\n",
       "         -2.2939, -2.3109],\n",
       "        [ 1.7339,  1.6954,  1.6844,  1.6297,  1.6185,  1.5475,  1.5026,  1.4974,\n",
       "          1.4937,  1.4849],\n",
       "        [ 0.7961,  0.5104,  0.4615,  0.3790,  0.3368,  0.2915,  0.2889,  0.2520,\n",
       "          0.1922,  0.1887],\n",
       "        [ 0.2463,  0.2086,  0.1662,  0.1149,  0.1054,  0.1006,  0.0962,  0.0273,\n",
       "          0.0064,  0.0053],\n",
       "        [ 1.5790,  1.5733,  1.5476,  1.5344,  1.5073,  1.5060,  1.4974,  1.4901,\n",
       "          1.4753,  1.4732],\n",
       "        [ 1.9598,  1.9237,  1.8618,  1.7630,  1.7442,  1.7156,  1.7027,  1.6913,\n",
       "          1.6764,  1.6728],\n",
       "        [-0.0545, -0.0759, -0.1108, -0.1169, -0.1391, -0.1554, -0.1726, -0.1836,\n",
       "         -0.1848, -0.1884],\n",
       "        [ 0.5430, -0.1229, -0.1594, -0.2012, -0.2028, -0.2153, -0.2283, -0.2347,\n",
       "         -0.2373, -0.2547],\n",
       "        [ 0.8899,  0.8130,  0.7263,  0.6984,  0.6883,  0.6879,  0.6877,  0.6765,\n",
       "          0.6506,  0.6380],\n",
       "        [ 0.6902,  0.5979,  0.5784,  0.5613,  0.5165,  0.5006,  0.4398,  0.4237,\n",
       "          0.4112,  0.4078],\n",
       "        [ 1.5915,  1.3298,  1.2407,  1.2386,  1.2365,  1.2353,  1.2171,  1.2126,\n",
       "          1.2074,  1.2057],\n",
       "        [ 2.2675,  2.2506,  2.1531,  2.1319,  2.1190,  2.0960,  2.0888,  2.0616,\n",
       "          2.0478,  2.0161],\n",
       "        [ 0.4270,  0.2400,  0.2351,  0.2097,  0.0604,  0.0281,  0.0268,  0.0241,\n",
       "         -0.0307, -0.0422],\n",
       "        [ 2.2241,  1.6863,  1.6060,  1.6022,  1.5861,  1.5850,  1.5806,  1.5759,\n",
       "          1.5702,  1.5701],\n",
       "        [-0.5005, -0.8010, -0.8732, -0.9274, -0.9406, -0.9554, -0.9860, -1.0031,\n",
       "         -1.0288, -1.0444],\n",
       "        [ 1.3167,  1.3140,  1.2711,  1.2553,  1.2535,  1.2458,  1.2280,  1.2241,\n",
       "          1.2054,  1.1865],\n",
       "        [-0.0398, -0.1976, -0.2179, -0.2969, -0.3096, -0.3102, -0.3578, -0.3740,\n",
       "         -0.4046, -0.4127],\n",
       "        [-0.1653, -0.2578, -0.3673, -0.4578, -0.5318, -0.5415, -0.5519, -0.5561,\n",
       "         -0.5916, -0.6003],\n",
       "        [ 0.5252,  0.4546,  0.4166,  0.3771,  0.3498,  0.3498,  0.2946,  0.2875,\n",
       "          0.2688,  0.2514],\n",
       "        [ 2.2817,  2.2802,  2.2708,  2.1912,  2.1717,  2.1525,  2.1397,  2.1391,\n",
       "          2.1155,  2.1127]])"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "d5899b92-6851-4150-8bbd-0a53b54975ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = defaultdict(list)\n",
    "for i, term_l in enumerate(top_terms):\n",
    "    top_words['topic ' + str(i)] = [IDX_TO_TERM[term_id] for term_id in term_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "055abe1b-2295-4e1e-a097-8fad192d7fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic 0</th>\n",
       "      <td>blackberry</td>\n",
       "      <td>number</td>\n",
       "      <td>info</td>\n",
       "      <td>fine</td>\n",
       "      <td>www</td>\n",
       "      <td>08</td>\n",
       "      <td>01</td>\n",
       "      <td>lynn</td>\n",
       "      <td>464</td>\n",
       "      <td>deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 1</th>\n",
       "      <td>good</td>\n",
       "      <td>sorry</td>\n",
       "      <td>50</td>\n",
       "      <td>contract</td>\n",
       "      <td>morning</td>\n",
       "      <td>just</td>\n",
       "      <td>following</td>\n",
       "      <td>probably</td>\n",
       "      <td>know</td>\n",
       "      <td>regards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 2</th>\n",
       "      <td>ll</td>\n",
       "      <td>pm</td>\n",
       "      <td>send</td>\n",
       "      <td>today</td>\n",
       "      <td>03</td>\n",
       "      <td>home</td>\n",
       "      <td>susan</td>\n",
       "      <td>10</td>\n",
       "      <td>amy</td>\n",
       "      <td>xls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 3</th>\n",
       "      <td>need</td>\n",
       "      <td>mike</td>\n",
       "      <td>received</td>\n",
       "      <td>tonight</td>\n",
       "      <td>great</td>\n",
       "      <td>hi</td>\n",
       "      <td>let</td>\n",
       "      <td>said</td>\n",
       "      <td>meeting</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 4</th>\n",
       "      <td>try</td>\n",
       "      <td>intervention</td>\n",
       "      <td>monday</td>\n",
       "      <td>start</td>\n",
       "      <td>713</td>\n",
       "      <td>like</td>\n",
       "      <td>highlighted</td>\n",
       "      <td>download</td>\n",
       "      <td>donna</td>\n",
       "      <td>853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 5</th>\n",
       "      <td>asap</td>\n",
       "      <td>credit</td>\n",
       "      <td>task</td>\n",
       "      <td>people</td>\n",
       "      <td>piece</td>\n",
       "      <td>st</td>\n",
       "      <td>sure</td>\n",
       "      <td>power</td>\n",
       "      <td>don</td>\n",
       "      <td>updated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 6</th>\n",
       "      <td>guy</td>\n",
       "      <td>use</td>\n",
       "      <td>weather</td>\n",
       "      <td>28</td>\n",
       "      <td>jason</td>\n",
       "      <td>contact</td>\n",
       "      <td>review</td>\n",
       "      <td>ll</td>\n",
       "      <td>distribution</td>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 7</th>\n",
       "      <td>going</td>\n",
       "      <td>report</td>\n",
       "      <td>power</td>\n",
       "      <td>explorer</td>\n",
       "      <td>28</td>\n",
       "      <td>home</td>\n",
       "      <td>mail</td>\n",
       "      <td>msn</td>\n",
       "      <td>time</td>\n",
       "      <td>tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 8</th>\n",
       "      <td>final</td>\n",
       "      <td>assistance</td>\n",
       "      <td>fyi</td>\n",
       "      <td>right</td>\n",
       "      <td>just</td>\n",
       "      <td>new</td>\n",
       "      <td>does</td>\n",
       "      <td>thank</td>\n",
       "      <td>tell</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 9</th>\n",
       "      <td>mail</td>\n",
       "      <td>sure</td>\n",
       "      <td>99</td>\n",
       "      <td>sent</td>\n",
       "      <td>delainey</td>\n",
       "      <td>attached</td>\n",
       "      <td>read</td>\n",
       "      <td>going</td>\n",
       "      <td>questions</td>\n",
       "      <td>things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 10</th>\n",
       "      <td>need</td>\n",
       "      <td>2000</td>\n",
       "      <td>dave</td>\n",
       "      <td>sara</td>\n",
       "      <td>said</td>\n",
       "      <td>counterparty</td>\n",
       "      <td>07</td>\n",
       "      <td>jan</td>\n",
       "      <td>oops</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 11</th>\n",
       "      <td>just</td>\n",
       "      <td>fine</td>\n",
       "      <td>sorry</td>\n",
       "      <td>30</td>\n",
       "      <td>conference</td>\n",
       "      <td>need</td>\n",
       "      <td>weather</td>\n",
       "      <td>ok</td>\n",
       "      <td>pm</td>\n",
       "      <td>having</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 12</th>\n",
       "      <td>enron</td>\n",
       "      <td>713</td>\n",
       "      <td>09</td>\n",
       "      <td>review</td>\n",
       "      <td>given</td>\n",
       "      <td>853</td>\n",
       "      <td>way</td>\n",
       "      <td>look</td>\n",
       "      <td>work</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 13</th>\n",
       "      <td>thanks</td>\n",
       "      <td>changed</td>\n",
       "      <td>pl</td>\n",
       "      <td>ll</td>\n",
       "      <td>ve</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>office</td>\n",
       "      <td>sheet</td>\n",
       "      <td>address</td>\n",
       "      <td>ds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 14</th>\n",
       "      <td>today</td>\n",
       "      <td>explorer</td>\n",
       "      <td>week</td>\n",
       "      <td>just</td>\n",
       "      <td>provide</td>\n",
       "      <td>changed</td>\n",
       "      <td>04</td>\n",
       "      <td>probably</td>\n",
       "      <td>task</td>\n",
       "      <td>questions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 15</th>\n",
       "      <td>try</td>\n",
       "      <td>jim</td>\n",
       "      <td>thought</td>\n",
       "      <td>house</td>\n",
       "      <td>day</td>\n",
       "      <td>hear</td>\n",
       "      <td>ena</td>\n",
       "      <td>great</td>\n",
       "      <td>monday</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 16</th>\n",
       "      <td>2001</td>\n",
       "      <td>pm</td>\n",
       "      <td>start</td>\n",
       "      <td>know</td>\n",
       "      <td>task</td>\n",
       "      <td>11</td>\n",
       "      <td>need</td>\n",
       "      <td>john</td>\n",
       "      <td>sure</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 17</th>\n",
       "      <td>know</td>\n",
       "      <td>713</td>\n",
       "      <td>enron</td>\n",
       "      <td>00</td>\n",
       "      <td>thanks</td>\n",
       "      <td>02</td>\n",
       "      <td>eol</td>\n",
       "      <td>doing</td>\n",
       "      <td>let</td>\n",
       "      <td>financial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 18</th>\n",
       "      <td>agree</td>\n",
       "      <td>dan</td>\n",
       "      <td>just</td>\n",
       "      <td>ok</td>\n",
       "      <td>north</td>\n",
       "      <td>check</td>\n",
       "      <td>america</td>\n",
       "      <td>price</td>\n",
       "      <td>08</td>\n",
       "      <td>deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic 19</th>\n",
       "      <td>current</td>\n",
       "      <td>agreement</td>\n",
       "      <td>number</td>\n",
       "      <td>26</td>\n",
       "      <td>html</td>\n",
       "      <td>http</td>\n",
       "      <td>john</td>\n",
       "      <td>www</td>\n",
       "      <td>leslie</td>\n",
       "      <td>yesterday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0             1         2         3           4  \\\n",
       "topic 0   blackberry        number      info      fine         www   \n",
       "topic 1         good         sorry        50  contract     morning   \n",
       "topic 2           ll            pm      send     today          03   \n",
       "topic 3         need          mike  received   tonight       great   \n",
       "topic 4          try  intervention    monday     start         713   \n",
       "topic 5         asap        credit      task    people       piece   \n",
       "topic 6          guy           use   weather        28       jason   \n",
       "topic 7        going        report     power  explorer          28   \n",
       "topic 8        final    assistance       fyi     right        just   \n",
       "topic 9         mail          sure        99      sent    delainey   \n",
       "topic 10        need          2000      dave      sara        said   \n",
       "topic 11        just          fine     sorry        30  conference   \n",
       "topic 12       enron           713        09    review       given   \n",
       "topic 13      thanks       changed        pl        ll          ve   \n",
       "topic 14       today      explorer      week      just     provide   \n",
       "topic 15         try           jim   thought     house         day   \n",
       "topic 16        2001            pm     start      know        task   \n",
       "topic 17        know           713     enron        00      thanks   \n",
       "topic 18       agree           dan      just        ok       north   \n",
       "topic 19     current     agreement    number        26        html   \n",
       "\n",
       "                      5            6         7             8            9  \n",
       "topic 0              08           01      lynn           464         deal  \n",
       "topic 1            just    following  probably          know      regards  \n",
       "topic 2            home        susan        10           amy          xls  \n",
       "topic 3              hi          let      said       meeting          way  \n",
       "topic 4            like  highlighted  download         donna          853  \n",
       "topic 5              st         sure     power           don      updated  \n",
       "topic 6         contact       review        ll  distribution        going  \n",
       "topic 7            home         mail       msn          time      tonight  \n",
       "topic 8             new         does     thank          tell         free  \n",
       "topic 9        attached         read     going     questions       things  \n",
       "topic 10   counterparty           07       jan          oops           09  \n",
       "topic 11           need      weather        ok            pm       having  \n",
       "topic 12            853          way      look          work  information  \n",
       "topic 13  unfortunately       office     sheet       address           ds  \n",
       "topic 14        changed           04  probably          task    questions  \n",
       "topic 15           hear          ena     great        monday           02  \n",
       "topic 16             11         need      john          sure       thanks  \n",
       "topic 17             02          eol     doing           let    financial  \n",
       "topic 18          check      america     price            08         deal  \n",
       "topic 19           http         john       www        leslie    yesterday  "
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(top_words).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "007ec718-0f90-4f92-9296-79d1b24ffff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = vectorizer.vocabulary_['paper']\n",
    "idx_2 = vectorizer.vocabulary_['regards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "7708b331-51fc-488f-8949-73cc489170dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 1079)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bows['training'][:, idx_1] >= 1).sum(), (bows['training'][:, idx_2] >= 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "8e26b28d-33c0-4cb8-a229-2828643d4167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((bows['training'][:, idx_1] + bows['training'][:, idx_2]).toarray().squeeze() >= 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f0e767b9-c933-448b-8dde-10d957c2d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_beta_document(model, vectorizer, top_k=20):\n",
    "    betas_document = model.beta_document()\n",
    "    features_to_betas = {}\n",
    "    idx_to_name = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    for feature, logits in betas_document.items():\n",
    "        features_to_betas[feature] = []\n",
    "        num_features = logits.shape[0]\n",
    "        top_results = torch.topk(logits, top_k, dim=-1)\n",
    "        \n",
    "        ids = top_results.indices.cpu().numpy()\n",
    "        values = top_results.values.cpu().numpy()\n",
    "        \n",
    "        for i in tqdm(range(num_features)):\n",
    "            features_to_betas[feature].append({'values':values[i], 'top':[idx_to_name[idx] for idx in ids[i]]})\n",
    "                \n",
    "    return features_to_betas\n",
    "\n",
    "def top_beta_meta(model, meta_feature_to_names, top_k=20):\n",
    "    betas_metas = model.beta_meta()\n",
    "    features_to_betas = {}\n",
    "    for feature, logits in betas_metas.items():\n",
    "        idx_to_name = {i:k for i,k in enumerate(meta_feature_to_names[feature])}\n",
    "        features_to_betas[feature] = []\n",
    "        num_features = logits.shape[0]\n",
    "        top_results = torch.topk(logits, top_k, dim=-1)\n",
    "        ids = top_results.indices.cpu().numpy()\n",
    "        values = top_results.values.cpu().numpy()\n",
    "        for i in tqdm(range(num_features)):\n",
    "            features_to_betas[feature].append({'values':values[i], 'top':[idx_to_name[idx] for idx in ids[i]]})\n",
    "        \n",
    "    return features_to_betas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "a72b1e4b-b50d-476e-a0d9-71f2a14e3489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 12278.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 12052.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Info\n",
      "\tbeta_topic (5):\n",
      "\t\t beta_topic (0):\n",
      "['tired', 'comfortable', 'peters', 'blair', 'try', 'usual', 'time', 'historical', 'christopher', 'bike', 'quality', 'anytime', 'wear', 'facilities', 'issues', 'uk', 'returned', 'recipients', '20', 'benjamin']\n",
      "\n",
      "\t\t beta_topic (1):\n",
      "['change', 'gcp_enron', 'mona', 'wear', 'duty', 'worried', '916', 'young', 'returned', 'mailing', 'source', 'worry', 'nominations', 'internally', 'referenced', 'hasn', 'network', 'program', 'messaging', 'auto']\n",
      "\n",
      "\t\t beta_topic (2):\n",
      "['let', 'know', '20', 'need', 'america', 'kitchen', 'oct', 'mike', 'agreements', 'try', 'want', 'does', 'date', 'attached', 'make', '54', 'cc', 'bring', 'trading', 'job']\n",
      "\n",
      "\t\t beta_topic (3):\n",
      "['send', 'assume', 'mary', 'fine', 'ed', 'plant', '18', 'phone', 'asst', '30th', 'chance', '30', '01', 'originally', 'volumes', 'prebon', 'appreciate', 'somebody', 'ferc', 'hourahead']\n",
      "\n",
      "\t\t beta_topic (4):\n",
      "['haven', 'regards', 'weekend', 'hello', 'leave', 'home', 'comments', 'rick', 'considered', 'credit', 'points', 'prepare', 'msn', 'redlined', 'try', 'know', 'think', 'heads', '16', 'recently']\n",
      "\n",
      "Meta Var Info\n",
      "\tpos_bigrams (2):\n",
      "\t\t pos_bigrams (0):\n",
      "['pos_bigrams:INTJ INTJ', 'pos_bigrams:PART PROPN', 'pos_bigrams:PROPN ADJ', 'pos_bigrams:NUM X', 'pos_bigrams:DET PUNCT', 'pos_bigrams:PRON VERB', 'pos_bigrams:SPACE ADJ', 'pos_bigrams:PUNCT CCONJ', 'pos_bigrams:PRON INTJ', 'pos_bigrams:NOUN PUNCT', 'pos_bigrams:SYM INTJ', 'pos_bigrams:SYM VERB', 'pos_bigrams:PRON PART', 'pos_bigrams:NOUN SYM', 'pos_bigrams:INTJ ADV', 'pos_bigrams:SCONJ CCONJ', 'pos_bigrams:ADP X', 'pos_bigrams:PART SYM', 'pos_bigrams:CCONJ SCONJ', 'pos_bigrams:PRON AUX']\n",
      "\n",
      "\t\t pos_bigrams (1):\n",
      "['pos_bigrams:AUX PRON', 'pos_bigrams:PROPN PUNCT', 'pos_bigrams:INTJ NUM', 'pos_bigrams:VERB VERB', 'pos_bigrams:PUNCT PART', 'pos_bigrams:PUNCT VERB', 'pos_bigrams:SPACE SYM', 'pos_bigrams:CCONJ NOUN', 'pos_bigrams:X SCONJ', 'pos_bigrams:PUNCT CCONJ', 'pos_bigrams:ADJ SPACE', 'pos_bigrams:NOUN SPACE', 'pos_bigrams:VERB PUNCT', 'pos_bigrams:SYM PUNCT', 'pos_bigrams:NOUN NUM', 'pos_bigrams:PROPN SCONJ', 'pos_bigrams:ADP SCONJ', 'pos_bigrams:ADJ PRON', 'pos_bigrams:NOUN PUNCT', 'pos_bigrams:CCONJ PART']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_words_per_latent = top_beta_document(prod_slda, vectorizer,  top_k=20)\n",
    "top_meta_per_latent = top_beta_meta(prod_slda, meta_feature_to_names, top_k=20)\n",
    "\n",
    "print('Document Term Info')\n",
    "for latent, top in top_words_per_latent.items():\n",
    "    print(f'\\t{latent} ({len(top)}):')\n",
    "    for i, results in enumerate(top):\n",
    "        print(f'\\t\\t {latent} ({i}):\\n{results[\"top\"]}')\n",
    "        print()\n",
    "\n",
    "print('Meta Var Info')\n",
    "for latent, top in top_meta_per_latent.items():\n",
    "\n",
    "    print(f'\\t{latent} ({len(top)}):')\n",
    "    for i, results in enumerate(top):\n",
    "        print(f'\\t\\t {latent} ({i}):\\n{results[\"top\"]}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480600f-6e2f-4838-8c5a-8363c334bce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa278c-6a39-48a7-a8fc-8e8760ea68ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b23fb-ab53-4dff-a3db-fcdac8b23a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcfcab56-571b-406f-9609-23bdf0593b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_bigrams tensor([[0.0031, 0.0031, 0.0028,  ..., 0.0043, 0.0032, 0.0028],\n",
      "        [0.0031, 0.0037, 0.0031,  ..., 0.0047, 0.0034, 0.0025],\n",
      "        [0.0031, 0.0039, 0.0028,  ..., 0.0042, 0.0032, 0.0029],\n",
      "        [0.0029, 0.0031, 0.0022,  ..., 0.0027, 0.0023, 0.0033],\n",
      "        [0.0025, 0.0027, 0.0018,  ..., 0.0022, 0.0019, 0.0033]])\n"
     ]
    }
   ],
   "source": [
    "for key, value in prod_slda.beta_meta().items():\n",
    "    print(key, F.softmax(value,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7b77b68e-c765-440f-aed3-62cdb37d5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /burg/nlp/users/zfh2000/style_results/pos_bigrams/2023-12-14_17_54_45/model_epoch5_20914.218841552734.pt\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(PATH, f'model_epoch{total_epochs}_{val_elbo[-1]}.pt')\n",
    "prod_slda.eval()\n",
    "torch.save(prod_slda, path)\n",
    "# pyro.clear_param_store()\n",
    "# prod_slda = torch.load(path)\n",
    "print(f'Saved to {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0251941f-a584-430a-8b9c-c424b49a0333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos_bigrams': tensor([[-1.8304e+00, -1.8904e+00, -2.0618e+00,  ..., -1.5975e+00,\n",
       "          -2.6415e+00, -1.9489e+00],\n",
       "         [ 7.8684e-01,  1.0342e+00,  1.1435e+00,  ...,  1.0513e+00,\n",
       "           1.1998e+00,  1.0992e+00],\n",
       "         [ 8.8633e-01,  1.1214e+00,  1.3023e+00,  ...,  1.2147e+00,\n",
       "           1.2440e+00,  1.1934e+00],\n",
       "         [-4.2701e-01, -2.5076e-01, -3.2173e-01,  ..., -1.0346e-01,\n",
       "          -5.0079e-01, -2.3791e-01],\n",
       "         [-2.4423e-03,  1.2616e-01,  1.4202e-01,  ...,  2.4994e-01,\n",
       "           1.5193e-02,  2.2705e-01]])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_slda.beta_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f2250-4d98-446b-b710-7e503080e1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
