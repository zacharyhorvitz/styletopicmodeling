{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import os \n",
    "from prodslda_cls import ProdSLDA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "MODEL_PATH = '/burg/nlp/users/zfh2000/style_results/pos_bigrams/2023-12-14_17_54_45/model_epoch5_20914.218841552734.pt'\n",
    "DATA_DIR_PATH = '/burg/nlp/users/zfh2000/style_results/pos_bigrams/maxdf0.5_mindf5_DATA'\n",
    "\n",
    "with open(os.path.join(DATA_DIR_PATH, 'bows.pickle'), 'rb') as in_file:\n",
    "    bows = pickle.load(in_file)\n",
    "        \n",
    "with open(os.path.join(DATA_DIR_PATH, 'meta_vectorized.pickle'), 'rb') as in_file:\n",
    "    meta_vectorized = pickle.load(in_file)    \n",
    "\n",
    "with open(os.path.join(DATA_DIR_PATH, \"raw_text.json\"), 'r') as in_file:\n",
    "    raw_text = json.load(in_file)    \n",
    "\n",
    "with open(os.path.join(DATA_DIR_PATH, \"authors_json.json\"), 'r') as in_file:\n",
    "    authors_json = json.load(in_file)    \n",
    "\n",
    "with open(os.path.join(DATA_DIR_PATH, \"meta_feature_to_names.json\"), 'r') as in_file:\n",
    "    meta_feature_to_names = json.load(in_file)\n",
    "\n",
    "with open(os.path.join(DATA_DIR_PATH, \"vectorizer.pickle\"), 'rb') as in_file:\n",
    "    vectorizer = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProdSLDA(\n",
       "  (encoder): GeneralEncoder(\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (fc1s): ModuleDict(\n",
       "      (doc): Linear(in_features=9267, out_features=64, bias=True)\n",
       "    )\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (fcmu): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (fclv): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (bnmu): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (bnlv): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (beta): Linear(in_features=10, out_features=9267, bias=False)\n",
       "    (bn): BatchNorm1d(9267, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (style_encoder): GeneralEncoder(\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (fc1s): ModuleDict(\n",
       "      (pos_bigrams): Linear(in_features=324, out_features=64, bias=True)\n",
       "    )\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (fcmu): Linear(in_features=64, out_features=5, bias=True)\n",
       "    (fclv): Linear(in_features=64, out_features=5, bias=True)\n",
       "    (bnmu): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (bnlv): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  )\n",
       "  (style_decoder): ModuleDict(\n",
       "    (pos_bigrams): Decoder(\n",
       "      (beta): Linear(in_features=5, out_features=324, bias=False)\n",
       "      (bn): BatchNorm1d(324, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "prodsdla = torch.load(MODEL_PATH)\n",
    "prodsdla.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def top_beta_document(model, vectorizer, top_k=20):\n",
    "    betas_document = model.beta_document()\n",
    "    features_to_betas = {}\n",
    "    idx_to_name = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    for feature, logits in betas_document.items():\n",
    "        features_to_betas[feature] = []\n",
    "        num_features = logits.shape[0]\n",
    "        top_results = torch.topk(logits, top_k, dim=-1)\n",
    "        \n",
    "        ids = top_results.indices.cpu().numpy()\n",
    "        values = top_results.values.cpu().numpy()\n",
    "        \n",
    "        for i in tqdm(range(num_features)):\n",
    "            features_to_betas[feature].append({'values':values[i], 'top':[idx_to_name[idx] for idx in ids[i]]})\n",
    "                \n",
    "    return features_to_betas\n",
    "\n",
    "def top_beta_meta(model, meta_feature_to_names, top_k=20):\n",
    "    betas_metas = model.beta_meta()\n",
    "    features_to_betas = {}\n",
    "    for feature, logits in betas_metas.items():\n",
    "        idx_to_name = {i:k for i,k in enumerate(meta_feature_to_names[feature])}\n",
    "        features_to_betas[feature] = []\n",
    "        num_features = logits.shape[0]\n",
    "        top_results = torch.topk(logits, top_k, dim=-1)\n",
    "        ids = top_results.indices.cpu().numpy()\n",
    "        values = top_results.values.cpu().numpy()\n",
    "        for i in tqdm(range(num_features)):\n",
    "            features_to_betas[feature].append({'values':values[i], 'top':[idx_to_name[idx] for idx in ids[i]]})\n",
    "        \n",
    "    return features_to_betas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2612.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 87381.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Info\n",
      "\tbeta_topic (10):\n",
      "\t\t beta_topic (0):\n",
      "['fine', 'hope', 'sally', '28', '09', 'stuff', '08', 'hourahead', 'night', 'attached', 'jim', 'tickets', 'energy', 'making', 'presentation', 'kitchen', 'manual', '713', 'linda', 'original']\n",
      "\n",
      "\t\t beta_topic (1):\n",
      "['silva', 'geae', '962', '7566', 'eb3892', 'manis', 'x33278', '1575', 'freyre', 'bambos', 'ccampbell', 'giuseppe', 'vande', 'kupiecki', '4727', 'noncore', 'kayne', 'mckinsey', 'nassos', 'centilli']\n",
      "\n",
      "\t\t beta_topic (2):\n",
      "['fax', '713', 'john', 'market', 'bcc', 'help', 'yes', 'needs', 'hey', 'request', 'credit', 'questions', 'deals', 'address', 'updates', 'message', 'email', '646', 'management', 'www']\n",
      "\n",
      "\t\t beta_topic (3):\n",
      "['print', 'attachment', 'enron', 'report', 'asked', '07', 'format', 'sppc', 'wrong', 'job', 'waiting', 'kate', 'information', 'language', 'retain', 'time', 'sorry', 'attached', '00', 'basis']\n",
      "\n",
      "\t\t beta_topic (4):\n",
      "['fyi', 'need', 'just', 'meeting', 'sent', 'thanks', 'ferc', '2000', 'mark', 'revised', 'going', 'draft', 'help', 'update', '07', 'report', 'file', 'description', 'tuesday', 'order']\n",
      "\n",
      "\t\t beta_topic (5):\n",
      "['know', '12', 'deal', 'fyi', 'enron', 'did', 'wednesday', 'day', 'weekend', 'master', 'pdt', 'soon', 'john', 'bob', 'chris', 'yes', 'com', 'cc', '14', '05']\n",
      "\n",
      "\t\t beta_topic (6):\n",
      "['5426', 'eb3892', 'fairisaac', 'eb3816', 'nassos', '6794', 'retain', 'gco', 'freshfields', '3858', 'mday', '50m', 'centilli', 'harvey', '8600', 'livingston', 'sarimah', '4727', '0518', 'x33278']\n",
      "\n",
      "\t\t beta_topic (7):\n",
      "['kirstee', 'barkovich', 'lucas', 'redsky', 'howzabout', 'vande', 'mhc', 'psellers', 'master', '2001', 'eb3892', 'portable', 'maxwell', 'centilli', 'nmann', 'nassos', 'feeds', 'start', '1890', 'rakesh']\n",
      "\n",
      "\t\t beta_topic (8):\n",
      "['6794', 'nfb', 'bambos', 'napa', 'giuseppe', 'congratz', 'kayne', '4727', 'vande', 'ws', 'kupiecki', 'eb1336', 'montjoy', 'x37972', 'distance', 'ccampbell', 'kirstee', 'noncore', 'trv', 'silva']\n",
      "\n",
      "\t\t beta_topic (9):\n",
      "['week', 'afternoon', 'let', 'night', 'feedback', '2002', 'wednesday', 'send', '07', 'mon', 'john', 'feb', 'tomorrow', 'print', 'dynegy', 'kate', 'today', 'question', 'questions', '05']\n",
      "\n",
      "Meta Var Info\n",
      "\tpos_bigrams (5):\n",
      "\t\t pos_bigrams (0):\n",
      "['pos_bigrams:NOUN NOUN', 'pos_bigrams:ADP DET', 'pos_bigrams:PUNCT NUM', 'pos_bigrams:PROPN NUM', 'pos_bigrams:ADP PUNCT', 'pos_bigrams:PROPN PART', 'pos_bigrams:VERB DET', 'pos_bigrams:AUX VERB', 'pos_bigrams:PART NOUN', 'pos_bigrams:PRON VERB', 'pos_bigrams:PUNCT PUNCT', 'pos_bigrams:NOUN ADP', 'pos_bigrams:ADP CCONJ', 'pos_bigrams:PROPN PRON', 'pos_bigrams:NOUN ADV', 'pos_bigrams:NOUN CCONJ', 'pos_bigrams:PUNCT AUX', 'pos_bigrams:INTJ VERB', 'pos_bigrams:VERB PRON', 'pos_bigrams:ADJ NUM']\n",
      "\n",
      "\t\t pos_bigrams (1):\n",
      "['pos_bigrams:PART INTJ', 'pos_bigrams:AUX NUM', 'pos_bigrams:INTJ SPACE', 'pos_bigrams:X PROPN', 'pos_bigrams:DET SYM', 'pos_bigrams:DET PUNCT', 'pos_bigrams:SPACE NOUN', 'pos_bigrams:SPACE ADP', 'pos_bigrams:INTJ ADJ', 'pos_bigrams:NOUN DET', 'pos_bigrams:SYM ADV', 'pos_bigrams:SYM VERB', 'pos_bigrams:SPACE SPACE', 'pos_bigrams:ADV PRON', 'pos_bigrams:VERB SPACE', 'pos_bigrams:DET X', 'pos_bigrams:PROPN DET', 'pos_bigrams:ADJ SYM', 'pos_bigrams:SPACE X', 'pos_bigrams:DET CCONJ']\n",
      "\n",
      "\t\t pos_bigrams (2):\n",
      "['pos_bigrams:DET SYM', 'pos_bigrams:PART INTJ', 'pos_bigrams:INTJ PRON', 'pos_bigrams:SYM ADV', 'pos_bigrams:DET PRON', 'pos_bigrams:AUX SCONJ', 'pos_bigrams:DET PUNCT', 'pos_bigrams:AUX NUM', 'pos_bigrams:VERB SPACE', 'pos_bigrams:SPACE SPACE', 'pos_bigrams:SPACE ADP', 'pos_bigrams:INTJ ADJ', 'pos_bigrams:SYM VERB', 'pos_bigrams:ADP SPACE', 'pos_bigrams:PUNCT INTJ', 'pos_bigrams:PROPN DET', 'pos_bigrams:INTJ INTJ', 'pos_bigrams:ADP SCONJ', 'pos_bigrams:DET X', 'pos_bigrams:NUM SPACE']\n",
      "\n",
      "\t\t pos_bigrams (3):\n",
      "['pos_bigrams:PROPN PART', 'pos_bigrams:ADJ SYM', 'pos_bigrams:PRON DET', 'pos_bigrams:NOUN PROPN', 'pos_bigrams:INTJ SYM', 'pos_bigrams:PUNCT AUX', 'pos_bigrams:ADP INTJ', 'pos_bigrams:PRON VERB', 'pos_bigrams:ADP CCONJ', 'pos_bigrams:PUNCT PART', 'pos_bigrams:ADV AUX', 'pos_bigrams:ADJ NUM', 'pos_bigrams:PUNCT ADP', 'pos_bigrams:ADP VERB', 'pos_bigrams:NOUN VERB', 'pos_bigrams:AUX ADP', 'pos_bigrams:PROPN AUX', 'pos_bigrams:ADJ CCONJ', 'pos_bigrams:PUNCT ADJ', 'pos_bigrams:PRON SYM']\n",
      "\n",
      "\t\t pos_bigrams (4):\n",
      "['pos_bigrams:ADJ SYM', 'pos_bigrams:ADP INTJ', 'pos_bigrams:PRON SYM', 'pos_bigrams:INTJ SYM', 'pos_bigrams:PROPN PART', 'pos_bigrams:ADV NUM', 'pos_bigrams:INTJ ADJ', 'pos_bigrams:PUNCT PART', 'pos_bigrams:PRON DET', 'pos_bigrams:DET CCONJ', 'pos_bigrams:PUNCT AUX', 'pos_bigrams:NOUN PROPN', 'pos_bigrams:PROPN ADP', 'pos_bigrams:PROPN DET', 'pos_bigrams:SPACE SPACE', 'pos_bigrams:PROPN AUX', 'pos_bigrams:SPACE INTJ', 'pos_bigrams:DET X', 'pos_bigrams:AUX ADP', 'pos_bigrams:SPACE ADP']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_words_per_latent = top_beta_document(prodsdla, vectorizer,  top_k=20)\n",
    "top_meta_per_latent = top_beta_meta(prodsdla, meta_feature_to_names, top_k=20)\n",
    "\n",
    "print('Document Term Info')\n",
    "for latent, top in top_words_per_latent.items():\n",
    "    print(f'\\t{latent} ({len(top)}):')\n",
    "    for i, results in enumerate(top):\n",
    "        print(f'\\t\\t {latent} ({i}):\\n{results[\"top\"]}')\n",
    "        print()\n",
    "\n",
    "print('Meta Var Info')\n",
    "for latent, top in top_meta_per_latent.items():\n",
    "\n",
    "    print(f'\\t{latent} ({len(top)}):')\n",
    "    for i, results in enumerate(top):\n",
    "        print(f'\\t\\t {latent} ({i}):\\n{results[\"top\"]}')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text, author, bow, meta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(raw_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m], authors_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]): \u001b[38;5;66;03m#, bows['training'], meta_vectorized['training']):\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(author, text)\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for text, author, bow, meta in zip(raw_text['training'], authors_json['training']): #, bows['training'], meta_vectorized['training']):\n",
    "        print(author, text)\n",
    "        break\n",
    "        # print(text)\n",
    "        # print(author)\n",
    "        # print(bow)\n",
    "        # print(meta)\n",
    "        # print('------------------')\n",
    "        # result =  F.softmax(prodsdla.guide(bow.unsqueeze(0), meta.unsqueeze(0))[1])\n",
    "        # print(result)\n",
    "        # print('------------------')\n",
    "        # print('------------------'\n",
    "\n",
    "\n",
    "# label_to_topic = {}\n",
    "# label_to_max = {}\n",
    "\n",
    "# for d, text, encoded_label, label in zip(docs, data['data'], labels, data['labels']):\n",
    "#     if label not in label_to_topic:\n",
    "#         label_to_topic[label] = []\n",
    "#         label_to_max[label] = []\n",
    "#     # print(d)\n",
    "#     # print(label)\n",
    "#     # print('------------------')\n",
    "#     result =  F.softmax(prodLDA.guide(d.unsqueeze(0), encoded_label.unsqueeze(0))[1])\n",
    "\n",
    "#     # argmax = torch.argmax(result)\n",
    "#     # print(argmax)\n",
    "#     label_to_max[label].append(result[0].detach().cpu().numpy())\n",
    "#     print(label, result)\n",
    "#     label_to_topic[label].append((text,result))\n",
    "\n",
    "\n",
    "# for label in label_to_topic:\n",
    "#     print(label, np.mean(label_to_max[label]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
