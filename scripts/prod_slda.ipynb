{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2f8012e-f322-4234-aa7f-212c6c1620de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "073131aa-9d82-4037-91e6-8a5d750a0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SRC = './styletopicmodeling/data/enron/users_data_50_unique_clean_min_10_fixed_sender.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b5753-b006-44c8-885f-32f70635d4ca",
   "metadata": {},
   "source": [
    "# Data Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7665c5b-ab47-4176-ad3a-975eb88978dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01d8b37c-de66-4ce1-bf2b-9deefce39f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_SRC, sep = '\\t', header = None)\n",
    "data.columns = ['author', 'text', 'from', 'to', 'cc', 'bcc', 'meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4074fda7-483a-4ced-a581-a02ba56dc9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>cc</th>\n",
       "      <th>bcc</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p</td>\n",
       "      <td>Here is our forecast</td>\n",
       "      <td>From: phillip.allen@enron.com</td>\n",
       "      <td>X-To: Tim Belden</td>\n",
       "      <td>X-cc:</td>\n",
       "      <td>X-bcc:</td>\n",
       "      <td>Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p</td>\n",
       "      <td>test successful. way to go!!!</td>\n",
       "      <td>From: phillip.allen@enron.com</td>\n",
       "      <td>X-To: Leah Van Arsdall</td>\n",
       "      <td>X-cc:</td>\n",
       "      <td>X-bcc:</td>\n",
       "      <td>Date: Wed, 18 Oct 2000 03:00:00 -0700 (PDT)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                           text                           from  \\\n",
       "0  allen-p           Here is our forecast  From: phillip.allen@enron.com   \n",
       "1  allen-p  test successful. way to go!!!  From: phillip.allen@enron.com   \n",
       "\n",
       "                       to     cc     bcc  \\\n",
       "0        X-To: Tim Belden  X-cc:  X-bcc:   \n",
       "1  X-To: Leah Van Arsdall  X-cc:  X-bcc:   \n",
       "\n",
       "                                          meta  \n",
       "0  Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)  \n",
       "1  Date: Wed, 18 Oct 2000 03:00:00 -0700 (PDT)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec51068-a48f-46e4-9ae6-c1b4eb9d8244",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d96ec-eea5-45d0-8ab2-4fe1780e5824",
   "metadata": {},
   "source": [
    "### BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f79754a-9fa0-4fdd-a647-0084e702f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "bows = vectorizer.fit_transform(data['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d9a2a-6288-40a4-a62f-400f8c218402",
   "metadata": {},
   "source": [
    "### Bag-of-POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7024959-053c-456a-b3a6-2da1ebcc4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tags = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "853c2d94-7d2f-43c4-aff7-e3c61bf1711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_rep(text, tagset):\n",
    "    emb = {tag: 0 for tag in uni_tags}\n",
    "    tags = [tag[1] for tag in pos_tag(word_tokenize(test_txt), tagset = 'universal')]\n",
    "    for tag in tags:\n",
    "        emb[tag] += 1\n",
    "    return list(emb.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7303c4c-5106-4b59-ba47-a6977ca3eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89064/89064 [00:26<00:00, 3412.52it/s]\n"
     ]
    }
   ],
   "source": [
    "bops = data['text'].progress_apply(lambda txt: get_pos_rep(txt, uni_tags)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20a0166c-b4d0-4dd0-80dd-6f5fbca9b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bops = sparse.csr_matrix(bops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97291dbb-5aa9-4612-8961-a629cfa16c13",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14566e36-f4aa-4812-8633-d6e6c638f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Dims: (89064, 9222)\n",
      "BOP Dims: (89064, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f'BOW Dims: {bows.shape}')\n",
    "print(f'BOP Dims: {bops.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e3a9e-420d-4b50-887a-6e00b5ec98f7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae84587-4202-4136-a98a-8cc5f52b4402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d1b0ce8-9814-431f-8ecb-f557327c933d",
   "metadata": {},
   "source": [
    "# Model Definition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e5545-1066-4331-8d2f-4afc11fe9469",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Neural Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd5ea24-a797-4986-afb4-b0806a3d8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # Base class for the encoder net, used in the guide\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout, eps = 1e-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_topics)\n",
    "        self.fclv = nn.Linear(hidden, num_topics)\n",
    "\n",
    "        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = F.softplus(self.fc1(inputs))\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # μ and Σ are the outputs\n",
    "        logtheta_loc = self.bnmu(self.fcmu(h))\n",
    "        logtheta_logvar = self.bnlv(self.fclv(h))\n",
    "        logtheta_scale = self.eps + (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "        return logtheta_loc, logtheta_scale\n",
    "    \n",
    "class MetaDocEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, meta_size, num_styles, hidden, dropout, eps = 1e-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        self.fc1_doc = nn.Linear(vocab_size, hidden)\n",
    "        self.fc1_meta = nn.Linear(meta_size, hidden)\n",
    "        self.fc2 = nn.Linear(2 * hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_styles)\n",
    "        self.fclv = nn.Linear(hidden, num_styles)\n",
    "\n",
    "        self.bnmu = nn.BatchNorm1d(num_styles, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_styles, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs_doc, inputs_meta):\n",
    "        \n",
    "        h_doc = F.softplus(self.fc1_doc(inputs_doc))\n",
    "        h_meta = F.softplus(self.fc1_meta(inputs_meta))\n",
    "        h = torch.hstack([h_doc, h_meta])\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # μ and Σ are the outputs\n",
    "        logkappa_loc = self.bnmu(self.fcmu(h))\n",
    "        logkappa_logvar = self.bnlv(self.fclv(h))\n",
    "        logkappa_scale = self.eps + (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "        return logkappa_loc, logkappa_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db798a1e-8552-4d0a-b185-14b3e4425bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.drop(inputs)\n",
    "        # the output is σ(βθ)\n",
    "        return self.bn(self.beta(inputs))\n",
    "    \n",
    "class MetaDocDecoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.t_beta = nn.Linear(num_topics, meta_size, bias=False)\n",
    "        self.s_beta = nn.Linear(num_styles, meta_size, bias=False)\n",
    "        self.t_bn = nn.BatchNorm1d(meta_size, affine=False)\n",
    "        self.s_bn = nn.BatchNorm1d(meta_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs_doc, inputs_meta):\n",
    "        inputs_doc  = self.drop(inputs_doc)\n",
    "        inputs_meta = self.drop(inputs_meta)\n",
    "        # the output is σ(βθ)\n",
    "        \n",
    "        dist_t = self.t_bn(self.t_beta(inputs_doc))\n",
    "        dist_s = self.s_bn(self.s_beta(inputs_meta))\n",
    "        \n",
    "        return 0.5 * (dist_t + dist_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3b961-5983-4575-af2a-b5fe5ca56f84",
   "metadata": {},
   "source": [
    "## ProdSLDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d033aa-6189-415b-8d37-80ca01b8dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdSLDA(nn.Module):\n",
    "    \n",
    "    PRIOR_DISTS  = {'gaussian': dist.Normal,\n",
    "                    'laplace': dist.Laplace,\n",
    "                   }\n",
    "    TK_LINKS     = ('none', # Model style and documents independently\n",
    "                    'kappa_doc', # Allow kappa to effect word distributions\n",
    "                    'kappa_doc_style', # Allow kappa to effect word distributions and sampled words to effect style\n",
    "                   )\n",
    "    \n",
    "    def __init__(self, vocab_size, meta_size, num_topics, num_styles, hidden, dropout, \n",
    "                 theta_prior_dist = 'gaussian', theta_prior_loc = 0., theta_prior_scale = 1.,\n",
    "                 kappa_prior_dist = 'laplace', kappa_prior_loc = 0., kappa_prior_scale = 1.,\n",
    "                 style_topic_link = 'none',\n",
    "                 eps = 1e-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Global model variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_topics = num_topics\n",
    "        self.hidden     = hidden\n",
    "        self.dropout    = dropout\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "        # Theta Prior\n",
    "        if theta_prior_dist not in ProdSLDA.PRIOR_DISTS.keys():\n",
    "            raise ValueError(f'Theta prior {theta_prior_dist} not yet implemented. Must be one of {\", \".join(ProdSLDA.PRIOR_DISTS.keys())}')\n",
    "        self.theta_prior_dist = theta_prior_dist\n",
    "        \n",
    "        self.theta_prior_scale = theta_prior_scale\n",
    "        self.theta_prior_loc = theta_prior_loc\n",
    "        \n",
    "        # Kappa Prior\n",
    "        if kappa_prior_dist not in ProdSLDA.PRIOR_DISTS.keys():\n",
    "            raise ValueError(f'Kappa prior {kappa_prior_dist} not yet implemented. Must be one of {\", \".join(ProdSLDA.PRIOR_DISTS.keys())}')\n",
    "        self.kappa_prior_dist = kappa_prior_dist\n",
    "        \n",
    "        self.kappa_prior_scale = kappa_prior_scale\n",
    "        self.kappa_prior_loc = kappa_prior_loc\n",
    "        \n",
    "        \n",
    "        # Document style linking\n",
    "        self.style_topic_link = style_topic_link\n",
    "        \n",
    "        if self.style_topic_link not in ProdSLDA.TK_LINKS:\n",
    "            raise ValueError(f'Link {self.style_topic_link} not yet implemented. Must be one of {\", \".join(ProdSLDA.TK_LINKS)}')\n",
    "        elif self.style_topic_link == 'none':\n",
    "            # Independent modeling of style and topic, all normal encoder/decoders\n",
    "            self.encoder = Encoder(vocab_size, num_topics, hidden, dropout, self.eps)\n",
    "            self.decoder = Decoder(vocab_size, num_topics, dropout)\n",
    "            self.style_encoder = Encoder(meta_size, num_styles, hidden, dropout, self.eps)\n",
    "            self.style_decoder = Decoder(meta_size, num_styles, dropout)\n",
    "        elif self.style_topic_link == 'kappa_doc':\n",
    "            # Doc influences kappa encoding, style encoder takes in doc\n",
    "            self.encoder = MetaDocEncoder(vocab_size, meta_size, num_styles, hidden, dropout, self.eps)\n",
    "            self.decoder = MetaDocDecoder(vocab_size, num_topics, dropout)\n",
    "            self.style_encoder = Encoder(meta_size, num_styles, hidden, dropout, self.eps)\n",
    "            self.style_decoder = Decoder(meta_size, num_styles, dropout)\n",
    "\n",
    "    def model(self, docs, metas):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        pyro.module(\"style_decoder\", self.style_decoder)\n",
    "        \n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics)) * self.theta_prior_loc\n",
    "            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics)) * self.theta_prior_scale\n",
    "            logkappa_loc = docs.new_zeros((docs.shape[0], self.num_styles)) * self.kappa_prior_loc\n",
    "            logkappa_scale = docs.new_ones((docs.shape[0], self.num_styles)) * self.kappa_prior_scale\n",
    "            \n",
    "            if self.style_topic_link == 'kappa_doc':\n",
    "                logtheta_s_loc = docs.new_zeros((docs.shape[0], self.num_topics)) * self.theta_prior_loc\n",
    "                logtheta_s_scale = docs.new_ones((docs.shape[0], self.num_topics)) * self.theta_prior_scale\n",
    "                \n",
    "                logtheta_s = pyro.sample(\n",
    "                    \"logtheta_s\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_s_loc, logtheta_s_scale).to_event(1))\n",
    "                \n",
    "                theta_s = F.softmax(logtheta_s, -1)\n",
    "            \n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_loc, logtheta_scale).to_event(1))\n",
    "            logkappa = pyro.sample(\n",
    "                \"logkappa\", ProdSLDA.PRIOR_DISTS[self.kappa_prior_dist](logkappa_loc, logkappa_scale).to_event(1))\n",
    "            \n",
    "            theta = F.softmax(logtheta, -1)\n",
    "            kappa = F.softmax(logkappa, -1)\n",
    "\n",
    "            if self.style_topic_link == 'none':\n",
    "                word_logits = self.decoder(theta)\n",
    "            elif self.style_topic_link == 'kappa_doc':\n",
    "                word_logits = self.decoder(theta, theta_s)\n",
    "                \n",
    "            style_logits = self.style_decoder(kappa)\n",
    "\n",
    "            total_count = int(docs.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs_doc',\n",
    "                dist.Multinomial(total_count, logits = word_logits),\n",
    "                obs=docs\n",
    "            )\n",
    "            \n",
    "            total_s_count = int(meta.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs_meta',\n",
    "                dist.Multinomial(total_s_count, logits = style_logits),\n",
    "                obs=metas\n",
    "            )\n",
    "\n",
    "    def guide(self, docs, metas):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        pyro.module(\"style_encoder\", self.encoder)\n",
    "            \n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution,\n",
    "            # where μ and Σ are the encoder network outputs\n",
    "            \n",
    "            if self.style_topic_link == 'none':\n",
    "                logtheta_loc, logtheta_scale = self.encoder(docs)\n",
    "                logkappa_loc, logkappa_scale = self.style_encoder(metas)\n",
    "\n",
    "            elif self.style_topic_link == 'kappa_doc':\n",
    "                logtheta_loc, logtheta_scale, logkappa_d_loc, logkappa_d_scale = self.encoder(docs, metas)\n",
    "                logkappa_loc, logkappa_scale = self.style_encoder(metas)\n",
    "                \n",
    "                # Average theta loc from document and style\n",
    "                logkappa_loc = 0.5 * (logkappa_loc + logkappa_d_loc) \n",
    "                logkappa_scale = 0.5 * (logkappa_scale + logkappa_d_scale)\n",
    "            \n",
    "            # Sample logtheta/logkappa from guide\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_loc, logtheta_scale).to_event(1))\n",
    "            logkappa = pyro.sample(\n",
    "                \"logkappa\", ProdSLDA.PRIOR_DISTS[self.kappa_prior_dist](logkappa_loc, logkappa_scale).to_event(1))\n",
    "            \n",
    "\n",
    "    def beta_topic(self):\n",
    "        if self.style_topic_link = 'none':\n",
    "            return self.decoder.beta.weight.cpu().detach().T\n",
    "        elif self.style_topic_link = 'kappa_doc':\n",
    "            t_beta = self.decoder.t_beta.weight.cpu().detach().T\n",
    "            s_beta = self.style_decoder.s_beta.weight.cpu().detach().T\n",
    "            return t_beta, s_beta\n",
    "            \n",
    "    def betas(self):\n",
    "        # beta matrix elements are the weights of the FC layer on the decoder\n",
    "        return self.decoder.beta.weight.cpu().detach().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
