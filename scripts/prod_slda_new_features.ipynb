{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e2f8012e-f322-4234-aa7f-212c6c1620de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "073131aa-9d82-4037-91e6-8a5d750a0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_SRC = './styletopicmodeling/data/enron/users_data_50_unique_clean_min_10_fixed_sender.tsv'\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 2 #8\n",
    "\n",
    "LR = 1e-2\n",
    "BETAS = (0.99, 0.999)\n",
    "EPS   = 1e-8\n",
    "CLIP_NORM = 10.\n",
    "ADAM_ARGS = {'lr': LR, 'betas': BETAS, 'eps': EPS, 'clip_norm': CLIP_NORM}\n",
    "\n",
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b5753-b006-44c8-885f-32f70635d4ca",
   "metadata": {},
   "source": [
    "# Data Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7665c5b-ab47-4176-ad3a-975eb88978dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "1726f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATA_SRC = '/burg/nlp/users/zfh2000/enron_processed.json'\n",
    "SPLITS_PATH = '/burg/home/zfh2000/styletopicmodeling/scripts/authors_splits.json'\n",
    "SYNTHETIC_SRC = '/burg/nlp/users/zfh2000/gpt_4_enron_processed.json'\n",
    "\n",
    "# NICK: files have been uploaded here: https://drive.google.com/drive/folders/1uF9GWEGe4aqSeo2MlachAWR9bTHsJq1q?usp=sharing\n",
    "\n",
    "with open(DATA_SRC, 'r') as in_file:\n",
    "    data = json.load(in_file)\n",
    "\n",
    "with open(SYNTHETIC_SRC, 'r') as in_file:\n",
    "   synthetic_data = json.load(in_file)\n",
    "\n",
    "data = data + synthetic_data\n",
    "\n",
    "with open(SPLITS_PATH, 'r') as in_file:\n",
    "    SPLITS = json.load(in_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "01d8b37c-de66-4ce1-bf2b-9deefce39f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(DATA_SRC, sep = '\\t', header = None)\n",
    "# data.columns = ['author', 'text', 'from', 'to', 'cc', 'bcc', 'meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "98eee48f-e862-487e-a962-de6479efb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_features(x):\n",
    "    # emotion:\n",
    "    if 'unknown' not in x['emotion']:\n",
    "        if sum(x['emotion'].values()) == 0:\n",
    "            x['emotion']['unknown'] = 1\n",
    "        else:\n",
    "            x['emotion']['unknown'] = 0\n",
    "\n",
    "    # morph\n",
    "    x['morph_tags'] = {k:v for k,v in x['gram2vec'].items() if k.startswith('morph')}\n",
    "    x['punc_tags'] = {k:v for k,v in x['gram2vec'].items() if k.startswith('punctuation')}\n",
    "    x['sentences'] = {k:v for k,v in x['gram2vec'].items() if k.startswith('sentences')}\n",
    "    \n",
    "    if 'none' not in x['punc_tags']:\n",
    "        if sum(x['punc_tags'].values()) == 0:\n",
    "            x['punc_tags']['none'] = 1\n",
    "        else:\n",
    "            x['punc_tags']['none'] = 0\n",
    "\n",
    "    if 'none' not in x['sentences']:\n",
    "        if sum(x['sentences'].values()) == 0:\n",
    "            x['sentences']['none'] = 1\n",
    "        else:\n",
    "            x['sentences']['none'] = 0\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "4074fda7-483a-4ced-a581-a02ba56dc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(2)\n",
    "for x in data:\n",
    "    clean_up_features(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d445c531-b424-4615-9756-c32c74a2cd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 6]\n",
      "[0 0 9 1]\n",
      "[0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "def fix_normalization(x):\n",
    "    # print(x)\n",
    "    replaced = np.where(x==0, 1, x)\n",
    "    minimum = np.min(replaced)\n",
    "    if minimum < 1:\n",
    "        # print(minimum)\n",
    "        scale = 1/minimum\n",
    "        x = (x*scale).astype(int)\n",
    "    # print(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def extract_features(doc, meta_feature_to_names):\n",
    "    features = {'text':doc['text']}\n",
    "    for key, possible in meta_feature_to_names.items():\n",
    "        if isinstance(doc[key], list):\n",
    "            features[key] = np.array([doc[key].count(l) for l in possible])\n",
    "            # features[key] = features[key] #/ np.sum(features[key])\n",
    "        elif isinstance(doc[key], dict):\n",
    "            features[key] = np.array([doc[key][l] for l in possible]).astype(float)\n",
    "            # features[key] = features[key] #/ np.sum(features[key])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        features[key] = fix_normalization(features[key])\n",
    "\n",
    "         \n",
    "\n",
    "        \n",
    "\n",
    "        # features[key] = np.nan_to_num(features[key], nan=1/len(features[key]))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "                        \n",
    "            \n",
    "print(fix_normalization(np.array([1,0,4,6])))\n",
    "print(fix_normalization(np.array([0,0,0.9,0.1])))\n",
    "print(fix_normalization(np.array([0,0,1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "65683c01-fe9a-4b67-9237-212038e7af3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'test successful. way to go!!!',\n",
       " 'info': {'author': 'allen-p',\n",
       "  'from': 'From: phillip.allen@enron.com',\n",
       "  'to': 'X-To: Leah Van Arsdall',\n",
       "  'cc': 'X-cc:',\n",
       "  'bcc': 'X-bcc:',\n",
       "  'meta': 'Date: Wed, 18 Oct 2000 03:00:00 -0700 (PDT)'},\n",
       " 'sentences': {'sentences:it-cleft': 0.0,\n",
       "  'sentences:pseudo-cleft': 0.0,\n",
       "  'sentences:all-cleft': 0.0,\n",
       "  'sentences:there-cleft': 0.0,\n",
       "  'sentences:if-because-cleft': 0.0,\n",
       "  'sentences:passive': 0.0,\n",
       "  'sentences:subj-relcl': 0.0,\n",
       "  'sentences:obj-relcl': 0.0,\n",
       "  'sentences:tag-question': 0.0,\n",
       "  'sentences:coordinate-clause': 0.0,\n",
       "  'none': 1},\n",
       " 'passive': [0, 0],\n",
       " 'formality': ['Formal', 'Informal'],\n",
       " 'pos': {'ADJ': 1,\n",
       "  'ADP': 0,\n",
       "  'ADV': 0,\n",
       "  'CONJ': 0,\n",
       "  'DET': 0,\n",
       "  'NOUN': 2,\n",
       "  'NUM': 0,\n",
       "  'PRT': 1,\n",
       "  'PRON': 0,\n",
       "  'VERB': 1,\n",
       "  '.': 4,\n",
       "  'X': 0},\n",
       " 'question': ['statement', 'statement'],\n",
       " 'gram2vec': {'pos_unigrams:ADJ': 0.1111111111111111,\n",
       "  'pos_unigrams:ADP': 0.0,\n",
       "  'pos_unigrams:ADV': 0.0,\n",
       "  'pos_unigrams:AUX': 0.0,\n",
       "  'pos_unigrams:CCONJ': 0.0,\n",
       "  'pos_unigrams:DET': 0.0,\n",
       "  'pos_unigrams:INTJ': 0.0,\n",
       "  'pos_unigrams:NOUN': 0.2222222222222222,\n",
       "  'pos_unigrams:NUM': 0.0,\n",
       "  'pos_unigrams:PART': 0.1111111111111111,\n",
       "  'pos_unigrams:PRON': 0.0,\n",
       "  'pos_unigrams:PROPN': 0.0,\n",
       "  'pos_unigrams:PUNCT': 0.4444444444444444,\n",
       "  'pos_unigrams:SCONJ': 0.0,\n",
       "  'pos_unigrams:SYM': 0.0,\n",
       "  'pos_unigrams:VERB': 0.1111111111111111,\n",
       "  'pos_unigrams:X': 0.0,\n",
       "  'pos_unigrams:SPACE': 0.0,\n",
       "  'pos_bigrams:ADJ ADP': 0.0,\n",
       "  'pos_bigrams:ADP ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ ADV': 0.0,\n",
       "  'pos_bigrams:ADV ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ AUX': 0.0,\n",
       "  'pos_bigrams:AUX ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ DET': 0.0,\n",
       "  'pos_bigrams:DET ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN ADJ': 0.14285714285714285,\n",
       "  'pos_bigrams:ADJ NUM': 0.0,\n",
       "  'pos_bigrams:NUM ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ PART': 0.0,\n",
       "  'pos_bigrams:PART ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ PRON': 0.0,\n",
       "  'pos_bigrams:PRON ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ PUNCT': 0.14285714285714285,\n",
       "  'pos_bigrams:PUNCT ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ SYM': 0.0,\n",
       "  'pos_bigrams:SYM ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ VERB': 0.0,\n",
       "  'pos_bigrams:VERB ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ X': 0.0,\n",
       "  'pos_bigrams:X ADJ': 0.0,\n",
       "  'pos_bigrams:ADJ SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE ADJ': 0.0,\n",
       "  'pos_bigrams:ADP ADV': 0.0,\n",
       "  'pos_bigrams:ADV ADP': 0.0,\n",
       "  'pos_bigrams:ADP AUX': 0.0,\n",
       "  'pos_bigrams:AUX ADP': 0.0,\n",
       "  'pos_bigrams:ADP CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ ADP': 0.0,\n",
       "  'pos_bigrams:ADP DET': 0.0,\n",
       "  'pos_bigrams:DET ADP': 0.0,\n",
       "  'pos_bigrams:ADP INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ ADP': 0.0,\n",
       "  'pos_bigrams:ADP NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN ADP': 0.0,\n",
       "  'pos_bigrams:ADP NUM': 0.0,\n",
       "  'pos_bigrams:NUM ADP': 0.0,\n",
       "  'pos_bigrams:ADP PART': 0.0,\n",
       "  'pos_bigrams:PART ADP': 0.0,\n",
       "  'pos_bigrams:ADP PRON': 0.0,\n",
       "  'pos_bigrams:PRON ADP': 0.0,\n",
       "  'pos_bigrams:ADP PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN ADP': 0.0,\n",
       "  'pos_bigrams:ADP PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT ADP': 0.0,\n",
       "  'pos_bigrams:ADP SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ ADP': 0.0,\n",
       "  'pos_bigrams:ADP SYM': 0.0,\n",
       "  'pos_bigrams:SYM ADP': 0.0,\n",
       "  'pos_bigrams:ADP VERB': 0.0,\n",
       "  'pos_bigrams:VERB ADP': 0.0,\n",
       "  'pos_bigrams:ADP X': 0.0,\n",
       "  'pos_bigrams:X ADP': 0.0,\n",
       "  'pos_bigrams:ADP SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE ADP': 0.0,\n",
       "  'pos_bigrams:ADV AUX': 0.0,\n",
       "  'pos_bigrams:AUX ADV': 0.0,\n",
       "  'pos_bigrams:ADV CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ ADV': 0.0,\n",
       "  'pos_bigrams:ADV DET': 0.0,\n",
       "  'pos_bigrams:DET ADV': 0.0,\n",
       "  'pos_bigrams:ADV INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ ADV': 0.0,\n",
       "  'pos_bigrams:ADV NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN ADV': 0.0,\n",
       "  'pos_bigrams:ADV NUM': 0.0,\n",
       "  'pos_bigrams:NUM ADV': 0.0,\n",
       "  'pos_bigrams:ADV PART': 0.0,\n",
       "  'pos_bigrams:PART ADV': 0.0,\n",
       "  'pos_bigrams:ADV PRON': 0.0,\n",
       "  'pos_bigrams:PRON ADV': 0.0,\n",
       "  'pos_bigrams:ADV PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN ADV': 0.0,\n",
       "  'pos_bigrams:ADV PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT ADV': 0.0,\n",
       "  'pos_bigrams:ADV SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ ADV': 0.0,\n",
       "  'pos_bigrams:ADV SYM': 0.0,\n",
       "  'pos_bigrams:SYM ADV': 0.0,\n",
       "  'pos_bigrams:ADV VERB': 0.0,\n",
       "  'pos_bigrams:VERB ADV': 0.0,\n",
       "  'pos_bigrams:ADV X': 0.0,\n",
       "  'pos_bigrams:X ADV': 0.0,\n",
       "  'pos_bigrams:ADV SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE ADV': 0.0,\n",
       "  'pos_bigrams:AUX CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ AUX': 0.0,\n",
       "  'pos_bigrams:AUX DET': 0.0,\n",
       "  'pos_bigrams:DET AUX': 0.0,\n",
       "  'pos_bigrams:AUX INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ AUX': 0.0,\n",
       "  'pos_bigrams:AUX NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN AUX': 0.0,\n",
       "  'pos_bigrams:AUX NUM': 0.0,\n",
       "  'pos_bigrams:NUM AUX': 0.0,\n",
       "  'pos_bigrams:AUX PART': 0.0,\n",
       "  'pos_bigrams:PART AUX': 0.0,\n",
       "  'pos_bigrams:AUX PRON': 0.0,\n",
       "  'pos_bigrams:PRON AUX': 0.0,\n",
       "  'pos_bigrams:AUX PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN AUX': 0.0,\n",
       "  'pos_bigrams:AUX PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT AUX': 0.0,\n",
       "  'pos_bigrams:AUX SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ AUX': 0.0,\n",
       "  'pos_bigrams:AUX SYM': 0.0,\n",
       "  'pos_bigrams:SYM AUX': 0.0,\n",
       "  'pos_bigrams:AUX VERB': 0.0,\n",
       "  'pos_bigrams:VERB AUX': 0.0,\n",
       "  'pos_bigrams:AUX X': 0.0,\n",
       "  'pos_bigrams:X AUX': 0.0,\n",
       "  'pos_bigrams:AUX SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE AUX': 0.0,\n",
       "  'pos_bigrams:CCONJ DET': 0.0,\n",
       "  'pos_bigrams:DET CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ NUM': 0.0,\n",
       "  'pos_bigrams:NUM CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ PART': 0.0,\n",
       "  'pos_bigrams:PART CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ PRON': 0.0,\n",
       "  'pos_bigrams:PRON CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ SYM': 0.0,\n",
       "  'pos_bigrams:SYM CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ VERB': 0.0,\n",
       "  'pos_bigrams:VERB CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ X': 0.0,\n",
       "  'pos_bigrams:X CCONJ': 0.0,\n",
       "  'pos_bigrams:CCONJ SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE CCONJ': 0.0,\n",
       "  'pos_bigrams:DET INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ DET': 0.0,\n",
       "  'pos_bigrams:DET NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN DET': 0.0,\n",
       "  'pos_bigrams:DET NUM': 0.0,\n",
       "  'pos_bigrams:NUM DET': 0.0,\n",
       "  'pos_bigrams:DET PART': 0.0,\n",
       "  'pos_bigrams:PART DET': 0.0,\n",
       "  'pos_bigrams:DET PRON': 0.0,\n",
       "  'pos_bigrams:PRON DET': 0.0,\n",
       "  'pos_bigrams:DET PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN DET': 0.0,\n",
       "  'pos_bigrams:DET PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT DET': 0.0,\n",
       "  'pos_bigrams:DET SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ DET': 0.0,\n",
       "  'pos_bigrams:DET SYM': 0.0,\n",
       "  'pos_bigrams:SYM DET': 0.0,\n",
       "  'pos_bigrams:DET VERB': 0.0,\n",
       "  'pos_bigrams:VERB DET': 0.0,\n",
       "  'pos_bigrams:DET X': 0.0,\n",
       "  'pos_bigrams:X DET': 0.0,\n",
       "  'pos_bigrams:DET SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE DET': 0.0,\n",
       "  'pos_bigrams:INTJ NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ NUM': 0.0,\n",
       "  'pos_bigrams:NUM INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ PART': 0.0,\n",
       "  'pos_bigrams:PART INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ PRON': 0.0,\n",
       "  'pos_bigrams:PRON INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ SYM': 0.0,\n",
       "  'pos_bigrams:SYM INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ VERB': 0.0,\n",
       "  'pos_bigrams:VERB INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ X': 0.0,\n",
       "  'pos_bigrams:X INTJ': 0.0,\n",
       "  'pos_bigrams:INTJ SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE INTJ': 0.0,\n",
       "  'pos_bigrams:NOUN NUM': 0.0,\n",
       "  'pos_bigrams:NUM NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN PART': 0.14285714285714285,\n",
       "  'pos_bigrams:PART NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN PRON': 0.0,\n",
       "  'pos_bigrams:PRON NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN SYM': 0.0,\n",
       "  'pos_bigrams:SYM NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN VERB': 0.0,\n",
       "  'pos_bigrams:VERB NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN X': 0.0,\n",
       "  'pos_bigrams:X NOUN': 0.0,\n",
       "  'pos_bigrams:NOUN SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE NOUN': 0.0,\n",
       "  'pos_bigrams:NUM PART': 0.0,\n",
       "  'pos_bigrams:PART NUM': 0.0,\n",
       "  'pos_bigrams:NUM PRON': 0.0,\n",
       "  'pos_bigrams:PRON NUM': 0.0,\n",
       "  'pos_bigrams:NUM PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN NUM': 0.0,\n",
       "  'pos_bigrams:NUM PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT NUM': 0.0,\n",
       "  'pos_bigrams:NUM SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ NUM': 0.0,\n",
       "  'pos_bigrams:NUM SYM': 0.0,\n",
       "  'pos_bigrams:SYM NUM': 0.0,\n",
       "  'pos_bigrams:NUM VERB': 0.0,\n",
       "  'pos_bigrams:VERB NUM': 0.0,\n",
       "  'pos_bigrams:NUM X': 0.0,\n",
       "  'pos_bigrams:X NUM': 0.0,\n",
       "  'pos_bigrams:NUM SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE NUM': 0.0,\n",
       "  'pos_bigrams:PART PRON': 0.0,\n",
       "  'pos_bigrams:PRON PART': 0.0,\n",
       "  'pos_bigrams:PART PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN PART': 0.0,\n",
       "  'pos_bigrams:PART PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT PART': 0.0,\n",
       "  'pos_bigrams:PART SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ PART': 0.0,\n",
       "  'pos_bigrams:PART SYM': 0.0,\n",
       "  'pos_bigrams:SYM PART': 0.0,\n",
       "  'pos_bigrams:PART VERB': 0.14285714285714285,\n",
       "  'pos_bigrams:VERB PART': 0.0,\n",
       "  'pos_bigrams:PART X': 0.0,\n",
       "  'pos_bigrams:X PART': 0.0,\n",
       "  'pos_bigrams:PART SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE PART': 0.0,\n",
       "  'pos_bigrams:PRON PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN PRON': 0.0,\n",
       "  'pos_bigrams:PRON PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT PRON': 0.0,\n",
       "  'pos_bigrams:PRON SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ PRON': 0.0,\n",
       "  'pos_bigrams:PRON SYM': 0.0,\n",
       "  'pos_bigrams:SYM PRON': 0.0,\n",
       "  'pos_bigrams:PRON VERB': 0.0,\n",
       "  'pos_bigrams:VERB PRON': 0.0,\n",
       "  'pos_bigrams:PRON X': 0.0,\n",
       "  'pos_bigrams:X PRON': 0.0,\n",
       "  'pos_bigrams:PRON SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE PRON': 0.0,\n",
       "  'pos_bigrams:PROPN PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN SYM': 0.0,\n",
       "  'pos_bigrams:SYM PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN VERB': 0.0,\n",
       "  'pos_bigrams:VERB PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN X': 0.0,\n",
       "  'pos_bigrams:X PROPN': 0.0,\n",
       "  'pos_bigrams:PROPN SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE PROPN': 0.0,\n",
       "  'pos_bigrams:PUNCT SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT SYM': 0.0,\n",
       "  'pos_bigrams:SYM PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT VERB': 0.0,\n",
       "  'pos_bigrams:VERB PUNCT': 0.14285714285714285,\n",
       "  'pos_bigrams:PUNCT X': 0.0,\n",
       "  'pos_bigrams:X PUNCT': 0.0,\n",
       "  'pos_bigrams:PUNCT SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE PUNCT': 0.0,\n",
       "  'pos_bigrams:SCONJ SYM': 0.0,\n",
       "  'pos_bigrams:SYM SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ VERB': 0.0,\n",
       "  'pos_bigrams:VERB SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ X': 0.0,\n",
       "  'pos_bigrams:X SCONJ': 0.0,\n",
       "  'pos_bigrams:SCONJ SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE SCONJ': 0.0,\n",
       "  'pos_bigrams:SYM VERB': 0.0,\n",
       "  'pos_bigrams:VERB SYM': 0.0,\n",
       "  'pos_bigrams:SYM X': 0.0,\n",
       "  'pos_bigrams:X SYM': 0.0,\n",
       "  'pos_bigrams:SYM SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE SYM': 0.0,\n",
       "  'pos_bigrams:VERB X': 0.0,\n",
       "  'pos_bigrams:X VERB': 0.0,\n",
       "  'pos_bigrams:VERB SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE VERB': 0.0,\n",
       "  'pos_bigrams:X SPACE': 0.0,\n",
       "  'pos_bigrams:SPACE X': 0.0,\n",
       "  'pos_bigrams:ADJ ADJ': 0.0,\n",
       "  'pos_bigrams:ADP ADP': 0.0,\n",
       "  'pos_bigrams:ADV ADV': 0.0,\n",
       "  'pos_bigrams:AUX AUX': 0.0,\n",
       "  'pos_bigrams:CCONJ CCONJ': 0.0,\n",
       "  'pos_bigrams:DET DET': 0.0,\n",
       "  'pos_bigrams:INTJ INTJ': 0.0,\n",
       "  'pos_bigrams:NOUN NOUN': 0.0,\n",
       "  'pos_bigrams:NUM NUM': 0.0,\n",
       "  'pos_bigrams:PART PART': 0.0,\n",
       "  'pos_bigrams:PRON PRON': 0.0,\n",
       "  'pos_bigrams:PROPN PROPN': 0.0,\n",
       "  'pos_bigrams:PUNCT PUNCT': 0.2857142857142857,\n",
       "  'pos_bigrams:SCONJ SCONJ': 0.0,\n",
       "  'pos_bigrams:SYM SYM': 0.0,\n",
       "  'pos_bigrams:VERB VERB': 0.0,\n",
       "  'pos_bigrams:X X': 0.0,\n",
       "  'pos_bigrams:SPACE SPACE': 0.0,\n",
       "  'func_words:i': 0.0,\n",
       "  'func_words:me': 0.0,\n",
       "  'func_words:my': 0.0,\n",
       "  'func_words:myself': 0.0,\n",
       "  'func_words:we': 0.0,\n",
       "  'func_words:our': 0.0,\n",
       "  'func_words:ours': 0.0,\n",
       "  'func_words:ourselves': 0.0,\n",
       "  'func_words:you': 0.0,\n",
       "  \"func_words:'re\": 0.0,\n",
       "  \"func_words:'ve\": 0.0,\n",
       "  \"func_words:'ll\": 0.0,\n",
       "  \"func_words:'d\": 0.0,\n",
       "  \"func_words:'s\": 0.0,\n",
       "  \"func_words:'t\": 0.0,\n",
       "  'func_words:your': 0.0,\n",
       "  'func_words:yours': 0.0,\n",
       "  'func_words:yourself': 0.0,\n",
       "  'func_words:yourselves': 0.0,\n",
       "  'func_words:he': 0.0,\n",
       "  'func_words:him': 0.0,\n",
       "  'func_words:his': 0.0,\n",
       "  'func_words:himself': 0.0,\n",
       "  'func_words:she': 0.0,\n",
       "  'func_words:her': 0.0,\n",
       "  'func_words:ers': 0.0,\n",
       "  'func_words:herself': 0.0,\n",
       "  'func_words:it': 0.0,\n",
       "  'func_words:its': 0.0,\n",
       "  'func_words:itself': 0.0,\n",
       "  'func_words:they': 0.0,\n",
       "  'func_words:them': 0.0,\n",
       "  'func_words:their': 0.0,\n",
       "  'func_words:theirs': 0.0,\n",
       "  'func_words:themselves': 0.0,\n",
       "  'func_words:what': 0.0,\n",
       "  'func_words:which': 0.0,\n",
       "  'func_words:who': 0.0,\n",
       "  'func_words:this': 0.0,\n",
       "  'func_words:that': 0.0,\n",
       "  'func_words:these': 0.0,\n",
       "  'func_words:those': 0.0,\n",
       "  'func_words:am': 0.0,\n",
       "  'func_words:is': 0.0,\n",
       "  'func_words:are': 0.0,\n",
       "  'func_words:was': 0.0,\n",
       "  'func_words:were': 0.0,\n",
       "  'func_words:be': 0.0,\n",
       "  'func_words:been': 0.0,\n",
       "  'func_words:being': 0.0,\n",
       "  'func_words:have': 0.0,\n",
       "  'func_words:has': 0.0,\n",
       "  'func_words:had': 0.0,\n",
       "  'func_words:having': 0.0,\n",
       "  'func_words:do': 0.0,\n",
       "  'func_words:does': 0.0,\n",
       "  'func_words:did': 0.0,\n",
       "  'func_words:doing': 0.0,\n",
       "  'func_words:a': 0.0,\n",
       "  'func_words:an': 0.0,\n",
       "  'func_words:the': 0.0,\n",
       "  'func_words:and': 0.0,\n",
       "  'func_words:but': 0.0,\n",
       "  'func_words:if': 0.0,\n",
       "  'func_words:or': 0.0,\n",
       "  'func_words:because': 0.0,\n",
       "  'func_words:as': 0.0,\n",
       "  'func_words:until': 0.0,\n",
       "  'func_words:while': 0.0,\n",
       "  'func_words:of': 0.0,\n",
       "  'func_words:at': 0.0,\n",
       "  'func_words:by': 0.0,\n",
       "  'func_words:for': 0.0,\n",
       "  'func_words:with': 0.0,\n",
       "  'func_words:about': 0.0,\n",
       "  'func_words:against': 0.0,\n",
       "  'func_words:between': 0.0,\n",
       "  'func_words:into': 0.0,\n",
       "  'func_words:through': 0.0,\n",
       "  'func_words:during': 0.0,\n",
       "  'func_words:before': 0.0,\n",
       "  'func_words:after': 0.0,\n",
       "  'func_words:above': 0.0,\n",
       "  'func_words:below': 0.0,\n",
       "  'func_words:to': 1.0,\n",
       "  'func_words:from': 0.0,\n",
       "  'func_words:up': 0.0,\n",
       "  'func_words:down': 0.0,\n",
       "  'func_words:in': 0.0,\n",
       "  'func_words:out': 0.0,\n",
       "  'func_words:on': 0.0,\n",
       "  'func_words:off': 0.0,\n",
       "  'func_words:over': 0.0,\n",
       "  'func_words:under': 0.0,\n",
       "  'func_words:again': 0.0,\n",
       "  'func_words:further': 0.0,\n",
       "  'func_words:then': 0.0,\n",
       "  'func_words:once': 0.0,\n",
       "  'func_words:here': 0.0,\n",
       "  'func_words:there': 0.0,\n",
       "  'func_words:when': 0.0,\n",
       "  'func_words:where': 0.0,\n",
       "  'func_words:why': 0.0,\n",
       "  'func_words:how': 0.0,\n",
       "  'func_words:all': 0.0,\n",
       "  'func_words:any': 0.0,\n",
       "  'func_words:both': 0.0,\n",
       "  'func_words:each': 0.0,\n",
       "  'func_words:few': 0.0,\n",
       "  'func_words:more': 0.0,\n",
       "  'func_words:most': 0.0,\n",
       "  'func_words:other': 0.0,\n",
       "  'func_words:some': 0.0,\n",
       "  'func_words:such': 0.0,\n",
       "  'func_words:no': 0.0,\n",
       "  'func_words:nor': 0.0,\n",
       "  'func_words:not': 0.0,\n",
       "  'func_words:only': 0.0,\n",
       "  'func_words:own': 0.0,\n",
       "  'func_words:same': 0.0,\n",
       "  'func_words:so': 0.0,\n",
       "  'func_words:than': 0.0,\n",
       "  'func_words:too': 0.0,\n",
       "  'func_words:very': 0.0,\n",
       "  'func_words:can': 0.0,\n",
       "  'func_words:will': 0.0,\n",
       "  'func_words:just': 0.0,\n",
       "  'func_words:don': 0.0,\n",
       "  'func_words:should': 0.0,\n",
       "  'func_words:now': 0.0,\n",
       "  'func_words:ain': 0.0,\n",
       "  'func_words:aren': 0.0,\n",
       "  'func_words:couldn': 0.0,\n",
       "  'func_words:didn': 0.0,\n",
       "  'func_words:doesn': 0.0,\n",
       "  'func_words:hadn': 0.0,\n",
       "  'func_words:hasn': 0.0,\n",
       "  'func_words:haven': 0.0,\n",
       "  'func_words:isn': 0.0,\n",
       "  'func_words:ma': 0.0,\n",
       "  'func_words:shouldn': 0.0,\n",
       "  'func_words:wasn': 0.0,\n",
       "  'func_words:weren': 0.0,\n",
       "  'func_words:won': 0.0,\n",
       "  'func_words:wouldn': 0.0,\n",
       "  'punctuation:.': 0.25,\n",
       "  'punctuation:,': 0.0,\n",
       "  'punctuation::': 0.0,\n",
       "  'punctuation:;': 0.0,\n",
       "  \"punctuation:'\": 0.0,\n",
       "  'punctuation:\"': 0.0,\n",
       "  'punctuation:?': 0.0,\n",
       "  'punctuation:!': 0.75,\n",
       "  'punctuation:`': 0.0,\n",
       "  'punctuation:*': 0.0,\n",
       "  'punctuation:&': 0.0,\n",
       "  'punctuation:_': 0.0,\n",
       "  'punctuation:-': 0.0,\n",
       "  'punctuation:%': 0.0,\n",
       "  'punctuation:(': 0.0,\n",
       "  'punctuation:)': 0.0,\n",
       "  'punctuation:â€“': 0.0,\n",
       "  'punctuation:â€˜': 0.0,\n",
       "  'punctuation:â€™': 0.0,\n",
       "  'emojis:ðŸ˜…': 0.0,\n",
       "  'emojis:ðŸ˜‚': 0.0,\n",
       "  'emojis:ðŸ˜Š': 0.0,\n",
       "  'emojis:â¤ï¸': 0.0,\n",
       "  'emojis:ðŸ˜­': 0.0,\n",
       "  'emojis:ðŸ‘': 0.0,\n",
       "  'emojis:ðŸ‘Œ': 0.0,\n",
       "  'emojis:ðŸ˜': 0.0,\n",
       "  'emojis:ðŸ’•': 0.0,\n",
       "  'emojis:ðŸ¥°': 0.0,\n",
       "  'dep_labels:ROOT': 0.0,\n",
       "  'dep_labels:acl': 0.0,\n",
       "  'dep_labels:acomp': 0.0,\n",
       "  'dep_labels:advcl': 0.0,\n",
       "  'dep_labels:advmod': 0.0,\n",
       "  'dep_labels:agent': 0.0,\n",
       "  'dep_labels:amod': 0.0,\n",
       "  'dep_labels:appos': 0.0,\n",
       "  'dep_labels:attr': 0.0,\n",
       "  'dep_labels:aux': 0.0,\n",
       "  'dep_labels:auxpass': 0.0,\n",
       "  'dep_labels:case': 0.0,\n",
       "  'dep_labels:cc': 0.0,\n",
       "  'dep_labels:ccomp': 0.0,\n",
       "  'dep_labels:compound': 0.0,\n",
       "  'dep_labels:conj': 0.0,\n",
       "  'dep_labels:csubj': 0.0,\n",
       "  'dep_labels:csubjpass': 0.0,\n",
       "  'dep_labels:dative': 0.0,\n",
       "  'dep_labels:dep': 0.0,\n",
       "  'dep_labels:det': 0.0,\n",
       "  'dep_labels:dobj': 0.0,\n",
       "  'dep_labels:expl': 0.0,\n",
       "  'dep_labels:intj': 0.0,\n",
       "  'dep_labels:mark': 0.0,\n",
       "  'dep_labels:meta': 0.0,\n",
       "  'dep_labels:neg': 0.0,\n",
       "  'dep_labels:nmod': 0.0,\n",
       "  'dep_labels:npadvmod': 0.0,\n",
       "  'dep_labels:nsubj': 0.0,\n",
       "  'dep_labels:nsubjpass': 0.0,\n",
       "  'dep_labels:nummod': 0.0,\n",
       "  'dep_labels:oprd': 0.0,\n",
       "  'dep_labels:parataxis': 0.0,\n",
       "  'dep_labels:pcomp': 0.0,\n",
       "  'dep_labels:pobj': 0.0,\n",
       "  'dep_labels:poss': 0.0,\n",
       "  'dep_labels:preconj': 0.0,\n",
       "  'dep_labels:predet': 0.0,\n",
       "  'dep_labels:prep': 0.0,\n",
       "  'dep_labels:prt': 0.0,\n",
       "  'dep_labels:punct': 0.0,\n",
       "  'dep_labels:quantmod': 0.0,\n",
       "  'dep_labels:relcl': 0.0,\n",
       "  'dep_labels:xcomp': 0.0,\n",
       "  'morph_tags:Aspect=Perf': 0.0,\n",
       "  'morph_tags:Aspect=Prog': 0.0,\n",
       "  'morph_tags:Case=Acc': 0.0,\n",
       "  'morph_tags:Case=Nom': 0.0,\n",
       "  'morph_tags:ConjType=Cmp': 0.0,\n",
       "  'morph_tags:Definite=Def': 0.0,\n",
       "  'morph_tags:Definite=Ind': 0.0,\n",
       "  'morph_tags:Degree=Cmp': 0.0,\n",
       "  'morph_tags:Degree=Pos': 0.125,\n",
       "  'morph_tags:Degree=Sup': 0.0,\n",
       "  'morph_tags:Foreign=Yes': 0.0,\n",
       "  'morph_tags:Gender=Fem': 0.0,\n",
       "  'morph_tags:Gender=Masc': 0.0,\n",
       "  'morph_tags:Gender=Neut': 0.0,\n",
       "  'morph_tags:Hyph=Yes': 0.0,\n",
       "  'morph_tags:Mood=Ind': 0.0,\n",
       "  'morph_tags:NumType=Card': 0.0,\n",
       "  'morph_tags:NumType=Mult': 0.0,\n",
       "  'morph_tags:NumType=Ord': 0.0,\n",
       "  'morph_tags:Number=Plur': 0.0,\n",
       "  'morph_tags:Number=Sing': 0.25,\n",
       "  'morph_tags:Person=1': 0.0,\n",
       "  'morph_tags:Person=2': 0.0,\n",
       "  'morph_tags:Person=3': 0.0,\n",
       "  'morph_tags:Polarity=Neg': 0.0,\n",
       "  'morph_tags:Poss=Yes': 0.0,\n",
       "  'morph_tags:PronType=Art': 0.0,\n",
       "  'morph_tags:PronType=Dem': 0.0,\n",
       "  'morph_tags:PronType=Ind': 0.0,\n",
       "  'morph_tags:PronType=Prs': 0.0,\n",
       "  'morph_tags:PronType=Rel': 0.0,\n",
       "  'morph_tags:PunctSide=Fin': 0.0,\n",
       "  'morph_tags:PunctSide=Ini': 0.0,\n",
       "  'morph_tags:PunctType=Brck': 0.0,\n",
       "  'morph_tags:PunctType=Comm': 0.0,\n",
       "  'morph_tags:PunctType=Dash': 0.0,\n",
       "  'morph_tags:PunctType=Peri': 0.5,\n",
       "  'morph_tags:PunctType=Quot': 0.0,\n",
       "  'morph_tags:Reflex=Yes': 0.0,\n",
       "  'morph_tags:Tense=Past': 0.0,\n",
       "  'morph_tags:Tense=Pres': 0.0,\n",
       "  'morph_tags:VerbForm=Fin': 0.0,\n",
       "  'morph_tags:VerbForm=Ger': 0.0,\n",
       "  'morph_tags:VerbForm=Inf': 0.125,\n",
       "  'morph_tags:VerbForm=Part': 0.0,\n",
       "  'morph_tags:VerbType=Mod': 0.0,\n",
       "  'sentences:it-cleft': 0.0,\n",
       "  'sentences:pseudo-cleft': 0.0,\n",
       "  'sentences:all-cleft': 0.0,\n",
       "  'sentences:there-cleft': 0.0,\n",
       "  'sentences:if-because-cleft': 0.0,\n",
       "  'sentences:passive': 0.0,\n",
       "  'sentences:subj-relcl': 0.0,\n",
       "  'sentences:obj-relcl': 0.0,\n",
       "  'sentences:tag-question': 0.0,\n",
       "  'sentences:coordinate-clause': 0.0},\n",
       " 'emotion': {'admiration': 0,\n",
       "  'approval': 0,\n",
       "  'optimism': 0,\n",
       "  'neutral': 0,\n",
       "  'gratitude': 0,\n",
       "  'excitement': 0,\n",
       "  'pride': 0,\n",
       "  'joy': 0,\n",
       "  'caring': 0,\n",
       "  'desire': 0,\n",
       "  'relief': 0,\n",
       "  'realization': 0,\n",
       "  'disapproval': 0,\n",
       "  'annoyance': 0,\n",
       "  'disappointment': 0,\n",
       "  'surprise': 0,\n",
       "  'curiosity': 0,\n",
       "  'love': 0,\n",
       "  'confusion': 0,\n",
       "  'sadness': 0,\n",
       "  'amusement': 0,\n",
       "  'fear': 0,\n",
       "  'grief': 0,\n",
       "  'disgust': 0,\n",
       "  'anger': 0,\n",
       "  'nervousness': 0,\n",
       "  'remorse': 0,\n",
       "  'embarrassment': 0,\n",
       "  'unknown': 1},\n",
       " 'emoji_task': ['ðŸ˜‚', 'ðŸ˜Ž'],\n",
       " 'emotion_task': ['optimism', 'joy'],\n",
       " 'hate_task': ['not-hate', 'not-hate'],\n",
       " 'irony_task': ['irony', 'irony'],\n",
       " 'offensive_task': ['not-offensive', 'not-offensive'],\n",
       " 'sentiment_task': ['positive', 'positive'],\n",
       " 'casing': ['lower', 'lower'],\n",
       " 'morph_tags': {'morph_tags:Aspect=Perf': 0.0,\n",
       "  'morph_tags:Aspect=Prog': 0.0,\n",
       "  'morph_tags:Case=Acc': 0.0,\n",
       "  'morph_tags:Case=Nom': 0.0,\n",
       "  'morph_tags:ConjType=Cmp': 0.0,\n",
       "  'morph_tags:Definite=Def': 0.0,\n",
       "  'morph_tags:Definite=Ind': 0.0,\n",
       "  'morph_tags:Degree=Cmp': 0.0,\n",
       "  'morph_tags:Degree=Pos': 0.125,\n",
       "  'morph_tags:Degree=Sup': 0.0,\n",
       "  'morph_tags:Foreign=Yes': 0.0,\n",
       "  'morph_tags:Gender=Fem': 0.0,\n",
       "  'morph_tags:Gender=Masc': 0.0,\n",
       "  'morph_tags:Gender=Neut': 0.0,\n",
       "  'morph_tags:Hyph=Yes': 0.0,\n",
       "  'morph_tags:Mood=Ind': 0.0,\n",
       "  'morph_tags:NumType=Card': 0.0,\n",
       "  'morph_tags:NumType=Mult': 0.0,\n",
       "  'morph_tags:NumType=Ord': 0.0,\n",
       "  'morph_tags:Number=Plur': 0.0,\n",
       "  'morph_tags:Number=Sing': 0.25,\n",
       "  'morph_tags:Person=1': 0.0,\n",
       "  'morph_tags:Person=2': 0.0,\n",
       "  'morph_tags:Person=3': 0.0,\n",
       "  'morph_tags:Polarity=Neg': 0.0,\n",
       "  'morph_tags:Poss=Yes': 0.0,\n",
       "  'morph_tags:PronType=Art': 0.0,\n",
       "  'morph_tags:PronType=Dem': 0.0,\n",
       "  'morph_tags:PronType=Ind': 0.0,\n",
       "  'morph_tags:PronType=Prs': 0.0,\n",
       "  'morph_tags:PronType=Rel': 0.0,\n",
       "  'morph_tags:PunctSide=Fin': 0.0,\n",
       "  'morph_tags:PunctSide=Ini': 0.0,\n",
       "  'morph_tags:PunctType=Brck': 0.0,\n",
       "  'morph_tags:PunctType=Comm': 0.0,\n",
       "  'morph_tags:PunctType=Dash': 0.0,\n",
       "  'morph_tags:PunctType=Peri': 0.5,\n",
       "  'morph_tags:PunctType=Quot': 0.0,\n",
       "  'morph_tags:Reflex=Yes': 0.0,\n",
       "  'morph_tags:Tense=Past': 0.0,\n",
       "  'morph_tags:Tense=Pres': 0.0,\n",
       "  'morph_tags:VerbForm=Fin': 0.0,\n",
       "  'morph_tags:VerbForm=Ger': 0.0,\n",
       "  'morph_tags:VerbForm=Inf': 0.125,\n",
       "  'morph_tags:VerbForm=Part': 0.0,\n",
       "  'morph_tags:VerbType=Mod': 0.0},\n",
       " 'punc_tags': {'punctuation:.': 0.25,\n",
       "  'punctuation:,': 0.0,\n",
       "  'punctuation::': 0.0,\n",
       "  'punctuation:;': 0.0,\n",
       "  \"punctuation:'\": 0.0,\n",
       "  'punctuation:\"': 0.0,\n",
       "  'punctuation:?': 0.0,\n",
       "  'punctuation:!': 0.75,\n",
       "  'punctuation:`': 0.0,\n",
       "  'punctuation:*': 0.0,\n",
       "  'punctuation:&': 0.0,\n",
       "  'punctuation:_': 0.0,\n",
       "  'punctuation:-': 0.0,\n",
       "  'punctuation:%': 0.0,\n",
       "  'punctuation:(': 0.0,\n",
       "  'punctuation:)': 0.0,\n",
       "  'punctuation:â€“': 0.0,\n",
       "  'punctuation:â€˜': 0.0,\n",
       "  'punctuation:â€™': 0.0,\n",
       "  'none': 0}}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_possible_values(data, key):\n",
    "    if isinstance(data[0][key], dict):\n",
    "        return sorted(data[0][key].keys())\n",
    "    else:\n",
    "        values = []\n",
    "        for d in data:\n",
    "            values.extend(d[key])\n",
    "        return sorted(set(values))\n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "59f3485f-3f7a-4aa8-8770-0b5bc24e2465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Randy, Can you send me a schedule of the salary and level of everyone in the scheduling group. Plus your thoughts on any changes that need to be made. (Patti S for example) Phillip',\n",
       " 'formality': array([4, 0]),\n",
       " 'casing': array([0, 4, 0]),\n",
       " 'emoji_task': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])}"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_feature_to_names = {}\n",
    "#  'sentences',\n",
    "#  'irony_task'\n",
    "# 'punc_tags' < need to unnormalize\n",
    "# 'morph_tags' < need to unnormalize\n",
    "\n",
    "\n",
    "# for key in ['formality', 'punc_tags', 'morph_tags', 'pos', 'casing', 'question',  'passive', 'emotion_task', 'sentiment_task', 'emoji_task']:\n",
    "# for key in ['formality', 'punc_tags', 'pos', 'casing', 'emoji_task']:\n",
    "\n",
    "# NICK: Unsure which of of these features we should include!\n",
    "for key in ['formality', 'casing', 'emoji_task']:\n",
    "    meta_feature_to_names[key] = get_possible_values(data, key)\n",
    "    \n",
    "\n",
    "extracted_features = [extract_features(d, meta_feature_to_names) for d in data]\n",
    "\n",
    "extracted_features[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec51068-a48f-46e4-9ae6-c1b4eb9d8244",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d96ec-eea5-45d0-8ab2-4fe1780e5824",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "4f79754a-9fa0-4fdd-a647-0084e702f05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 24.03it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 80.79it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "vectorizer.fit([d['text'] for d in extracted_features])\n",
    "\n",
    "training = []\n",
    "holdout = []\n",
    "\n",
    "authors = [d['info']['from'] for d in data]\n",
    "for author, d  in zip(authors, extracted_features):\n",
    "    if author.startswith('gpt') or author in SPLITS['train']:\n",
    "        training.append(d)\n",
    "\n",
    "    else:\n",
    "        holdout.append(d)\n",
    "        assert author in SPLITS['test']\n",
    "        \n",
    "bows = {}\n",
    "meta_vectorized = {}\n",
    "\n",
    "for split_name, data_split in zip(['training','holdout'], [training, holdout]):\n",
    "     bows[split_name] = vectorizer.transform([d['text'] for d in data_split])\n",
    "     meta_vectorized[split_name] = {}\n",
    "     for key in tqdm(sorted(data_split[0].keys())):\n",
    "        if key == 'text': continue\n",
    "        meta_vectorized[split_name][key] = sparse.csr_matrix(np.stack([d[key] for d in data_split]))\n",
    "        assert(bows[split_name].shape[0] == meta_vectorized[split_name][key].shape[0])\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d9a2a-6288-40a4-a62f-400f8c218402",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bag-of-POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "d7024959-053c-456a-b3a6-2da1ebcc4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_tags = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "853c2d94-7d2f-43c4-aff7-e3c61bf1711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pos_rep(text, tagset):\n",
    "#     emb = {tag: 0 for tag in uni_tags}\n",
    "#     tags = [tag[1] for tag in pos_tag(word_tokenize(test_txt), tagset = 'universal')]\n",
    "#     for tag in tags:\n",
    "#         emb[tag] += 1\n",
    "#     return list(emb.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "e7303c4c-5106-4b59-ba47-a6977ca3eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bops = data['text'].progress_apply(lambda txt: get_pos_rep(txt, uni_tags)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "20a0166c-b4d0-4dd0-80dd-6f5fbca9b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bops = sparse.csr_matrix(bops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97291dbb-5aa9-4612-8961-a629cfa16c13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "fa09ece7-3827-47a7-b7fc-a441a589298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DOCS   = bows['training'].shape[0]\n",
    "VOCAB_SIZE = bows['training'].shape[1]\n",
    "META_SIZE  = {k:v.shape[1] for k,v in meta_vectorized['training'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "14566e36-f4aa-4812-8633-d6e6c638f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Dims: (66668, 9267)\n",
      "META Dims: {'casing': 3, 'emoji_task': 20, 'formality': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'BOW Dims: {bows[\"training\"].shape}')\n",
    "print(f'META Dims: {META_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e3a9e-420d-4b50-887a-6e00b5ec98f7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "eae84587-4202-4136-a98a-8cc5f52b4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocMetaData(Dataset):\n",
    "    \n",
    "    def __init__(self, bows, metas, dtype = np.float32):\n",
    "        self.bows = bows\n",
    "        self.metas = metas\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.bows.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        bow = self.bows[idx].toarray().astype(self.dtype)[0]\n",
    "        \n",
    "        meta = {key:self.metas[key][idx].toarray().astype(self.dtype)[0] for key in self.metas}\n",
    "        \n",
    "        batch = {\n",
    "            'bow': bow,\n",
    "            'meta': meta,\n",
    "        }\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e16dec22-8379-430b-8f25-5675e8ef8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DocMetaData(bows['training'], meta_vectorized['training'])\n",
    "eval_dataset = DocMetaData(bows['training'], meta_vectorized['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e231698c-1fd3-48de-98ee-43843613b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle=True)\n",
    "eval_dl = DataLoader(eval_dataset, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "eb31ab8a-2164-4454-93be-ea3fb6899331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "meta {'casing': tensor([[0., 2., 0.],\n",
      "        [1., 2., 0.],\n",
      "        [1., 2., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]]), 'emoji_task': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'formality': tensor([[1., 1.],\n",
      "        [2., 1.],\n",
      "        [2., 1.],\n",
      "        ...,\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "test_ = next(iter(dl))\n",
    "for k, v in test_.items():\n",
    "    print(k, v)\n",
    "    # print(f'{k:7s}: {str(v.shape):25s} {str(v.dtype):25s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b0ce8-9814-431f-8ecb-f557327c933d",
   "metadata": {},
   "source": [
    "# Model Definition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e5545-1066-4331-8d2f-4afc11fe9469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "bcd5ea24-a797-4986-afb4-b0806a3d8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     # Base class for the encoder net, used in the guide\n",
    "#     def __init__(self, vocab_size, num_topics, hidden, dropout, eps = 1e-10):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.eps = eps\n",
    "        \n",
    "#         self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "#         self.fc1 = nn.Linear(vocab_size, hidden)\n",
    "#         self.fc2 = nn.Linear(hidden, hidden)\n",
    "#         self.fcmu = nn.Linear(hidden, num_topics)\n",
    "#         self.fclv = nn.Linear(hidden, num_topics)\n",
    "\n",
    "#         self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "#         self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         h = F.softplus(self.fc1(inputs))\n",
    "#         h = F.softplus(self.fc2(h))\n",
    "#         h = self.drop(h)\n",
    "#         # Î¼ and Î£ are the outputs\n",
    "#         logtheta_loc = self.bnmu(self.fcmu(h))\n",
    "#         logtheta_logvar = self.bnlv(self.fclv(h))\n",
    "#         logtheta_scale = self.eps + (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "#         return logtheta_loc, logtheta_scale\n",
    "    \n",
    "# class MetaDocEncoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, vocab_size, meta_size, num_styles, hidden, dropout, eps = 1e-10):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.eps = eps\n",
    "        \n",
    "#         self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "#         self.fc1_doc = nn.Linear(vocab_size, hidden)\n",
    "#         self.fc1_meta = nn.Linear(meta_size, hidden)\n",
    "#         self.fc2 = nn.Linear(2 * hidden, hidden)\n",
    "#         self.fcmu = nn.Linear(hidden, num_styles)\n",
    "#         self.fclv = nn.Linear(hidden, num_styles)\n",
    "\n",
    "#         self.bnmu = nn.BatchNorm1d(num_styles, affine=False)  # to avoid component collapse\n",
    "#         self.bnlv = nn.BatchNorm1d(num_styles, affine=False)  # to avoid component collapse\n",
    "\n",
    "#     def forward(self, inputs_doc, inputs_meta):\n",
    "#         h_doc = F.softplus(self.fc1_doc(inputs_doc))\n",
    "#         h_meta = F.softplus(self.fc1_meta(inputs_meta))\n",
    "#         h = torch.hstack([h_doc, h_meta])\n",
    "#         h = F.softplus(self.fc2(h))\n",
    "#         h = self.drop(h)\n",
    "#         # Î¼ and Î£ are the outputs\n",
    "#         logkappa_loc = self.bnmu(self.fcmu(h))\n",
    "#         logkappa_logvar = self.bnlv(self.fclv(h))\n",
    "#         logkappa_scale = self.eps + (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "#         return logkappa_loc, logkappa_scale\n",
    "\n",
    "\n",
    "class GeneralEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, size_dict, num_styles, hidden, dropout, eps = 1e-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eps = eps\n",
    "\n",
    "        self.sizes = size_dict\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        \n",
    "        # self.fc1_doc = nn.Linear(vocab_size, hidden)\n",
    "        # self.fc1_meta = nn.Linear(meta_size, hidden)\n",
    "\n",
    "        self.features = sorted(size_dict.keys())\n",
    "        self.fc1s = nn.ModuleDict({feature:nn.Linear(self.sizes[feature], hidden) for feature in self.features})\n",
    "        self.fc2 = nn.Linear(len(self.features) * hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_styles)\n",
    "        self.fclv = nn.Linear(hidden, num_styles)\n",
    "\n",
    "        self.bnmu = nn.BatchNorm1d(num_styles, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_styles, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs): #inputs_doc, inputs_meta):\n",
    "        assert isinstance(inputs, dict)\n",
    "        first_hiddens = []\n",
    "        for _, feature in enumerate(self.features):\n",
    "            first_hiddens.append(F.softplus(self.fc1s[feature](inputs[feature])))\n",
    "        \n",
    "        h = torch.hstack(first_hiddens)\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # Î¼ and Î£ are the outputs\n",
    "        logkappa_loc = self.bnmu(self.fcmu(h))\n",
    "        logkappa_logvar = self.bnlv(self.fclv(h))\n",
    "        logkappa_scale = self.eps + (0.5 * logkappa_logvar).exp()  # Enforces positivity\n",
    "        return logkappa_loc, logkappa_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "db798a1e-8552-4d0a-b185-14b3e4425bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.drop(inputs)\n",
    "        # the output is Ïƒ(Î²Î¸)\n",
    "        return self.bn(self.beta(inputs))\n",
    "    \n",
    "class MetaDocDecoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, num_styles, dropout):\n",
    "        super().__init__()\n",
    "        self.t_beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.s_beta = nn.Linear(num_styles, vocab_size, bias=False)\n",
    "        self.t_bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.s_bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs_doc, inputs_meta):\n",
    "        inputs_doc  = self.drop(inputs_doc)\n",
    "        inputs_meta = self.drop(inputs_meta)\n",
    "        # the output is Ïƒ(Î²Î¸)\n",
    "        \n",
    "        dist_t = self.t_bn(self.t_beta(inputs_doc))\n",
    "        dist_s = self.s_bn(self.s_beta(inputs_meta))\n",
    "        \n",
    "        return 0.5 * (dist_t + dist_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3b961-5983-4575-af2a-b5fe5ca56f84",
   "metadata": {},
   "source": [
    "## ProdSLDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "d7d033aa-6189-415b-8d37-80ca01b8dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdSLDA(nn.Module):\n",
    "    \n",
    "    PRIOR_DISTS  = {'gaussian': dist.Normal,\n",
    "                    'laplace': dist.Laplace,\n",
    "                   }\n",
    "    TK_LINKS     = ('none', # Model style and documents independently\n",
    "                    'kappa_doc', # Allow kappa to effect word distributions\n",
    "                    'kappa_doc_style', # Allow kappa to effect word distributions and sampled words to effect style\n",
    "                   )\n",
    "    \n",
    "    def __init__(self, vocab_size, meta_sizes, num_topics, num_styles, hidden, dropout, \n",
    "                 theta_prior_dist = 'gaussian', theta_prior_loc = 0., theta_prior_scale = 1.,\n",
    "                 kappa_prior_dist = 'laplace', kappa_prior_loc = 0., kappa_prior_scale = 1.,\n",
    "                 style_topic_link = 'none',\n",
    "                 eps = 1e-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Global model variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.meta_sizes = meta_sizes\n",
    "        self.num_topics = num_topics\n",
    "        self.num_styles = num_styles\n",
    "        self.hidden     = hidden\n",
    "        self.dropout    = dropout\n",
    "\n",
    "        self.meta_features = sorted(self.meta_sizes.keys())\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "        # Theta Prior\n",
    "        if theta_prior_dist not in ProdSLDA.PRIOR_DISTS.keys():\n",
    "            raise ValueError(f'Theta prior {theta_prior_dist} not yet implemented. Must be one of {\", \".join(ProdSLDA.PRIOR_DISTS.keys())}')\n",
    "        self.theta_prior_dist = theta_prior_dist\n",
    "        \n",
    "        self.theta_prior_scale = theta_prior_scale\n",
    "        self.theta_prior_loc = theta_prior_loc\n",
    "        \n",
    "        # Kappa Prior\n",
    "        if kappa_prior_dist not in ProdSLDA.PRIOR_DISTS.keys():\n",
    "            raise ValueError(f'Kappa prior {kappa_prior_dist} not yet implemented. Must be one of {\", \".join(ProdSLDA.PRIOR_DISTS.keys())}')\n",
    "        self.kappa_prior_dist = kappa_prior_dist\n",
    "        \n",
    "        self.kappa_prior_scale = kappa_prior_scale\n",
    "        self.kappa_prior_loc = kappa_prior_loc\n",
    "        \n",
    "        \n",
    "        # Document style linking\n",
    "        self.style_topic_link = style_topic_link\n",
    "        \n",
    "        if self.style_topic_link not in ProdSLDA.TK_LINKS:\n",
    "            raise ValueError(f'Link {self.style_topic_link} not yet implemented. Must be one of {\", \".join(ProdSLDA.TK_LINKS)}')\n",
    "        elif self.style_topic_link == 'none':\n",
    "            # Independent modeling of style and topic, all normal encoder/decoders\n",
    "            \n",
    "            self.encoder = GeneralEncoder({'doc':vocab_size}, num_topics, hidden, dropout, self.eps)\n",
    "            self.decoder = Decoder(vocab_size, num_topics, dropout)\n",
    "            self.style_encoder = GeneralEncoder(meta_sizes, num_styles, hidden, dropout, self.eps)\n",
    "            self.style_decoder = nn.ModuleDict({feature: Decoder(meta_s, num_styles, dropout) for feature, meta_s in meta_sizes.items()})\n",
    "            \n",
    "        elif self.style_topic_link == 'kappa_doc':\n",
    "            # raise NotImplementedError()\n",
    "            # Doc influences kappa encoding, style encoder takes in doc\n",
    "            self.encoder = GeneralEncoder({'doc':vocab_size}, num_styles, hidden, dropout, self.eps)\n",
    "            self.style_encoder = GeneralEncoder({'doc':vocab_size, **meta_sizes}, num_styles, hidden, dropout, self.eps)\n",
    "\n",
    "            self.decoder = MetaDocDecoder(vocab_size=vocab_size, num_topics=num_topics, num_styles=num_styles, dropout=dropout)\n",
    "            self.style_decoder = nn.ModuleDict({feature: Decoder(meta_s, num_styles, dropout) for feature, meta_s in meta_sizes.items()})\n",
    "\n",
    "\n",
    "    def model(self, docs, metas):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        pyro.module(\"style_decoder\", self.style_decoder)\n",
    "        \n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ð‘(ðœƒ|ð›¼) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics)) * self.theta_prior_loc\n",
    "            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics)) * self.theta_prior_scale\n",
    "            logkappa_loc = docs.new_zeros((docs.shape[0], self.num_styles)) * self.kappa_prior_loc\n",
    "            logkappa_scale = docs.new_ones((docs.shape[0], self.num_styles)) * self.kappa_prior_scale\n",
    "            \n",
    "            # if self.style_topic_link == 'kappa_doc':\n",
    "                # logtheta_s_loc = docs.new_zeros((docs.shape[0], self.num_topics)) * self.theta_prior_loc\n",
    "                # logtheta_s_scale = docs.new_ones((docs.shape[0], self.num_topics)) * self.theta_prior_scale\n",
    "                \n",
    "                # logtheta_s = pyro.sample(\n",
    "                    # \"logtheta_s\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_s_loc, logtheta_s_scale).to_event(1))\n",
    "                \n",
    "                # theta_s = F.softmax(logtheta_s, -1)\n",
    "            \n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_loc, logtheta_scale).to_event(1))\n",
    "            logkappa = pyro.sample(\n",
    "                \"logkappa\", ProdSLDA.PRIOR_DISTS[self.kappa_prior_dist](logkappa_loc, logkappa_scale).to_event(1))\n",
    "            \n",
    "            theta = F.softmax(logtheta, -1)\n",
    "            kappa = F.softmax(logkappa, -1)\n",
    "\n",
    "            if self.style_topic_link == 'none':\n",
    "                word_logits = self.decoder(theta)\n",
    "            elif self.style_topic_link == 'kappa_doc':\n",
    "                word_logits = self.decoder(theta, kappa)\n",
    "                \n",
    "            style_logits = {feature:self.style_decoder[feature](kappa) for feature in self.meta_features}\n",
    "\n",
    "            total_count = int(docs.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs_doc',\n",
    "                dist.Multinomial(total_count, logits = word_logits),\n",
    "                obs=docs\n",
    "            )\n",
    "\n",
    "            for feature in self.meta_features:\n",
    "                total_s_count = int(metas[feature].sum(-1).max())\n",
    "                pyro.sample(\n",
    "                    'obs_meta_'+feature,\n",
    "                    dist.Multinomial(total_s_count, logits = style_logits[feature]),\n",
    "                    obs=metas[feature]\n",
    "                )\n",
    "\n",
    "    def guide(self, docs, metas):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        pyro.module(\"style_encoder\", self.encoder)\n",
    "            \n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ð‘(ðœƒ|ð›¼) is replaced by a logistic-normal distribution,\n",
    "            # where Î¼ and Î£ are the encoder network outputs\n",
    "            \n",
    "            if self.style_topic_link == 'none':\n",
    "                logtheta_loc, logtheta_scale = self.encoder({'doc':docs})\n",
    "                logkappa_loc, logkappa_scale = self.style_encoder(metas)\n",
    "\n",
    "            elif self.style_topic_link == 'kappa_doc':\n",
    "                # raise NotImplementedError()\n",
    "                # logtheta_loc, logtheta_scale, logkappa_d_loc, logkappa_d_scale = self.encoder({'doc':docs, **metas})\n",
    "                logtheta_loc, logtheta_scale  = self.encoder({'doc':docs})\n",
    "                logkappa_loc, logkappa_scale = self.style_encoder({'doc':docs, **metas})\n",
    "\n",
    "                # NICK what was the point of d_loc and d_scale? Shoudln't we be generating one set of kappas from both features?\n",
    "                \n",
    "                # Average theta loc from document and style\n",
    "                # logkappa_loc = 0.5 * (logkappa_loc + logkappa_d_loc) \n",
    "                # logkappa_scale = 0.5 * (logkappa_scale + logkappa_d_scale)\n",
    "            \n",
    "            # Sample logtheta/logkappa from guide\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", ProdSLDA.PRIOR_DISTS[self.theta_prior_dist](logtheta_loc, logtheta_scale).to_event(1))\n",
    "            logkappa = pyro.sample(\n",
    "                \"logkappa\", ProdSLDA.PRIOR_DISTS[self.kappa_prior_dist](logkappa_loc, logkappa_scale).to_event(1))\n",
    "\n",
    "        return logtheta, logkappa\n",
    "            \n",
    "\n",
    "    def beta_document(self):\n",
    "        if self.style_topic_link == 'none':\n",
    "            return {'beta_topic':self.decoder.beta.weight.cpu().detach().T}\n",
    "        elif self.style_topic_link == 'kappa_doc':\n",
    "            return {\n",
    "                'beta_topic':self.decoder.t_beta.weight.cpu().detach().T,\n",
    "                'beta_style':self.decoder.s_beta.weight.cpu().detach().T,\n",
    "            }\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    def beta_meta(self):\n",
    "        # beta matrix elements are the weights of the FC layer on the decoder\n",
    "        # return self.decoder.beta.weight.cpu().detach().T\n",
    "        retval = {}\n",
    "        for key, layer in self.style_decoder.items():\n",
    "            retval[key] = layer.beta.weight.cpu().detach().T\n",
    "        return retval\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93050dff-fe64-4737-862e-2e6baa0af03e",
   "metadata": {},
   "source": [
    "# Model Fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed443b9-6ebe-4351-aeb7-5c85caff576b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "a6ed2353-d5e5-47d0-a9f9-e37c82db4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 2 #10 #20 # NICK totally subject to changes\n",
    "NUM_STYLES = 5 #20\n",
    "HIDDEN_DIM = 64\n",
    "DROPOUT    = 0 #0.2\n",
    "\n",
    "THETA_PRIOR_DIST = 'gaussian'\n",
    "THETA_PRIOR_LOC = 0.\n",
    "THETA_PRIOR_SCALE = 1.\n",
    "\n",
    "KAPPA_PRIOR_DIST = 'gaussian' #'laplace'\n",
    "KAPPA_PRIOR_LOC = 0.\n",
    "KAPPA_PRIOR_SCALE = 5 #10 #1.\n",
    "\n",
    "STYLE_TOPIC_LINK = 'none' #'kappa_doc' #'none'\n",
    "\n",
    "NUM_EPOCHS = 5 #100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "5cb86678-f33a-49a1-971c-1641b2da6e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta_topic': tensor([[ 0.3219, -0.3228,  0.2473,  ...,  0.2530,  0.4218,  0.3623],\n",
      "        [ 0.6130, -0.4193,  0.2803,  ...,  0.2308, -0.0440,  0.3505]])}\n",
      "{'casing': tensor([[-0.3513, -0.1330, -0.1180],\n",
      "        [ 0.3769,  0.3620, -0.2804],\n",
      "        [ 0.2608,  0.2996,  0.3237],\n",
      "        [-0.1080, -0.0335,  0.0210],\n",
      "        [ 0.1613, -0.3005,  0.4137]]), 'emoji_task': tensor([[-0.2523,  0.2563, -0.2395, -0.0471, -0.1673,  0.2004, -0.0693,  0.4118,\n",
      "          0.4196, -0.2406,  0.1092, -0.1767, -0.0996, -0.4023, -0.0252,  0.3606,\n",
      "         -0.3093,  0.3609, -0.1606,  0.4234],\n",
      "        [-0.0951,  0.1032,  0.2689,  0.3702, -0.0755, -0.1163,  0.0347,  0.0747,\n",
      "         -0.3832,  0.2645,  0.1461,  0.3203,  0.2359, -0.1876, -0.1480,  0.0237,\n",
      "         -0.0767, -0.0182,  0.2285, -0.3614],\n",
      "        [ 0.2259,  0.2566, -0.1772,  0.0814,  0.3011, -0.1660,  0.0536,  0.0503,\n",
      "          0.0225, -0.1134, -0.1785,  0.2309, -0.2858, -0.2297,  0.0615, -0.2114,\n",
      "         -0.2107, -0.2188,  0.4054, -0.0154],\n",
      "        [ 0.2952,  0.2310, -0.1546, -0.2258,  0.0706, -0.0285,  0.2855, -0.0910,\n",
      "          0.4220,  0.0236,  0.2161, -0.2300, -0.3612, -0.1910,  0.3207,  0.2485,\n",
      "          0.3282,  0.2893, -0.0098, -0.2919],\n",
      "        [ 0.3246, -0.0707,  0.0338, -0.2218, -0.3986,  0.1367, -0.3170,  0.0756,\n",
      "         -0.2943, -0.2106,  0.2107,  0.1627, -0.2274, -0.4263, -0.1947,  0.0574,\n",
      "         -0.0489, -0.0608,  0.1593,  0.3274]]), 'formality': tensor([[-0.1072, -0.0893],\n",
      "        [-0.2009,  0.3825],\n",
      "        [ 0.3390, -0.0823],\n",
      "        [ 0.1228, -0.1952],\n",
      "        [ 0.1924, -0.0989]])}\n"
     ]
    }
   ],
   "source": [
    "prod_slda = ProdSLDA(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    meta_sizes  = META_SIZE,\n",
    "    num_topics = NUM_TOPICS, num_styles = NUM_STYLES, \n",
    "    hidden = HIDDEN_DIM, dropout = DROPOUT, \n",
    "    theta_prior_dist = THETA_PRIOR_DIST, \n",
    "    theta_prior_loc = THETA_PRIOR_LOC, theta_prior_scale = THETA_PRIOR_SCALE,\n",
    "    kappa_prior_dist = KAPPA_PRIOR_DIST, \n",
    "    kappa_prior_loc = KAPPA_PRIOR_LOC, kappa_prior_scale = KAPPA_PRIOR_SCALE,\n",
    "    style_topic_link = STYLE_TOPIC_LINK,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(prod_slda.beta_document())\n",
    "print(prod_slda.beta_meta())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf790d2c-25ba-4769-8703-5b9e672a735f",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "4487f140-7c91-4b84-9404-af5d21b233d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Randy, Can you send me a schedule of the salary and level of everyone in the scheduling group. Plus your thoughts on any changes that need to be made. (Patti S for example) Phillip',\n",
       " 'formality': array([4, 0]),\n",
       " 'casing': array([0, 4, 0]),\n",
       " 'emoji_task': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "prod_slda.train()\n",
    "print(DEVICE)\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "2cc93883-15f9-411c-98b0-851ee1f1390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = pyro.optim.ClippedAdam(ADAM_ARGS)\n",
    "\n",
    "svi = SVI(\n",
    "    pyro.poutine.scale(prod_slda.model, BATCH_SIZE/len(dl.dataset)),\n",
    "    pyro.poutine.scale(prod_slda.model, BATCH_SIZE/len(dl.dataset)),\n",
    "    optim,\n",
    "    loss = TraceMeanField_ELBO()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "26707463-b804-4ad2-9eac-a2c4251dfd80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(pyro.poutine.trace(prod_slda.model).get_trace(test_['bow'].to(DEVICE), test_['meta'].to(DEVICE)).format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "4f1bea62-f21c-46d2-9963-9a2b4ee9e90c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Trace Shapes:           \n",
      "                   Param Sites:           \n",
      "      encoder$$$fc1s.doc.weight  64 9267  \n",
      "        encoder$$$fc1s.doc.bias       64  \n",
      "           encoder$$$fc2.weight  64   64  \n",
      "             encoder$$$fc2.bias       64  \n",
      "          encoder$$$fcmu.weight   2   64  \n",
      "            encoder$$$fcmu.bias        2  \n",
      "          encoder$$$fclv.weight   2   64  \n",
      "            encoder$$$fclv.bias        2  \n",
      "style_encoder$$$fc1s.doc.weight  64 9267  \n",
      "  style_encoder$$$fc1s.doc.bias       64  \n",
      "     style_encoder$$$fc2.weight  64   64  \n",
      "       style_encoder$$$fc2.bias       64  \n",
      "    style_encoder$$$fcmu.weight   2   64  \n",
      "      style_encoder$$$fcmu.bias        2  \n",
      "    style_encoder$$$fclv.weight   2   64  \n",
      "      style_encoder$$$fclv.bias        2  \n",
      "                  Sample Sites:           \n",
      "                 documents dist        |  \n",
      "                          value 512    |  \n",
      "                  logtheta dist 512    | 2\n",
      "                          value 512    | 2\n",
      "                  logkappa dist 512    | 5\n",
      "                          value 512    | 5\n"
     ]
    }
   ],
   "source": [
    "print(pyro.poutine.trace(prod_slda.guide).get_trace(test_['bow'].to(DEVICE), {k:v.to(DEVICE) for k,v in test_['meta'].items()}).format_shapes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "e61c37b2-22a8-463a-9c4c-2aaec9f4e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TRAINING---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 0/131 [00:00<?, ?it/s]/burg/nlp/users/zfh2000/miniconda3/lib/python3.11/site-packages/pyro/util.py:288: UserWarning: Found non-auxiliary vars in guide but not model, consider marking these infer={'is_auxiliary': True}:\n",
      "{'obs_meta_formality', 'obs_meta_emoji_task', 'obs_doc', 'obs_meta_casing'}\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:19<00:00,  6.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:22<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 36589.786\n",
      "Epoch 0: Eval: 36471.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:20<00:00,  6.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:23<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 36357.700\n",
      "Epoch 1: Eval: 36375.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:19<00:00,  6.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:19<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 36300.900\n",
      "Epoch 2: Eval: 36315.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:21<00:00,  6.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:19<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 36278.818\n",
      "Epoch 3: Eval: 36237.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:20<00:00,  6.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:22<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 36241.503\n",
      "Epoch 4: Eval: 36240.070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_elbo = []\n",
    "val_elbo = []\n",
    "\n",
    "print(\"---TRAINING---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    epoch_elbo = 0.\n",
    "    prod_slda.train()\n",
    "    for batch in tqdm(dl):\n",
    "        \n",
    "        bow = batch['bow'].to(DEVICE)\n",
    "        bop = {k:v.to(DEVICE) for k,v in batch['meta'].items()}\n",
    "        \n",
    "        epoch_elbo += svi.step(bow, bop)\n",
    "\n",
    "    eval_elbo = 0\n",
    "    prod_slda.eval()\n",
    "    for batch in tqdm(eval_dl):\n",
    "        bow = batch['bow'].to(DEVICE)\n",
    "        bop = {k:v.to(DEVICE) for k,v in batch['meta'].items()}\n",
    "\n",
    "        # NICK: I think this may still be optimizing on holdout?\n",
    "        eval_elbo += svi.evaluate_loss(bow, bop)\n",
    "\n",
    "    \n",
    "    print(f'Epoch {epoch}: {epoch_elbo:.3f}')\n",
    "    print(f'Epoch {epoch}: Eval: {eval_elbo:.3f}')\n",
    "    \n",
    "    train_elbo.append(epoch_elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "310e69d2-833d-41c4-a81c-e1b92c3f8e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_elbo)), train_elbo)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "fcfcab56-571b-406f-9609-23bdf0593b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casing tensor([[0.4486, 0.2897, 0.2617],\n",
      "        [0.3345, 0.3537, 0.3119],\n",
      "        [0.2290, 0.3860, 0.3850],\n",
      "        [0.4337, 0.2909, 0.2754],\n",
      "        [0.3363, 0.3339, 0.3298]])\n",
      "emoji_task tensor([[0.0673, 0.0494, 0.0508, 0.0447, 0.0498, 0.0518, 0.0419, 0.0484, 0.0378,\n",
      "         0.0407, 0.0735, 0.0607, 0.0365, 0.0345, 0.0418, 0.0506, 0.0514, 0.0509,\n",
      "         0.0629, 0.0546],\n",
      "        [0.0785, 0.0343, 0.0427, 0.0459, 0.0526, 0.0514, 0.0358, 0.0559, 0.0486,\n",
      "         0.0326, 0.0657, 0.0603, 0.0399, 0.0213, 0.0419, 0.0388, 0.0497, 0.0407,\n",
      "         0.0636, 0.0999],\n",
      "        [0.0595, 0.0573, 0.0523, 0.0451, 0.0470, 0.0525, 0.0452, 0.0452, 0.0333,\n",
      "         0.0463, 0.0806, 0.0532, 0.0360, 0.0415, 0.0432, 0.0617, 0.0505, 0.0518,\n",
      "         0.0560, 0.0420],\n",
      "        [0.0760, 0.0374, 0.0462, 0.0468, 0.0541, 0.0518, 0.0386, 0.0546, 0.0472,\n",
      "         0.0348, 0.0690, 0.0633, 0.0414, 0.0208, 0.0427, 0.0415, 0.0494, 0.0436,\n",
      "         0.0665, 0.0744],\n",
      "        [0.0385, 0.0890, 0.0546, 0.0360, 0.0369, 0.0462, 0.0509, 0.0330, 0.0200,\n",
      "         0.0570, 0.0825, 0.0447, 0.0285, 0.0815, 0.0431, 0.0860, 0.0481, 0.0612,\n",
      "         0.0460, 0.0163]])\n",
      "formality tensor([[0.4975, 0.5025],\n",
      "        [0.5518, 0.4482],\n",
      "        [0.5774, 0.4226],\n",
      "        [0.5064, 0.4936],\n",
      "        [0.5663, 0.4337]])\n"
     ]
    }
   ],
   "source": [
    "# print(prod_slda.beta_document())\n",
    "# print(prod_slda.beta_meta())\n",
    "for key, value in prod_slda.beta_meta().items():\n",
    "    print(key, F.softmax(value,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "7b77b68e-c765-440f-aed3-62cdb37d5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'prod_slda_saved_model'\n",
    "prod_slda.eval()\n",
    "torch.save(prod_slda, path)\n",
    "\n",
    "# pyro.clear_param_store()\n",
    "# prod_slda = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4973a-1f99-4cdb-b144-a4ffac32db9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
